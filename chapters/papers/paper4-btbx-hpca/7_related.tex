\section{Related work}
\label{sec:related}

%The front-end bottleneck is composed of two main components: BTB misses and L1-I misses. In this section, we discuss the prior work addressing each of these components.
\subsection{Mitigating BTB misses}
BTB was first disclosed by Losq~\cite{losq} and was further expanded by Lee et al~\cite{btb}. Since BTB lies on the critical path for instruction delivery, there has been several proposals to increase its effectiveness. Instead of accessing BTB with the PC of each individual instruction, Yeh et al. \cite{bbtb} proposed to access it with basic-block address and store not only the target but also the fall-through address in the BTB. In case the branch is predicted to be not taken, the fall-through address is used, after fetching the current basic-block, as the next PC for both instruction fetch as well as for the next BTB access. The advantage of such a BTB organization over the conventional BTB organization is that it reduces BTB bandwidth and power requirements as a single access provides the next control flow divergence point, whereas the conventional organization requires as many accesses as the number of instructions until the next branch. Whereas the initial proposal on basic-block-based BTB~\cite{bbtb} stores full fall-through address, the later work~\cite{fbtb} proposed to store the delta between two sequential basic-block addresses. Fagin \cite{fagin1997partial} proposed to use the BTB storage more effectively by storing only the partial tags. To further amortize the tag storage cost, some designs proposed to share a BTB entry among multiple branches that reside in the same cache block \cite{amdbtb, confluence}. Though these BTB designs aim to improve different aspects of BTB management, they all share a common trait, i.e., they store full target addresses. Thus, the key idea of BTB-X can be applied to all of these BTB designs to reduce their target storage cost.

Prior work \cite{DUPN, ittage, jan, pdede, btbxCAL, btbxPACT} has also explored mechanisms to reduce the storage cost of branch targets. Seznec \cite{DUPN, ittage} proposed to break the target address into page number and offset; and store a pointer to the page number, along with the page offset, in the BTB while the page number itself is stored in a separate structure. It reduces the storage cost as a pointer to page number is smaller than the page number itself, and the page number for all the targets in a page is stored only once. Hoogerbrugge \cite{jan} proposed to size some of the entries in a set for storing small target offsets, thus reducing BTB storage requirements. 

The state-of-the-art BTB design, PDede \cite{pdede}, combines these two ideas to address their individual limitations. Concretely, Sezenc's design is sub-optimal for same-page branches as it unnecessarily stores (pointer to) their target page number even though it is same as the page number of their branch PCs. In contrast, Hoogerbrugge's design is sub-optimal for inter-page branches as it stores their full targets. Inspired from Hoogerbrugge's design, PDede sizes some entries in a set to store same-page branch targets; and similar to Seznec's design, for inter-page branches, it stores pointers to page numbers instead of page numbers themselves. PDede further reduces the inter-page target storage cost by dividing the page number into page- and region-number. However, as it is based on Seznec's design, it also has to pay the addition latency cost of indirection between main-BTB and the page-/region-BTB. Micro BTB~\cite{microbtb}, proposes a flexible BTB entry structure where each entry can store either one branch, if its offset is large, or two branches if their offsets are small. We show that all these designs are sub-optimal in exploiting the storage optimization opportunity presented by the uneven branch offset distribution. BTB-X not only captures this opportunity but also avoids the BTB indirection of the state-of-the-art.%, and its latency cost, that PDede suffers from.

Apart from optimizing BTB organization, prior work~\cite{phantom, ibmBTB, twig, boomerang, shotgun} has also explored BTB prefilling/prefetching to mitigate BTB misses. The state-of-the-art in BTB prefetching is a profile guided software prefetcher, called Twig~\cite{twig}. It analyzes an application's execution profile to identify critical BTB misses and then injects software prefetch instructions. The prefetch instruction takes compressed branch PC and target as operands and its execution fills this information in BTB. These prefetching techniques are complementary to BTB organization and, thus, can be used along with BTB-X.

%Prior work has also proposed to augment a low latency first-level BTB with a large capacity second-level BTB. A dedicated prefetcher is then used to bring entries from second-level to first-level BTB. IBM z-series processors use a technique called Two-level Bulk Preload~\cite{IBMz} to prefetch branches from a 24K-entry second-level BTB to a 4K-entry first-level BTB. The prefetching mechanism uses spatial correlation to prefetch a set of spatially-proximate entries into the first-level BTB upon a miss. Another technique, called Phantom BTB~\cite{burcea2009phantom}, virtualizes temporal streams of BTB entries into the last level cache instead of using a dedicated second-level BTB. Since these techniques are aimed at prefetching into BTB rather than improving its underlying organization, they are complementary to the techniques proposed in this paper.

%Shotgun \cite{shotgun} proposes to partition the BTB in separate regions for unconditional branches (U-BTB) and conditional branches (C-BTB). This design was motivated by the observation that program control flow often follow patterns of long unconditional jumps followed by short conditional jumps. By encoding the spatial footprints of target regions within the U-BTB, Shotgun prefetches short conditional branches into the C-BTB. However, it BTB's still store full target addresses.

%\vspace{0.1in}
\subsection{Mitigating L1-I misses}
As L1-I misses continue to be a major performance limiter in server applications\cite{profileWarehouse, jukebox, wosc}, prior work has proposed both hardware and software mechanisms to mitigate L1-I misses. On the hardware side, state-of-the-art temporal stream prefetchers \cite{tifs, pif} record the L1-I miss/access history and replay it to discover prefetch candidates. While such prefetchers are highly effective, their huge metadata storage cost renders them impractical despite recent attempts to address this weakness \cite{shift, confluence}. Fetch-directed prefetchers use in-core structures (BTB and branch direction predictor) to run ahead of the fetch unit to find prefetch candidates. While the early work \cite{fdip} focused on L1-I prefetching only, the state-of-the-art fetch-directed prefetchers \cite{boomerang, shotgun} also prefill into the BTB.


Several purely-software based approaches to instruction prefetching and improving the L1-I capacity has also been proposed \cite{chen2016autofdo, li2010lightweight, ottoni2017optimizing, luk2004ispike, panchenko2019bolt, luk1998cooperative, annavaram2003call, ayers2019asmdb, khan2020spy}. These methods use data from application profiling to perform either compile-time, link-time or post-link time optimizations. Since these methods are software-only they will benefit from the increased BTB capacity provided by the BTB-X organization. %AsmDB \cite{ayers2019asmdb} modifies software by inserting explicit prefetch instructions in strategic locations based on data-center wide application profiling. I-spy \cite{khan2020spy} improves the prefetch accuracy of AsmDB by making prefetches conditional depending on application context.
