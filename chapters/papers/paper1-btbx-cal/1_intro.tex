\section{Introduction}
\label{cal:sec::intro}

Contemporary server applications feature massive instruction footprints stemming from deeply layered software stacks. These footprints may far exceed the capacity of the branch target buffer (BTB) and instruction cache (\liningfigures{L1-I}), resulting in the so-called front-end bottleneck. BTB misses may lead to wrong-path execution, triggering a pipeline flush when misspeculation is detected. Such pipeline flushes not only throw away tens of cycles of work but also expose the fill latency of the pipeline. Similarly, \liningfigures{L1-I} misses cause the core front-end to stall for tens of cycles while the miss is being served from lower-level caches. 

BTB stands at the center of a high-performance core front end for three key reasons: it determines the instruction stream being fetched, it identifies branchs for the branch predictor, and it affects the \liningfigures{L1-I} hit rate. Specifically, by identifying control flow divergences, the BTB ensures that the branch predictor can make predictions for upcoming conditional branches. For predicted-taken and unconditional branches, the BTB supplies targets to which instruction fetch should be redirected. Finally, the BTB together with the direction predictor enables an important class of instruction prefetchers called fetch-directed instruction prefetchers (FDIP)~\cite{fdip, boomerang, shotgun}, which rely on the BTB to discover \liningfigures{L1-I} prefetch candidates. 

Considering the criticality of capturing the large branch working sets of modern workloads, commercial CPUs feature BTBs with colossal capacities, a trend also observed by ~\cite{rebase}. Thus, IBM z-series processors~\cite{IBMz}, AMD Zen-2~\cite{zen2}, and ARM Neoverse N1~\cite{neoverse} feature 24K-entry, 8.5K-entry, and 6K-entry BTBs. With each BTB entry requiring 10 bytes or more (\Cref{cal:sec:background}), BTB storage costs can easily reach into tens and even hundreds of KBs. Indeed, the Samsung Exynos M6 mobile processor allocates a staggering 529KB of on-chip storage to BTBs~\cite{exynos}. While such massive BTBs are effective at capturing branch working sets, they do so at staggering area costs.

This work seeks to reduce BTB storage requirements by increasing its {\em branch density}, defined as branches per KB of storage. To that end, we aim to reorganize individual BTB entries to minimize their storage cost. Our key insight is that branch offsets, defined as delta between the address of the branch instruction and that of its target, are unequally distributed but tend to require significantly fewer bits to represent than full target addresses. Our analysis reveals that 37\% of dynamic branches require only 7 bits or fewer for offset encoding, while a meager 1\% of branches need 25 bits or more to store their offsets. 

Based on this insight, we propose to store offsets in the BTB rather than full target addresses, which can be up to 64 bits long depending on the size of virtual address space. To accommodate the varied distribution of branch offsets, we partition the BTB into several smaller BTBs, each storing only those branches whose target offsets can be encoded with a certain number of bits. Because the target field accounts for over half of each entry's storage budget in a conventional BTB (\Cref{cal:fig:conv-btb}), this optimization brings significant storage savings.
We further observe that the tag field is the second-largest contributor to each BTB entry's storage requirement. To reduce this cost, we propose compressing the tags through the use of hashing.

Our final design, called {\em BTB-X}, uses an ensemble of five BTBs, each with 16-bit tags. The BTBs differ only in the number of bits they allocate for branch target offsets. Our evaluation shows that BTB-X can track over 2.8x more branches than a conventional BTB with the same storage budget. Conversely, BTB-X can accommodate the same number of branches as existing BTBs while requiring 2.8x less storage.