\pdfminorversion=7
\documentclass[../main.tex]{subfiles}

\begin{document}
\ifx\chapincluded\undefined
  \begin{refsection}[main-bib]
 \fi

 \chapter{Research Contributions}
\label{chap:rcontrib}

The research contributions of this thesis are presented as five articles. The first four (\Cref{chap:cal-paper,chap:pact-paper,chap:wosc-paper,chap:hpca-paper}), and the last (\Cref{chap:eurosys-paper}) is pending review at the time of thesis submission. In this chapter, we summarize the contributions of each paper and contextualize the papers in regard to prior work. The papers that are included in this thesis are:




\section{Contribution overvew}

\subsection{Research direction 1: Hardware}

\subsubsection{Understanding FaaS}

\paragraph{Motivation.} Function-as-a-service (FaaS) computing is a rapidly growing cloud computing model. In FaaS, the unit of computation is a function, normally with a very short execution time. Since a FaaS function is loaded and started when it is needed and shut down when it has finished executing, will co-schedule a large number of functions on the same processors to maximize server utilization. Recent work has reported that this heavily interleaved execution leads to microarchitectural state thrashing causing most of FaaS function execution to occur on a cold microarchitectural state~\cite{shahrad19_archit_implic_funct_servic_comput,lukewarm_serverless}. This work, however, stops short of investigating which specific properties of FaaS execution that causes this poor utilization of microarchitectural state.

Use the profiling to motivate BTB-X

\paragraph{Methodology.}
To address this gap in the research, we perform an evaluation of FaaS
functions executed on a processor with warm and cold
microarchitectural states respectively. We use a representative suite
of FaaS functions consisting of both real-world and synthetic
workloads. Using synthetic workloads allow us to modify specific
parameters of interest in a targeted way. To measure the impact of
executing the functions on a warm and cold microarchitectural state we
execute the functions in tow modes: back-to-back and interleaved. In
back-to-back execution, the functions are executed repeatedly in a
tight loop. In the interleaved execution case, each invocation of a
function is interleaved with an invocation of a \emph{trhasher}
function that thrashes all existing microarchitectural state. The
thrasher function achieves this by performing two
actions:
\begin{inparaenum}[1)]
\item it invokes the x86 WBINVD
  instruction that writes back and invalidates the entire cache
  hierarchy of a processor core and \item it runs a function that
  executes a long sequence of branches in order to thrash the state of
  the BTB and the branch predictor. \end{inparaenum} We measure
microarchitectural parameters using perf and use the TopDown
methodology~\cite{yasin14_top_down} for identifying specific
microarchitectural bottlenecks. For both the back-to-back and
interleaved execution we measured both the wall-clock running time of
the functions and key microarchitectural parameters.

To support our findings (presented below) we perform a measurement of
the code footprint of each of the FaaS functions that we use in our
analysis. Doing this using a full microarchitectural simulator is a
slow and tedious process and it is not possible to perform this
analysis statically since the dynamic instruction trace of a program
is only known at runtime.  Therefore, we use a method described
in~\cite{splash2} that use branch-record traces and a cache simulator
to estimate the code footprint of a function. The branch-record traces
show the branches taken by a program. By running the locations of
these branches through a cache simulator, we can measure how big the
cache has to be in order to fit the entire code footprint of a
program. More specifically this is done by changing the size of the
simulator cache until the observed miss-rate reaches zero. When this
is the case, the entire code footprint of the program fit inside the
cache.


\paragraph{Key Results.}
Our results show that two properties of a FaaS function impacts how
sensitive it is to being executed on a cold microarchitectural state:
Its execution time and its code footprint. For example, the shortest
running function that we evaluate (0.25 ms) show a slowdown of
$17\times$ when executed on a processor with a cold microarchitectural
state. The longest running functions are largely unaffected by
microarchitectural state thrashing. This indicates that the warm-up
latency of microarchitectural structures is amortized for the vast
majority of FaaS functions.

Another important observation we make from our experiments, is that
the real-world FaaS functions we evaluate are overwhelmingly front-end
bound. This is in line with similar observations made for general
server
workloads~\cite{ferdman12_clear_cloud,kanev15_profil,ayers19_asmdb}
and for FaaS functions specifically~\cite{lukewarm_serverless}.

\subsubsection{A storage-efficient BTB organization for servers}

\paragraph{Motivation}
The front-end bottleneck is a well-known and widely documented problem affecting processors executing server workloads~\cite{ailamaki99_dbmss_moder_proces,ferdman12_clear_cloud,kanev15_profil,ayers19_asmdb}. Compared to other workload classes, server workloads pose particular challenges for processor frontends due to their very large instruction footprints. These footprints, quickly overwhelms the capacities of the instruction cache (L1-I) and the Branch Target Buffer (BTB). In order to handle this problem, computer architects has proposed a large number of different approaches. L1-I prefetchers are a highly effective way to alleviate the front-end bottleneck as they help avoiding the fill-latency that occurs when program control flow is diverted to an instruction that is not found in the L1-I.  As detailed in \truls{Add reference to relevant background section}, a particularly effective class of instruction prefetchers are known as Fetch Directed Instruction Prefetching (FDIP)~\cite{reinman99_fetch_direc_instr_prefet}. Since FDIP rely on the BTB to identify the control flow of a program, having a BTB with sufficient capacity is of crucial importance. By simply scaling the design of a conventional BTB design, the capacity required for holding realistic branch working sets of contemporary server workloads quickly become infeasible. To address this, previous work observed that several instruction addresses reside within the same memory page or region. Since these page numbers are shared between several memory addresses, they can be deduplicated by only storing them once in a dedicated table and replace their uses with pointer to that table~\cite{seznec96_dont_use_page_number_point_it,soundararajan21_pdede}. The limitation of these approaches is that they introduce a level of indirection for BTB lookups, adding complexity and increasing the BTB access latency.

\paragraph{Methodology}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/offset_distribution.pdf}
  \caption{\label{fig:offset-distr} Distribution of branch target offsets in the IPC-1~\cite{ipc1} workload traces}
\end{figure}

In an attempt to find a more efficient way of storing branch offsets, we performed an analysis of the branch target offsets in a large number of real-world workload traces released for the first Instruction Prefetching Championship (IPC-1)~\cite{ipc1}. A branch target offset is the distance from a branch to its destination. This analysis, shown in \Cref{fig:offset-distr} showed that the bast majority of branches have targets very close to their origin. For example, using just, 19 bits, we can represent the target offsets of 90\% of all branches.


\paragraph{Key results}

\truls{Emphasize how the evaluation methodology using the traces from server workloads applies to FaaS workloads since a) they are fundamentally similar except that FaaS workloads run for a shorter time and b) we and prior work shows that FaaS workloads also have a large code footprint}

\subsection{Research direciton B: Software optimizations for serverless computing}

\paragraph{Motivation}



\paragraph{Methodology}

\paragraph{Key results}


%\subsection{Primary contribution 2: 

\ifx\chapincluded\undefined
  \printbibliography
  \end{refsection}
 \fi
\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End: