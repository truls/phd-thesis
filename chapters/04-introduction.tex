\pdfminorversion=7
\documentclass[../main.tex]{subfiles}

\begin{document}
\ifx\chapincluded\undefined
  \begin{refsection}[main-bib]
 \fi

\chapter{Introduction}
\label{chap:intro}

\section{Motivation}
% Over the past couple of decades, the computing landscape has undergone
% a radical transformation. Driven by the ubiquity of the internet,
% computing resources has increasingly shifted from the hands of the
% user and into data centers belonging to a handful of global
% providers.

The emergence of cloud computing has driven a landslide change in the
computing landscape where computing resources are increasingly
centralized in the hands of a few global providers. The cloud
computing revolution is driven by the developers' desire for simple and
flexible hosting. Function-as-a-service (FaaS) computing, the core
technology powering serverless computing, is a cloud programming model
that is currently seeing rapid growth. FaaS promises simplicity,
elasticity, and cost-efficiency by moving deployment and resource
management decisions from the user to the provider. Currently, half of all organizations that rely on cloud computing deploy applications using the FaaS programming model~\cite{serverless_state}. In the FaaS model, the fundamental
unit of computation is a \emph{function}. A FaaS function is a
stateless unit of code that is executed on demand when triggered by an
external event, such as an incoming HTTP request or a timer
tick. Thus, when developing a FaaS function, the developer only needs
to implement its functionality, leaving all the deployment-level
decisions to the provider.
% A key property of
% FaaS functions is that they are stateless, meaning that they produce
% deterministic output when executed repeatedly with the same
% inputs.
When an event associated with a FaaS function is triggered, the
provider must allocate resources and spawn the target function in
order to handle the request. When the FaaS function finishes its
execution, it is torn down and the resources allocated for its
execution are freed, possibly after the expiry of a grace period to
prevent repeatedly starting the function in case it receives requests
with a short inter-arrival time. This ephemeral deployment structure
makes FaaS fundamentally different from the conventional cloud
computing model where computing resources are provisioned in advance
by the developer and services run as persistent daemons. Another
aspect that distinguishes FaaS functions from conventional cloud
services is that their running time is often very short, with 67\% of
functions studied
in~\cite{eismann20_review_server_use_cases_their_charac} having a
runtime in the order of seconds. Furthermore, the current trend
indicates that functions are getting shorter with median function
execution time reducing by 2\texttimes{} from 2019 to
2020~\cite{serverless_state_21}.


The unique deployment model and runtime characteristics of FaaS
functions motivates investigation into how FaaS interacts with current
hardware. Notably, performance-critical structures of computing
systems, such as caches, work under the fundamental assumption that
workloads exhibit temporal locality. Prior work investigating
this~\cite{lukewarm_serverless} notes that FaaS functions have large
code footprints and therefore exhibit a large number of
instruction-cache misses when executed on current processors. However,
their investigation does not identify the specific properties of
serverless functions that cause this behavior. In particular, they do
not identify a threshold where FaaS functions run long enough to be
considered equivalent to general server workloads from a
microarchitectural perspective.

The large code footprints and high instruction misses of FaaS
functions brings them in line with a general trend observed for server
workloads. Over the last three decades, long before the FaaS model
emerged and attracted the attention of researchers, the shift towards
Warehouse Scale Computing (WSC)~\cite{barroso18_datac_as_comput} has
challenged the way computer architects conventionally designed
systems. Since their emergence, a key feature of WSC is that they
consist of a large aggregation of commodity computing systems
containing commodity
processors~\cite{barroso03_web_searc_planet}. Commodity hardware is
favored due to its low cost and widespread availability. However,
scholarly research revealed that commodity processors are
fundamentally unfit for the WSC
workloads~\cite{ferdman12_clear_cloud,kanev15_profil}. Unlike the
workloads typically encountered by the client-side or in scientific
computing, WSC workloads are marked by massive, multi-megabyte code
footprints. These massive code footprints overwhelm the capabilities
of common processor frontends causing a well-documented phenomenon
known as the \emph{frontend
  bottleneck}~\cite{ailamaki99_dbmss_moder_proces,keeton98_perfor_charac_quad_pentium_pro,ranganathan98_perfor_datab_workl_shared_memor}. The
frontend bottleneck occurs when the backend of the processor is
starved for instructions due to inefficiencies in the frontend. In
response to this, the academic community has proposed a myriad of
diverse methods to mitigate the front-end
bottleneck~\cite{reinman99_fetch_direc_instr_prefet,kumar17_boomer,kumar18_blast_throug_front_end_bottl_with_shotg,kumar20_shoot_down_server_front_end_bottl,ferdman08_tempor,ferdman11_proac_instr_fetch,kaynak13_shift,kaynak15_confl,ayers19_asmdb,ajorpaz18_explor_predic_replac_polic_instr,khan20_i_spy,soundararajan21_pdede,ansari20_divid,khan21_rippl,}.
This thesis seeks to exploit outstanding opportunities in mitigating
the front-end bottleneck to improve the performance of FaaS
functions and server workloads in general.



The unique modularity, compositionality, and language-independence of
FaaS functions make the FaaS computing model highly attractive to
developers~\cite{williams16_growin_need_micros_bioin}. To maximize
these benefits, developers are encouraged to keep each FaaS
function small with a narrowly defined purpose. This also contributes
to the aforementioned very short running time of FaaS
functions~\cite{shahrad19_archit_implic_funct_servic_comput,mahgoub22_wisef,shahrad20_server_wild,lukewarm_serverless,du20_catal}. Additionally,
since providers charge for the CPU-time needed to execute a FaaS
function, developers also have a strong monetary incentive to keep the
running time of functions short~\cite{faas_procing}.  Larger and more
complex FaaS applications are built by composing multiple smaller FaaS
functions. Since a FaaS function can be written in any language
depending on what best fits its purpose, the functions comprising a
FaaS application normally communicate using network-backed Remote
Procedure Call (RPC)
interfaces~\cite{gan19_open_sourc_bench_suite_micros}. In addition to
facilitating language-independence, these loosely-coupled RPC-backed
communication interfaces also make it possible to transparently
replace parts of an application and rewrite it piece-by-piece
without discarding tested and reliable components. Language
independence is a critically important property of FaaS as 96\% of
large organizations that use FaaS deploy functions written in two or
more different languages\cite{serverless_state}. While this RPC-backed
communication fabric enables many of the most attractive features of
the FaaS model, it also introduces a considerable communication
overhead that is highly detrimental to the performance of FaaS
applications.  \truls{Cite work that investigates how bad the
  communication overhead is and emphasize that it is really bad}. Due
to the detrimental performance impact that the communication overhead
has on FaaS functions, this thesis aims to reduce it by introducing a
software-based transformation for FaaS functions.



% \begin{enumerate}
% \item FaaS functions have a short running time. Hence, they may not give essential performance-enhancing structures such as caches sufficient time to warm up and perform optimally
% \item Since FaaS applications are composed of several loosely-coupled functions, they have a significant internal communication overhead. Modularity in monolithic applications is achieved through tightly-coupled linking that adds no additional communication overhead.
% \end{enumerate}

% This thesis investigates serverless functions from these two perspectives.


\section{Research overview}
\label{sec:res-overview}

The interplay between hardware and software in computing systems is
complex and multi-faceted. Software optimizations can only go so far
until they are fundamentally limited by hardware
restrictions. Likewise, hardware optimizations cannot alleviate
fundamental overheads in software. Furthermore, introducing
workload-tailored hardware optimizations is always a trade off between
the ensuing increase in complexity and the scope of workloads that
benefit from the optimization.

The guiding research statement of this thesis is to:
\begin{framed}
\noindent
\textbf{Identify the sources of inefficiencies in the FaaS application execution model, determine their root causes, and investigate hardware and software mechanisms to alleviate them.}
\end{framed}
\noindent
To that end, the research presented in this thesis takes two
directions. The first research direction, RD-A, focuses on
understanding the microarchitectural behavior of FaaS functions and
leveraging this understanding to investigate microarchitectural
features for optimizing their execution.
%is concerned with hardware optimizations
%that improves the performance of serverless computing and in that
%regard we perform an in-depth microarchitectural evaluation of FaaS
%functions and we propose a microarchitectural optimization that
%itigates the front-end bottleneck for server workloads generally.
The second research direction, RD-B, focuses on reducing the
RPC-induced inter-function communication overhead in FaaS applications by
introducing a software-level transformation.

% The
% first step towards answering this question is to understand platform
% requirements for effectively executing FaaS applications. To improve
% this understanding, our first primary contribution is an in-depth
% characterization of the microarchitectural interactions occurring with
% FaaS function execution (\Cref{sec:pc1}). The results of this study
% opened two research directions. The first, RD-A, addresses the
% front-end bottleneck (as described in the introduction) found in
% server processors and is described in \Cref{sec:pc2}. The second,
% RD-2, is concerned with alleviating the massive inter-function
% communication overhead encountered in complex FaaS applications. This
% direction is described in \Cref{sec:pc3}.

\subsection{Research direction A: Hardware Optimizations for Serverless Computing}
\label{sec:rda}

The first step of solving any problem is to gain an in-depth
understanding of its characteristics. We begin approaching this
research direction with a study that performs an in-depths
characterization of how one of the unique features of FaaS functions,
their very short running times, interacts with current processors. Previous work has investigated the impact of FaaS on current
processors and concluded that FaaS has a significant negative impact on
current
microarchitectures~\cite{shahrad19_archit_implic_funct_servic_comput,lukewarm_serverless}. In
particular, \textcite{lukewarm_serverless} finds that
microarchitectural state thrashing occurs due to the short running
time of FaaS functions and the heavy interleaving that occurs when
many different functions are queued for execution on a single
processor core. However, they stop short of investigating \emph{why}
FaaS functions has this impact on common microarchitectures. To
address this, we investigate which properties of FaaS functions make
them susceptible to the negative effects of microarchitectural state
thrashing.

% We find that only very short
% running functions are negatively impacted an that this negative impact
% increases for functions with larger code footprints. Further, the
% functions with a running time that is sufficiently short to see a
% negative impact are rare in real-world applications. Therefore, the
% results indicate two things. First of all that microarchitectural
% structures warm up sufficielty fast for the warm-up delay to be
% armortized for most FaaS functions. Therefore, this aspect of FaaS
% function performance is better addressed through software
% optimizations rather than a targeted hardware optimization. Secondly,
% the large code footprints of FaaS functions mean that they suffer from
% the front-end bottleneck in the same way as conventional server
% workloads.

% and 2) that optimally, very quickly
% and therefore only functions with a very short execution time are
% amenable to targeted microarchitectural optimizations.

% The conclusions of this work prompted two research directions:
%  \begin{description}
%  \item[RD-A] Long-running FaaS functions ($>1\text{ms}$) challenges
%    current processor architectures in the same way as other server workloads. This research direction therefore
%    continues to address the front-end bottleneck of major processors.
%  \item[RD-B] The FaaS functions that have a sufficiently short running
%    time to be negatively affected by microarchitectural state
%    thrashing are rare in real-life deployments. Therefore, the benefit
%    of introducing a hardware optimization targeting this aspect of
%    FaaS performance is limited. Instead, this research direction
%    therefore focuses on addressing these challenges by transforming
%    FaaS applications in software allowing them to execute more efficiently on current hardware.
%  \end{description}

From this work, we learn that only functions with a very short
execution times (<1 ms) benefit from being executed on a processor
with a warm microarchitectural state. In real-world workloads such
functions are uncommon~\cite{shahrad20_server_wild,mahgoub22_wisef}
and therefore, adding a specialized hardware optimization to
accommodate them would not be worthwhile. Furthermore, the analysis
show that FaaS functions are heavily frontend bound. This corroborates
similar observations made in previous work
\cite{lukewarm_serverless,gan19_open_sourc_bench_suite_micros}. Thus,
to improve execution performance in the general case for
serverless functions we are strongly motivated to continue working on
eliminating the front-end bottleneck.

To that end, we propose BTB-X, a storage-efficient Branch Target
Buffer (BTB) (see \Cref{sec:btb-background}) organization for servers.
The starting point of this work is that Fetch Directed Instruction
Prefetchers (FDIP) have superior performance when given sufficient BTB
capacity~\cite{ishii21_re_fetch_direc_instr_prefet}. Considering the
vast branch working sets of contemporary server workloads, industry
has responded by allocating large amounts of storage to the
BTB~\cite{neoverse,IBMz,zen2} with resulting sizes reaching up to
500KB of on-chip storage~\cite{exynos}. This amount of space means
that large chunks of chip area and power budget is dedicated to the
BTB. Further, considering that instruction set sizes show no sign of
slowing down with Google observing 20\% annualized growth
rates~\cite{kanev15_profil}, it is infeasible to respond to this
simply by increasing the BTB storage budget. BTB-X defies this trend
by leveraging a key insight about the distribution of branch target
offset sizes. A conventional BTB (Conv-BTB) stores the full target
address of every branch it holds. We make a key observation that most
branch targets are close to the branch instructions
themselves. Therefore, we can achieve significant storage savings by
storing branch target offsets instead of the full target addresses in
the BTB. We further observe large variability in the target offset
sizes, i.e., some offsets are much larger/smaller than others. Thus, a
BTB design should be able to accommodate this variability to maximize
storage efficiency. The resulting design improves the state of the art
in terms of branch storage density, lookup latency and power
requirements. Since BTB-X increases the branch working set coverage of
the BTB, it also improves instruction prefetching performance (see
\Cref{sec:instr-delivery}). Thereby, it helps alleviating the
front-end bottleneck and improves the performance of, particularly,
server workloads.

The motivating analysis is presented in Paper \liningfigures{A1} (\Cref{chap:wosc-paper}).
The design of BTB-X evolved over time and is described in three paper. The first version of BTB-X is described in paper \liningfigures{A2}
(\Cref{chap:cal-paper}), an improved version is presented in the
short paper \liningfigures{A3} (\Cref{chap:pact-paper}) and the final design is
presented in the full length paper \liningfigures{A4} (\Cref{chap:hpca-paper}).

\noindent
\begin{tabular}{lp{0.8\textwidth}}
  \textbf{Paper \liningfigures{A1}} & \textbf{Impact of Microarchitectural State Reuse on Serverless Functions} \newline
                      \textbf{Truls Asheim}, Tanvir Ahmed Khan, Baris Kasicki and Rakesh Kumar \newline
                     \textbf{In:} Proceedings of the Eighth International Workshop on Serverless Computing, 2022
                     \vspace*{0.3cm}\\
  \textbf{Paper \liningfigures{A2}} & \textbf{BTB-X: A Storage-Effective BTB Organization} \newline
                     \textbf{Truls Asheim}, Boris Grot and Rakesh Kumar \newline
                     \textbf{In:} IEEE Computer Architecture Letters, Volume: 20, Issue: 2, July-December 2021
                     \vspace*{0.3cm} \\
\textbf{Paper \liningfigures{A3}} & \textbf{A Specialized BTB Organization for Servers} \newline
                     \textbf{Truls Asheim}, Boris Grot and Rakesh Kumar \newline
                     \textbf{In:} Proceedings of the International Conference on Parallel Architectures and Compilation Techniques, 2022
                    \vspace*{0.3cm} \\
\textbf{Paper \liningfigures{A4}} & \textbf{A Storage-Effective BTB Organization for Servers} \newline
                     \textbf{Truls Asheim}, Boris Grot and Rakesh Kumar
                    \newline
                     \textbf{In:} IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2023
                     \vspace*{0.3cm} 

\end{tabular}



\subsection{Research direction B: Software Approaches for Optimizing Serverless Computing}
\label{sec:pc3}
The second research direction investigates software optimizations for
serverless computing. The software inefficiencies in serverless
computing mainly have two origins. First of all, since resources for
executing a FaaS function are allocated transiently, starting a FaaS
function for the first time is marked by a significant latency known
as the cold-start latency. A large body of previous work investigates
this issue, for
example~\cite{du20_catal,,ustiugov21_bench_analy_optim_server_funct_snaps}. Another
well-documented performance challenge in FaaS is the communication
overhead experienced by FaaS
applications~\cite{gan19_open_sourc_bench_suite_micros}. As mentioned
previously, the loose coupling between the individual functions
comprising a FaaS application means that slow, high-latency RPC
interfaces are used for intra-application communication. Because of
the impact of this problem, a large body of previous work has
addressed this issue~\cite{kotni21_faast, mahgoub22_wisef,
  barcelona-pons19_faas_track,sreekanti20_cloud,shillaker20_faasm,jia21_night}. However,
we observe that these approaches compromise with at least one of the
critical properties that make FaaS attractive to developers. Some
approaches, require FaaS applications to be written in a specific
language, compromising the developers' ability to freely mix and match
implementation languages of functions.  Other approaches require
existing functions to be rewritten to use a certain specialized
API. Finally, some approaches propose entirely new runtimes that can
be difficult to integrate on existing FaaS computing platforms.

In response to this observation, we introduce CoFaaS, a
software-based, fully automatic, transformation for FaaS applications
that significantly reduces the inter-application communication
overhead without sacrificing any of the essential properties that make
FaaS attractive to developers. CoFaaS is based on the observation that
an individual FaaS function must have a formally defined external
interface in order to preserve the modularity of FaaS and support
composition with other functions. This means that, as long as we
preserve a function's external interface, we can freely transform
it. CoFaaS exploits this to retarget FaaS functions to a
platform-neutral and language independent bytecode language known as
WebAssembly (Wasm)~\cite{rossberg22_webas_core_specif}. The CoFaaS transformation turns each FaaS function
into a CoFaaS component. Multiple CoFaaS components can then be
composed, yielding a complete application, that can be executed on a
single Wasm runtime. Our evaluation shows that the CoFaaS
transformation improves inter-function communication overheads by up
to 100\texttimes{} without sacrificing any of FaaS' most attractive properties.


The complete CoFaaS design and implementation is described in Paper \liningfigures{B1}
(\Cref{chap:eurosys-paper}):


\vspace*{0.5em}
\noindent
\begin{tabular}{lp{0.8\textwidth}}
\textbf{Paper \liningfigures{B1}} & \textbf{CoFaaS: Automatic Transformation-based
                    Consolidation of Serverless Functions} \newline
                     \textbf{Truls Asheim}, Magnus Jahre and Rakesh Kumar \newline
                     \textbf{Submitted to:} Eurosys 2024
\end{tabular}

\section{Thesis Overview}
This thesis is structured in two parts. \Cref{part:one} introduces the
topic matter of the thesis and positions its
contribution. \Cref{chap:intro} introduces the topic of the thesis,
\Cref{chap:background} provides background material that aides
understanding and contextualizes the topics of the included paper,
\Cref{chap:rcontrib} summarizes the research contributions of the
thesis in more details and finally \Cref{chap:conclusions} presents
the thesis' conclusions. \Cref{part:two} contains the scientific
publications that collectively describe the contributions of the
thesis.

\ifx\chapincluded\undefined
  \printbibliography
  \end{refsection}
 \fi


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End: