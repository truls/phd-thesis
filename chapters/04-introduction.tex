\pdfminorversion=7
\documentclass[../main.tex]{subfiles}

\begin{document}
\ifx\chapincluded\undefined
  \begin{refsection}[main-bib]
 \fi


\chapter{Introduction}
\label{chap:intro}

\section{Motivation}
% Over the past couple of decades, the computing landscape has undergone
% a radical transformation. Driven by the ubiquity of the internet,
% computing resources has increasingly shifted from the hands of the
% user and into data centers belonging to a handful of global
% providers.

Function-as-a-Service (FaaS) computing, the core technology powering
serverless computing, is a cloud computing model that is currently
seeing rapid growth. FaaS promises simplicity, flexibility and
cost-efficiency by moving deployment and resource allocation decisions
from the user to the provider. Currently, FaaS technology is used by
half of all organizations that use cloud
computing~\cite{serverless_state}. In the FaaS model, the fundamental
unit of computation is a \emph{function}. A FaaS function is a
stateless unit of code that is executed on demand following the
triggering of an external event, such as an incoming HTTP request or a
timer tick. Thus, when developing a FaaS function, the developer only
needs to implement the functionality of the function, leaving all
other deployment-level decisions to the provider.
% A key property of
% FaaS functions is that they are stateless, meaning that they produce
% deterministic output when executed repeatedly with the same
% inputs.
When an event associated with a FaaS function is triggered,
the provider must allocate resources and invoke the target function in
order to handle the request. When the FaaS function has finished
executing, the function is shut down and the resources allocated to
its execution are freed, possibly after the expiry of a grace
period. This ephemeral deployment structure make FaaS fundamentally
different from the conventional cloud computing model where computing
resources are provisioned in advance by the developer and services run
as persistent daemons.

Additionally, the FaaS computing model heavily emphasizes application
modularity, code reuse and compositionality. A consequence of this is
that each individual FaaS function has a single well-defined purpose,
hence its running time is often less than 1
second~\cite{shahrad19_archit_implic_funct_servic_comput,mahgoub22_wisef,shahrad20_server_wild,lukewarm_serverless,du20_catal}.
Larger and more complex FaaS applications are then built by composing
multiple smaller FaaS functions. Since a FaaS function can be written
in any language depending on what best fits its purpose, the functions
comprising a FaaS application normally communicate using
network-backed Remote Procedure Call (RPC) interfaces. In addition to
facilitating language-independence, these loosely-coupled RPC-backed
communication interface also makes it possible to transparently
replace parts of an application and rewriting it piece-by-piece
without discarding tested and reliable components. While this
RPC-backed communication support many of the most attractive features
of the FaaS model, it also introduce a considerable communication
overhead that is highly detrimental to the performance of FaaS functions.

Over the last three decades, long before the FaaS model emerged and
attracted the attention of researchers, the shift towards Warehouse
Scale Computing (WSC)~\cite{barroso18_datac_as_comput} has challenged
the way computer architects conventionally designed systems. Since
their emergence, a key feature of WSC's is that they consist of a
large aggregation of commodity computing systems containing commodity
processors~\cite{barroso03_web_searc_planet}. Commodity hardware is
favored due to its low cost and widespread availability. However,
scholarly research revealed that commodity processors are
fundamentally unfit for the WSC
workloads~\cite{ferdman12_clear_cloud,kanev15_profil}. Unlike
workloads typically encountered client-side or in scientific
computing, WSC workloads are marked by massive, multi-MB code
footprints and comparatively meager data footprints. These massive
code footprints overwhelms the capabilities of common processor
frontends causing a well-documented phenomenon known as the
\emph{frontend
  bottleneck}~\cite{ailamaki99_dbmss_moder_proces,keeton98_perfor_charac_quad_pentium_pro,ranganathan98_perfor_datab_workl_shared_memor}. The
frontend bottleneck occurs when the backend of the processor is
starved for instructions due to inefficiencies in the frontend. In
response to this, the academic community has proposed a myriad of
diverse methods to mitigate the front-end
bottleneck~\cite{reinman99_fetch_direc_instr_prefet,kumar17_boomer,kumar18_blast_throug_front_end_bottl_with_shotg,kumar20_shoot_down_server_front_end_bottl,ferdman08_tempor,ferdman11_proac_instr_fetch,kaynak13_shift,kaynak15_confl,ayers19_asmdb,ajorpaz18_explor_predic_replac_polic_instr,khan20_i_spy,soundararajan21_pdede,ansari20_divid,khan21_rippl,}.
%\truls{[TODO:  More citations]}

In this thesis, we closely examine the challenges FaaS pose to
conventional hardware and software architectures. We observe that FaaS
functions are affected by the frontend bottleneck in the same way as
other server workloads. Therefore, we propose a microarchitectural
optimization that alleviate this bottleneck. However, many of the
performance challenges in FaaS originate not from inefficiencies in
the hardware, but rather from software overheads inherent to its
design. To that end, we propose a method to transform FaaS
applications in a way that removes many of these inherent software overheads.

% \begin{enumerate}
% \item FaaS functions have a short running time. Hence, they may not give essential performance-enhancing structures such as caches sufficient time to warm up and perform optimally
% \item Since FaaS applications are composed of several loosely-coupled functions, they have a significant internal communication overhead. Modularity in monolithic applications is achieved through tightly-coupled linking that adds no additional communication overhead.
% \end{enumerate}

% This thesis investigates serverless functions from these two perspectives.


\section{Research overview}
The guiding research statement of this thesis is to 
\begin{framed}
\noindent
\textbf{Identify sources of inefficiencies in FaaS applications, determine why they occur and to propose hardware and software mechanisms to alleviate these inefficiencies.}
\end{framed}
\noindent
To that end, the research presented in this thesis follows two
directions. The first, RD-A is concerned with hardware optimizations
that improves the performance of serverless computing and the second,
RD-B is concerned with software optimizations that improve the
performance of serverless computing.

% The
% first step towards answering this question is to understand platform
% requirements for effectively executing FaaS applications. To improve
% this understanding, our first primary contribution is an in-depth
% characterization of the microarchitectural interactions occurring with
% FaaS function execution (\Cref{sec:pc1}). The results of this study
% opened two research directions. The first, RD-A, addresses the
% front-end bottleneck (as described in the introduction) found in
% server processors and is described in \Cref{sec:pc2}. The second,
% RD-2, is concerned with alleviating the massive inter-function
% communication overhead encountered in complex FaaS applications. This
% direction is described in \Cref{sec:pc3}.

\subsection{Research direction A: Hardware Optimizations for Serverless Computing}
\label{sec:rda}

The interplay between hardware and software in computing systems is
complex and multi-faceted. Software optimizations can only go so far
until they are fundamentally limited by hardware
restrictions. Likewise, hardware optimizations cannot alleviate
fundamental overheads in software. Furthermore, introducing
workload-tailored hardware optimization is always a trade off between
the ensuing increase in complexity and the scope of workloads that
benefit from the optimization. Previous work has investigated the
impact of FaaS on current hardware and concluded that FaaS has a
significant negative impact on current
microarchitectures~\cite{shahrad19_archit_implic_funct_servic_comput,lukewarm_serverless}. In
particular, \textcite{lukewarm_serverless} finds that
microarchitectural state thrashing occurs due to the short running
time of FaaS functions and the heavy interleaving of functions
executed on the same processor core. However, they stop short of investigating \emph{why} FaaS
functions has this impact on common microarchitectures.

To address this, we investigate which properties of FaaS functions
that make them susceptible to the negative effects of
microarchitectural state thrashing.

% We find that only very short
% running functions are negatively impacted an that this negative impact
% increases for functions with larger code footprints. Further, the
% functions with a running time that is sufficiently short to see a
% negative impact are rare in real-world applications. Therefore, the
% results indicate two things. First of all that microarchitectural
% structures warm up sufficielty fast for the warm-up delay to be
% armortized for most FaaS functions. Therefore, this aspect of FaaS
% function performance is better addressed through software
% optimizations rather than a targeted hardware optimization. Secondly,
% the large code footprints of FaaS functions mean that they suffer from
% the front-end bottleneck in the same way as conventional server
% workloads.

% and 2) that optimally, very quickly
% and therefore only functions with a very short execution time are
% amenable to targeted microarchitectural optimizations.

% The conclusions of this work prompted two research directions:
%  \begin{description}
%  \item[RD-A] Long-running FaaS functions ($>1\text{ms}$) challenges
%    current processor architectures in the same way as other server workloads. This research direction therefore
%    continues to address the front-end bottleneck of major processors.
%  \item[RD-B] The FaaS functions that have a sufficiently short running
%    time to be negatively affected by microarchitectural state
%    thrashing are rare in real-life deployments. Therefore, the benefit
%    of introducing a hardware optimization targeting this aspect of
%    FaaS performance is limited. Instead, this research direction
%    therefore focuses on addressing these challenges by transforming
%    FaaS applications in software allowing them to execute more efficiently on current hardware.
%  \end{description}

From this work, we draw the conclusions that only functions with
a very short execution time (<1 ms) benefits from being executed on a
processor with a warm microarchitectural state. In real-world
workloads such functions are
uncommon~\cite{shahrad20_server_wild,mahgoub22_wisef} and therefore
adding a specialized hardware optimization to accommodate them would
not be worthwhile. Furthermore, the analysis show that FaaS functions
are heavily frontend bound. This corroborates similar observations made
in previous work
\cite{lukewarm_serverless,gan19_open_sourc_bench_suite_micros}. Thus,
in order to improve execution performance in the general case for
serverless functions we are strongly motivated to continue working on
eliminating the front-end bottleneck.

To that end, we propose a
storage-efficient Branch Target Buffer (BTB) (see
\Cref{chap:btb-background}) organization for servers called BTB-X.
The starting point of this work is that FDIP instruction prefetchers
have superior performance when given sufficient BTB
capacity~\cite{ishii21_re_fetch_direc_instr_prefet}. Considering the
vast branch working sets of contemporary server workloads, industry
has responded by allocating large amounts of storage to the
BTB~\cite{neoverse,IBMz,zen2} with resulting sizes reaching up to
500KB of on-chip storage~\cite{exynos}. This amount of space means
that significant amounts of chip area and power budget is dedicated to
the BTB. Further, considering that instruction set sizes show no sign
of slowing down with Google observing 20\% annualized growth
rates~\cite{kanev15_profil}, it is infeasible to respond to this
simply by increasing the BTB storage budget.

BTB-X defies this trend by leveraging a key insight about the
distribution of branch target offset sizes. Conventional BTBs
(Conv-BTB) stores the full target address of every branch it holds. We
make the observation that most branch targets are close to their
origin. Following this, our key insight is that we can redesign the
BTB to store branch target offsets instead of the full target
addresses. In a Conv-BTB, each way is sized to hold a full target
address. In BTB-X we size the ways differently so that smaller branch
offset sizes can be stored without redundant space.

The resulting design improves the state of the art in terms of branch
storage density, lookup latency and power requirements. Since BTB-X
increases the branch working set coverage of the BTB it improves
instruction prefetching performance. Thereby, it helps alleviating the
front-end bottleneck and improves the performance of, particularly,
server workloads.

The motivating analysis is presented in Paper C (\Cref{chap:wosc-paper}).
BTB-X evolved over time and is described in papers A, B
and D (\Cref{chap:cal-paper,chap:pact-paper,chap:hpca-paper}
respectively), where the latter describes the full, final design.


\subsection{Research direction B: Software Approaches for Optimizing Serverless Computing}
\label{sec:pc3}

\section{Thesis Overview}
This thesis is structured in two parts. \Cref{part:one} introduces the
topic matter of the thesis and positions its
contribution. \Cref{cap:intro} introduces the topic of the thesis,
\Cref{chap:background} provides background material that aides
understanding and contextualizes the topics of the included paper,
\Cref{chap:rcontrib} summarizes the research contributions of the
thesis in more details and finally \Cref{chap:conclusions} presents
the thesis' conclusions. \Cref{part:two} contains the scientific
publications that collectively comprise the contributions of the
thesis.

\ifx\chapincluded\undefined
  \printbibliography
  \end{refsection}
 \fi

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End: