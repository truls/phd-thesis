% -*- mode:bibtex; eval: (bibtex-set-dialect 'biblatex); -*-

@article{kesteren08_infer_static_non_monot_size,
  author =       {Ron van Kesteren and Olha Shkaravska and Marko van
                  Eekelen},
  title =        {Inferring Static Non-Monotone Size-Aware Types
                  Through Testing},
  volume =       216,
  number =       {nil},
  pages =        {45-63},
  year =         2008,
  doi =          {10.1016/j.entcs.20033},
  url =          {https://doi.org/10.1016/j.entcs.20033},
  date_added =   {Tue Apr 9 15:53:21 2019},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
}

@inproceedings{kanev15_profil,
  author =       {Svilen Kanev and Juan Pablo Darago and Kim Hazelwood
                  and Parthasarathy Ranganathan and Tipp Moseley and
                  Gu-Yeon Wei and David Brooks},
  title =        {Profiling a warehouse-scale computer},
  booktitle =    {Proceedings of the 42nd Annual International
                  Symposium on Computer Architecture - ISCA '15},
  year =         2015,
  pages =        {158-169},
  doi =          {10.1145/2749469.2750392},
  url =          {https://doi.org/10.1145/2749469.2750392},
  date_added =   {Mon May 6 13:01:12 2019},
  numpages =     12,
}

@inproceedings{ajorpaz18_explor_predic_replac_polic_instr,
  author =       {Samira Mirbagher Ajorpaz and Elba Garza and Sangam
                  Jindal and Daniel A. Jimenez},
  title =        {Exploring Predictive Replacement Policies for
                  Instruction Cache and Branch Target Buffer},
  booktitle =    {2018 ACM/IEEE 45th Annual International Symposium on
                  Computer Architecture (ISCA)},
  year =         2018,
  doi =          {10.1109/isca.2018.00050},
  url =          {https://doi.org/10.1109/isca.2018.00050},
  date_added =   {Mon May 6 13:02:16 2019},
  month =        6,
}

@article{kaynak15_confl,
  author =       {Kaynak, Cansu and Grot, Boris and Falsafi, Babak},
  title =        {Confluence},
  year =         2015,
  doi =          {10.1145/2830772.2830785},
  url =          {https://doi.org/10.1145/2830772.2830785},
  isbn =         9781450340342,
  journaltitle = {Proceedings of the 48th International Symposium on
                  Microarchitecture - MICRO-48},
  publisher =    {ACM Press},
}

@article{ferdman12_clear_cloud,
  author =       {Ferdman, Michael and Falsafi, Babak and Adileh,
                  Almutaz and Kocberber, Onur and Volos, Stavros and
                  Alisafaee, Mohammad and Jevdjic, Djordje and Kaynak,
                  Cansu and Popescu, Adrian Daniel and Ailamaki,
                  Anastasia},
  title =        {Clearing the Clouds},
  volume =       40,
  number =       1,
  pages =        37,
  year =         2012,
  doi =          {10.1145/2189750.2150982},
  url =          {https://doi.org/10.1145/2189750.2150982},
  issn =         {0163-5964},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  month =        {Apr},
  publisher =    {Association for Computing Machinery (ACM)},
}

@article{falsafi14_primer_hardw_prefet,
  author =       {Falsafi, Babak and Wenisch, Thomas F.},
  title =        {A Primer on Hardware Prefetching},
  volume =       9,
  number =       1,
  pages =        {1-67},
  year =         2014,
  doi =          {10.2200/s00581ed1v01y201405cac028},
  url =          {https://doi.org/10.2200/s00581ed1v01y201405cac028},
  issn =         {1935-3243},
  journaltitle = {Synthesis Lectures on Computer Architecture},
  month =        {May},
  publisher =    {Morgan \& Claypool Publishers LLC},
}

@book{hennessy11_comput,
  author =       {Hennessy, John L and Patterson, David A},
  title =        {Computer architecture: a quantitative approach},
  year =         2011,
  publisher =    {Elsevier},
}

@book{patterson13_comput_organ_desig_mips_edition,
  author =       {Patterson, David A and Hennessy, John L},
  title =        {Computer Organization and Design MIPS Edition: The
                  Hardware/Software Interface},
  year =         2013,
  publisher =    {Newnes},
}

@inproceedings{ferdman08_tempor,
  author =       {Ferdman, Michael and Wenisch, Thomas F. and
                  Ailamaki, Anastasia and Falsafi, Babak and Moshovos,
                  Andreas},
  title =        {Temporal instruction fetch streaming},
  booktitle =    {2008 41st IEEE/ACM International Symposium on
                  Microarchitecture},
  year =         2008,
  pages =        {1-10},
  doi =          {10.1109/MICRO.2008.4771774},
  url =          {https://doi.org/10.1109/MICRO.2008.4771774},
  ISSN =         {2379-3155},
  abstract =     {L1 instruction-cache misses pose a critical
                  performance bottleneck in commercial server
                  workloads. Cache access latency constraints preclude
                  L1 instruction caches large enough to capture the
                  application, library, and OS instruction working
                  sets of these workloads. To cope with capacity
                  constraints, researchers have proposed instruction
                  prefetchers that use branch predictors to explore
                  future control flow. However, such prefetchers
                  suffer from several fundamental flaws: their
                  lookahead is limited by branch prediction bandwidth,
                  their accuracy suffers from
                  geometrically-compounding branch misprediction
                  probability, and they are ignorant of the cache
                  contents, frequently predicting blocks already
                  present in L1. Hence, L1 instruction misses remain a
                  bottleneck. We propose temporal instruction fetch
                  streaming (TIFS)-a mechanism for prefetching
                  temporally-correlated instruction streams from
                  lower-level caches. Rather than explore a
                  programpsilas control flow graph, TIFS predicts
                  future instruction-cache misses directly, through
                  recording and replaying recurring L1 instruction
                  miss sequences. In this paper, we first present an
                  information-theoretic offline trace analysis of
                  instruction-miss repetition to show that 94 \% of L1
                  instruction misses occur in long, recurring
                  sequences. Then, we describe a practical mechanism
                  to record these recurring sequences in the L2 cache
                  and leverage them for instruction-cache
                  prefetching. Our TIFS design requires less than 5 \%
                  storage overhead over the baseline L2 cache and
                  improves performance by 11 \% on average and 24 \%
                  at best in a suite of commercial server workloads.},
  month =        {Nov},
}

@inproceedings{ferdman11_proac_instr_fetch,
  author =       {Ferdman, Michael and Kaynak, Cansu and Falsafi,
                  Babak},
  title =        {Proactive Instruction Fetch},
  booktitle =    {Proceedings of the 44th Annual IEEE/ACM
                  International Symposium on Microarchitecture -
                  MICRO-44 '11},
  year =         2011,
  doi =          {10.1145/2155620.2155638},
  url =          {https://doi.org/10.1145/2155620.2155638},
  isbn =         9781450310536,
  publisher =    {ACM Press},
}

@inproceedings{kaynak13_shift,
  author =       {Cansu Kaynak and Boris Grot and Babak Falsafi},
  title =        {SHIFT},
  booktitle =    {Proceedings of the 46th Annual IEEE/ACM
                  International Symposium on Microarchitecture -
                  MICRO-46},
  year =         2013,
  doi =          {10.1145/2540708.2540732},
  url =          {https://doi.org/10.1145/2540708.2540732},
  date_added =   {Sun May 12 23:19:07 2019},
  subtitle =     {shared history instruction fetch for lean-core
                  server processors},
}

@article{jacob09_memor_system,
  author =       {Jacob, Bruce},
  title =        {The Memory System: You Can't Avoid It, You Can't
                  Ignore It, You Can't Fake It},
  volume =       4,
  number =       1,
  pages =        {1-77},
  year =         2009,
  doi =          {10.2200/s00201ed1v01y200907cac007},
  url =          {https://doi.org/10.2200/s00201ed1v01y200907cac007},
  issn =         {1935-3243},
  journaltitle = {Synthesis Lectures on Computer Architecture},
  month =        {Jan},
  publisher =    {Morgan \& Claypool Publishers LLC},
}

@inproceedings{reinman99_fetch_direc_instr_prefet,
  author =       {Reinman, G. and Calder, B. and Austin, T.},
  title =        {Fetch Directed Instruction Prefetching},
  booktitle =    {MICRO-32. Proceedings of the 32nd Annual ACM/IEEE
                  International Symposium on Microarchitecture},
  year =         1999,
  doi =          {10.1109/micro.1999.809439},
  url =          {https://doi.org/10.1109/micro.1999.809439},
  isbn =         {076950437X},
  publisher =    {IEEE Comput. Soc},
}

@inproceedings{aamodt04_hardw_suppor_presc_instr_prefet,
  author =       {Aamodt, T.M. and Chow, P. and Hammarlund, P. and
                  Hong Wang and Shen, J.P.},
  title =        {Hardware Support for Prescient Instruction Prefetch},
  booktitle =    {10th International Symposium on High Performance
                  Computer Architecture (HPCA'04)},
  year =         2004,
  doi =          {10.1109/hpca.2004.10028},
  url =          {https://doi.org/10.1109/hpca.2004.10028},
  isbn =         0769520537,
  publisher =    {IEEE},
}

@inproceedings{kumar19_freew,
  author =       {Kumar, Rakesh and Alipour, Mehdi and Black-Schaffer,
                  David},
  title =        {Freeway: Maximizing Mlp for Slice-Out-Of-Order
                  Execution},
  booktitle =    {2019 IEEE International Symposium on High
                  Performance Computer Architecture (HPCA)},
  year =         2019,
  doi =          {10.1109/hpca.2019.00009},
  url =          {https://doi.org/10.1109/hpca.2019.00009},
  isbn =         9781728114446,
  month =        {Feb},
  publisher =    {IEEE},
}

@inproceedings{kumar17_boomer,
  author =       {Kumar, Rakesh and Huang, Cheng-Chieh and Grot, Boris
                  and Nagarajan, Vijay},
  title =        {Boomerang: a Metadata-Free Architecture for Control
                  Flow Delivery},
  booktitle =    {2017 IEEE International Symposium on High
                  Performance Computer Architecture (HPCA)},
  year =         2017,
  doi =          {10.1109/hpca.2017.53},
  url =          {https://doi.org/10.1109/hpca.2017.53},
  isbn =         9781509049851,
  month =        {Feb},
  publisher =    {IEEE},
}

@article{kumar18_blast_throug_front_end_bottl_with_shotg,
  author =       {Kumar, Rakesh and Grot, Boris and Nagarajan, Vijay},
  title =        {Blasting Through the Front-End Bottleneck With
                  Shotgun},
  volume =       53,
  number =       2,
  pages =        {30-42},
  year =         2018,
  doi =          {10.1145/3296957.3173178},
  url =          {https://doi.org/10.1145/3296957.3173178},
  issn =         {0362-1340},
  journaltitle = {ACM SIGPLAN Notices},
  month =        {Mar},
  publisher =    {Association for Computing Machinery (ACM)},
}

@article{barroso18_datac_as_comput,
  author =       {Luiz Andr{\'e} Barroso and Urs H{\"o}lzle and
                  Parthasarathy Ranganathan},
  title =        {The Datacenter As a Computer: Designing
                  Warehouse-Scale Machines, Third Edition},
  volume =       13,
  number =       3,
  pages =        {i-189},
  year =         2018,
  doi =          {10.2200/s00874ed3v01y201809cac046},
  url =          {https://doi.org/10.2200/s00874ed3v01y201809cac046},
  date_added =   {Tue May 14 13:48:44 2019},
  journaltitle = {Synthesis Lectures on Computer Architecture},
}

@inproceedings{margaritov19_stret,
  author =       {Margaritov, Artemiy and Gupta, Siddharth and
                  Gonzalez-Alberquilla, Rekai and Grot, Boris},
  title =        {Stretch: Balancing Qos and Throughput for Colocated
                  Server Workloads on Smt Cores},
  booktitle =    {2019 IEEE International Symposium on High
                  Performance Computer Architecture (HPCA)},
  year =         2019,
  doi =          {10.1109/hpca.2019.00024},
  url =          {https://doi.org/10.1109/hpca.2019.00024},
  isbn =         9781728114446,
  month =        {Feb},
  publisher =    {IEEE},
}

@article{hardavellas04_simfl,
  author =       {Hardavellas, Nikolaos and Somogyi, Stephen and
                  Wenisch, Thomas F. and Wunderlich, Roland E. and
                  Chen, Shelley and Kim, Jangwoo and Falsafi, Babak
                  and Hoe, James C. and Nowatzyk, Andreas G.},
  title =        {Simflex},
  volume =       31,
  number =       4,
  pages =        {31-34},
  year =         2004,
  doi =          {10.1145/1054907.1054914},
  url =          {https://doi.org/10.1145/1054907.1054914},
  issn =         {0163-5999},
  journaltitle = {ACM SIGMETRICS Performance Evaluation Review},
  month =        {Mar},
  publisher =    {Association for Computing Machinery (ACM)},
  subtitle =     {A Fast, Accurate, Flexible Full-System Simulation
                  Framework for Performance Evaluation of Server
                  Architecture},
}

@article{wenisch06_simfl,
  author =       {Wenisch, T.F. and Wunderlich, R.E. and Ferdman,
                  M. and Ailamaki, A. and Falsafi, B. and Hoe, J.C.},
  title =        {Simflex: Statistical Sampling of Computer System
                  Simulation},
  volume =       26,
  number =       4,
  pages =        {18-31},
  year =         2006,
  doi =          {10.1109/mm.2006.79},
  url =          {https://doi.org/10.1109/mm.2006.79},
  issn =         {0272-1732},
  journaltitle = {IEEE Micro},
  month =        {Jul},
  publisher =    {Institute of Electrical and Electronics Engineers
                  (IEEE)},
}

@article{angepat14_fpga_accel_simul_comput_system,
  author =       {Hari Angepat and Derek Chiou and Eric S. Chung and
                  James C. Hoe},
  title =        {Fpga-Accelerated Simulation of Computer Systems},
  volume =       9,
  number =       2,
  pages =        {1-80},
  year =         2014,
  doi =          {10.2200/s00586ed1v01y201407cac029},
  url =          {https://doi.org/10.2200/s00586ed1v01y201407cac029},
  date_added =   {Tue May 14 14:55:06 2019},
  journaltitle = {Synthesis Lectures on Computer Architecture},
}

@inproceedings{kroft81_lockup_instr_fetch_prefet_cache_organ,
  author =       {Kroft, David},
  title =        {Lockup-free Instruction Fetch/Prefetch Cache
                  Organization},
  booktitle =    {Proceedings of the 8th Annual Symposium on Computer
                  Architecture},
  year =         1981,
  pages =        {81--87},
  url =          {http://dl.acm.org/citation.cfm?id=800052.801868},
  acmid =        801868,
  address =      {Los Alamitos, CA, USA},
  location =     {Minneapolis, Minnesota, USA},
  numpages =     7,
  publisher =    {IEEE Computer Society Press},
  series =       {ISCA '81},
}

@inproceedings{ramey11_tile_gx100_manyc,
  author =       {Carl Ramey},
  title =        {TILE-Gx100 ManyCore processor: Acceleration
                  interfaces and architecture},
  booktitle =    {2011 IEEE Hot Chips 23 Symposium (HCS)},
  year =         2011,
  doi =          {10.1109/hotchips.2011.7477491},
  url =          {https://doi.org/10.1109/hotchips.2011.7477491},
  date_added =   {Tue May 21 14:58:38 2019},
  month =        8,
}

@article{fuchs19_accel_wall,
  author =       {Fuchs, Adi and Wentzlaff, David},
  title =        {The Accelerator Wall: Limits of Chip Specialization},
  year =         2019,
  doi =          {10.1109/hpca.2019.00023},
  url =          {https://doi.org/10.1109/hpca.2019.00023},
  isbn =         9781728114446,
  journaltitle = {2019 IEEE International Symposium on High
                  Performance Computer Architecture (HPCA)},
  month =        {Feb},
  publisher =    {IEEE},
}

@article{seznec06_case_tagged_geomet_histor_lengt_branc_predic,
  author =       {Andr{\'e} Seznec and Pierre Michaud},
  title =        {A Case for (partially) Tagged Geometric History
                  Length Branch Prediction.},
  volume =       8,
  year =         2006,
  journaltitle = {J. Instruction-Level Parallelism},
}

@article{dennard99_desig_ion_implan_mosfet_with,
  author =       {Dennard, R.H. and Gaensslen, F.H. and Hwa-Nien Yu
                  and Rideout, V.L. and Bassous, E. and Leblanc, A.R.},
  title =        {Design of Ion-Implanted Mosfet's With Very Small
                  Physical Dimensions},
  volume =       87,
  number =       4,
  pages =        {668-678},
  year =         1999,
  doi =          {10.1109/jproc.1999.752522},
  url =          {https://doi.org/10.1109/jproc.1999.752522},
  issn =         {1558-2256},
  journaltitle = {Proceedings of the IEEE},
  month =        4,
  publisher =    {Institute of Electrical and Electronics Engineers
                  (IEEE)},
}

@article{schaller97_moores_law,
  author =       {Schaller, R.R.},
  title =        {Moore's Law: Past, Present and Future},
  volume =       34,
  number =       6,
  pages =        {52-59},
  year =         1997,
  doi =          {10.1109/6.591665},
  url =          {https://doi.org/10.1109/6.591665},
  issn =         {0018-9235},
  journaltitle = {IEEE Spectrum},
  month =        6,
  publisher =    {Institute of Electrical and Electronics Engineers
                  (IEEE)},
}

@inproceedings{ailamaki99_dbmss_moder_proces,
  author =       {Ailamaki, Anastassia and DeWitt, David J. and Hill,
                  Mark D. and Wood, David A.},
  title =        {DBMSs on a Modern Processor: Where Does Time Go?},
  booktitle =    {Proceedings of the 25th International Conference on
                  Very Large Data Bases},
  year =         1999,
  pages =        {266--277},
  url =          {http://dl.acm.org/citation.cfm?id=645925.671662},
  acmid =        671662,
  address =      {San Francisco, CA, USA},
  isbn =         {1-55860-615-7},
  numpages =     12,
  publisher =    {Morgan Kaufmann Publishers Inc.},
  series =       {VLDB '99},
}

@article{jaleel15_high_perfor_cache_hierar_server_workl,
  author =       {Jaleel, Aamer and Nuzman, Joseph and Moga, Adrian
                  and Steely, Simon C. and Emer, Joel},
  title =        {High Performing Cache Hierarchies for Server
                  Workloads: Relaxing Inclusion To Capture the Latency
                  Benefits of Exclusive Caches},
  year =         2015,
  doi =          {10.1109/hpca.2015.7056045},
  url =          {https://doi.org/10.1109/hpca.2015.7056045},
  isbn =         9781479989300,
  journaltitle = {2015 IEEE 21st International Symposium on High
                  Performance Computer Architecture (HPCA)},
  month =        {Feb},
  publisher =    {IEEE},
}

@inproceedings{jaleel10_high_perfor_cache_replac_using,
  author =       {Jaleel, Aamer and Theobald, Kevin B. and Steely,Jr.,
                  Simon C. and Emer, Joel},
  title =        {High Performance Cache Replacement Using
                  Re-reference Interval Prediction (RRIP)},
  booktitle =    {Proceedings of the 37th Annual International
                  Symposium on Computer Architecture},
  year =         2010,
  pages =        {60--71},
  doi =          {10.1145/1815961.1815971},
  url =          {https://doi.org/10.1145/1815961.1815971},
  acmid =        1815971,
  address =      {New York, NY, USA},
  isbn =         {978-1-4503-0053-7},
  keywords =     {replacement, scan resistance, shared cache,
                  thrashing},
  location =     {Saint-Malo, France},
  numpages =     12,
  publisher =    {ACM},
  series =       {ISCA '10},
}

@inproceedings{gan19_open_sourc_bench_suite_micros,
  author =       {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and
                  Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan
                  and Bruno, Ariana and Hu, Justin and Ritchken, Brian
                  and Jackson, Brendon and Hu, Kelvin and Pancholi,
                  Meghna and He, Yuan and Clancy, Brett and Colen,
                  Chris and Wen, Fukang and Leung, Catherine and Wang,
                  Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and
                  Lin, Rick and Liu, Zhongling and Padilla, Jake and
                  Delimitrou, Christina},
  title =        {An Open-Source Benchmark Suite for Microservices and
                  Their Hardware-Software Implications for Cloud
                  \& Edge Systems},
  booktitle =    {Proceedings of the Twenty-Fourth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems},
  year =         2019,
  pages =        {3--18},
  doi =          {10.1145/3297858.3304013},
  url =          {https://doi.org/10.1145/3297858.3304013},
  acmid =        3304013,
  address =      {New York, NY, USA},
  isbn =         {978-1-4503-6240-5},
  keywords =     {acceleration, cloud computing, cluster management,
                  datacenters, fpga, microservices, qos, serverless},
  location =     {Providence, RI, USA},
  numpages =     16,
  publisher =    {ACM},
  series =       {ASPLOS '19},
}

@inproceedings{zhu15_microar_implic_event_server_web_applic,
  author =       {Zhu, Yuhao and Richins, Daniel and Halpern, Matthew
                  and Reddi, Vijay Janapa},
  title =        {Microarchitectural Implications of Event-driven
                  Server-side Web Applications},
  booktitle =    {Proceedings of the 48th International Symposium on
                  Microarchitecture},
  year =         2015,
  pages =        {762--774},
  doi =          {10.1145/2830772.2830792},
  url =          {https://doi.org/10.1145/2830772.2830792},
  acmid =        2830792,
  address =      {New York, NY, USA},
  isbn =         {978-1-4503-4034-2},
  keywords =     {JavaScript, event-driven, microarchitecture,
                  prefetcher},
  location =     {Waikiki, Hawaii},
  numpages =     13,
  publisher =    {ACM},
  series =       {MICRO-48},
}

@inproceedings{qureshi07_adapt_inser_polic_high_perfor_cachin,
  author =       {Qureshi, Moinuddin K. and Jaleel, Aamer and Patt,
                  Yale N. and Steely, Simon C. and Emer, Joel},
  title =        {Adaptive Insertion Policies for High Performance
                  Caching},
  booktitle =    {Proceedings of the 34th Annual International
                  Symposium on Computer Architecture},
  year =         2007,
  pages =        {381--391},
  doi =          {10.1145/1250662.1250709},
  url =          {https://doi.org/10.1145/1250662.1250709},
  acmid =        1250709,
  address =      {New York, NY, USA},
  isbn =         {978-1-59593-706-3},
  keywords =     {replacement, set dueling, set sampling, thrashing},
  location =     {San Diego, California, USA},
  numpages =     11,
  publisher =    {ACM},
  series =       {ISCA '07},
}

@article{sanchez11_vantag,
  author =       {Sanchez, Daniel and Kozyrakis, Christos},
  title =        {Vantage: Scalable and Efficient Fine-Grain Cache
                  Partitioning},
  volume =       39,
  number =       3,
  pages =        {57--68},
  year =         2011,
  doi =          {10.1145/2024723.2000073},
  url =          {https://doi.org/10.1145/2024723.2000073},
  acmid =        2000073,
  address =      {New York, NY, USA},
  issn =         {0163-5964},
  issue_date =   {June 2011},
  journaltitle = {SIGARCH Comput. Archit. News},
  keywords =     {cache partitioning, multi-core, qos, shared cache},
  month =        jun,
  numpages =     12,
  publisher =    {ACM},
}

@inproceedings{sanchez10_zcach,
  author =       {Sanchez, Daniel and Kozyrakis, Christos},
  title =        {The ZCache: Decoupling Ways and Associativity},
  booktitle =    {Proceedings of the 2010 43rd Annual IEEE/ACM
                  International Symposium on Microarchitecture},
  year =         2010,
  pages =        {187--198},
  doi =          {10.1109/MICRO.2010.20},
  url =          {https://doi.org/10.1109/MICRO.2010.20},
  acmid =        1935021,
  address =      {Washington, DC, USA},
  isbn =         {978-0-7695-4299-7},
  keywords =     {cache, associativity, performance, multi-core,
                  energy efficiency},
  numpages =     12,
  publisher =    {IEEE Computer Society},
  series =       {MICRO '43},
}

@inproceedings{khan10_sampl_dead_block_predic_last_level_caches,
  author =       {Khan, Samira Manabi and Tian, Yingying and Jimenez,
                  Daniel A.},
  title =        {Sampling Dead Block Prediction for Last-Level
                  Caches},
  booktitle =    {Proceedings of the 2010 43rd Annual IEEE/ACM
                  International Symposium on Microarchitecture},
  year =         2010,
  pages =        {175--186},
  doi =          {10.1109/MICRO.2010.24},
  url =          {https://doi.org/10.1109/MICRO.2010.24},
  acmid =        1934977,
  address =      {Washington, DC, USA},
  isbn =         {978-0-7695-4299-7},
  keywords =     {microarchitecture, cache, dead block prediction},
  numpages =     12,
  publisher =    {IEEE Computer Society},
  series =       {MICRO '43},
}

@inproceedings{keramidas07_cache,
  author =       {G. {Keramidas} and P. {Petoumenos} and S. {Kaxiras}},
  title =        {Cache replacement based on reuse-distance
                  prediction},
  booktitle =    {2007 25th International Conference on Computer
                  Design},
  year =         2007,
  pages =        {245-250},
  doi =          {10.1109/ICCD.2007.4601909},
  url =          {https://doi.org/10.1109/ICCD.2007.4601909},
  abstract =     {Several cache management techniques have been
                  proposed that indirectly try to base their decisions
                  on cacheline reuse-distance, like Cache Decay which
                  is a postdiction of reuse-distances: if a cacheline
                  has not been accessed for some ldquodecay
                  intervalrdquo we know that its reuse-distance is at
                  least as large as this decay interval. In this work,
                  we propose to directly predict reuse-distances via
                  instruction-based (PC) prediction and use this
                  information for cache level optimizations. In this
                  paper, we choose as our target for optimization the
                  replacement policy of the L2 cache, because the gap
                  between the LRU and the theoretical optimal
                  replacement algorithm is comparatively large for L2
                  caches. This indicates that, in many situations,
                  there is ample room for improvement. We evaluate our
                  reusedistance based replacement policy using a
                  subset of the most memory intensive SPEC2000 and our
                  results show significant benefits across the board.},
  keywords =     {cache storage;storage management;cache
                  replacement;reuse-distance prediction;cache
                  management techniques;cache decay;instruction-based
                  prediction;cache level optimizations;replacement
                  policy;LRU;SPEC2000},
  month =        {Oct},
}

@inproceedings{jimenez13_inser_pseud,
  author =       {D. A. {Jim{\'e}nez}},
  title =        {Insertion and promotion for tree-based PseudoLRU
                  last-level caches},
  booktitle =    {2013 46th Annual IEEE/ACM International Symposium on
                  Microarchitecture (MICRO)},
  year =         2013,
  pages =        {284-296},
  url =          {https://doi.org/},
  keywords =     {cache storage;trees (mathematics);tree-based
                  PseudoLRU;main memory;cache replacement
                  policy;memory intensive programs;least-recently-used
                  replacement policy;LRU replacement policy;last-level
                  cache replacement algorithm;storage
                  requirements;dynamic rereference interval
                  prediction;DRRIP;protecting distance
                  policy;PDP;set-dueling;insertion policy;promotion
                  policy;cache block;program behavior;set-associative
                  last-level cache;Benchmark testing;Heuristic
                  algorithms;Space exploration;Genetic
                  algorithms;Prediction algorithms;Context;Complexity
                  theory},
  month =        {Dec},
}

@inproceedings{khan14_improv,
  author =       {S. {Khan} and A. R. {Alameldeen} and C. {Wilkerson}
                  and O. {Mutluy} and D. A. {Jimenezz}},
  title =        {Improving cache performance using read-write
                  partitioning},
  booktitle =    {2014 IEEE 20th International Symposium on High
                  Performance Computer Architecture (HPCA)},
  year =         2014,
  pages =        {452-463},
  doi =          {10.1109/HPCA.2014.6835954},
  url =          {https://doi.org/10.1109/HPCA.2014.6835954},
  abstract =     {Cache read misses stall the processor if there are
                  no independent instructions to execute. In contrast,
                  most cache write misses are off the critical path of
                  execution, since writes can be buffered in the cache
                  or the store buffer. With few exceptions, cache
                  lines that serve loads are more critical for
                  performance than cache lines that serve only
                  stores. Unfortunately, traditional cache management
                  mechanisms do not take into account this disparity
                  between read-write criticality. This paper proposes
                  a Read-Write Partitioning (RWP) policy that
                  minimizes read misses by dynamically partitioning
                  the cache into clean and dirty partitions, where
                  partitions grow in size if they are more likely to
                  receive future read requests. We show that
                  exploiting the differences in read-write criticality
                  provides better performance over prior cache
                  management mechanisms. For a single-core system, RWP
                  provides 5 \% average speedup across the entire SPEC
                  CPU2006 suite, and 14 \% average speedup for
                  cache-sensitive benchmarks, over the baseline LRU
                  replacement policy. We also show that RWP can
                  perform within 3 \% of a new yet complex
                  instruction-address-based technique, Read Reference
                  Predictor (RRP), that bypasses cache lines which are
                  unlikely to receive any read requests, while
                  requiring only 5.4 \% of RRP's state overhead. On a
                  4-core system, our RWP mechanism improves system
                  throughput by 6 \% over the baseline and outperforms
                  three other state-of-the-art mechanisms we
                  evaluate.},
  keywords =     {cache storage;critical path analysis;cache
                  performance;read-write partitioning;critical
                  path;store buffer;cache management;single-core
                  system;read reference predictor;RRP;Benchmark
                  testing;Buffer storage;Prediction
                  algorithms;Resource management;Memory
                  management;Educational institutions;Throughput},
  month =        {Feb},
}

@inproceedings{jain16_back_futur,
  author =       {A. {Jain} and C. {Lin}},
  title =        {Back to the Future: Leveraging Belady's Algorithm
                  for Improved Cache Replacement},
  booktitle =    {2016 ACM/IEEE 43rd Annual International Symposium on
                  Computer Architecture (ISCA)},
  year =         2016,
  pages =        {78-89},
  doi =          {10.1109/ISCA.2016.17},
  url =          {https://doi.org/10.1109/ISCA.2016.17},
  abstract =     {Belady's algorithm is optimal but infeasible because
                  it requires knowledge of the future. This paper
                  explains how a cache replacement algorithm can
                  nonetheless learn from Belady's algorithm by
                  applying it to past cache accesses to inform future
                  cache replacement decisions. We show that the
                  implementation is surprisingly efficient, as we
                  introduce a new method of efficiently simulating
                  Belady's behavior, and we use known sampling
                  techniques to compactly represent the long history
                  information that is needed for high accuracy. For a
                  2MB LLC, our solution uses a 16KB hardware budget
                  (excluding replacement state in the tag array). When
                  applied to a memory-intensive subset of the SPEC
                  2006 CPU benchmarks, our solution improves
                  performance over LRU by 8.4 \%, as opposed to 6.2 \%
                  for the previous state-of-the-art. For a 4-core
                  system with a shared 8MB LLC, our solution improves
                  performance by 15.0 \%, compared to 12.0 \% for the
                  previous state-of-the-art.},
  keywords =     {cache storage;Belady algorithm;cache
                  replacement;cache accesses;Belady behavior;sampling
                  techniques;LLC;replacement state;tag
                  array;memory-intensive subset;4-core
                  system;Optimized production
                  technology;History;Prediction algorithms;Marine
                  vehicles;Benchmark testing;Hardware;Art;Cache
                  replacement;Belady's Algorithm},
  month =        {June},
}

@inproceedings{young17_ship,
  author =       {Vinson Young and Chia-Chen Chou and Aamer Jaleel and
                  Moinuddin K. Qureshi},
  title =        {SHiP++: Enhancing Signature-Based Hit Predictor for
                  Improved Cache Performance},
  year =         {2017},
}

@inproceedings{lo15_herac,
  author =       {Lo, David and Cheng, Liqun and Govindaraju, Rama and
                  Ranganathan, Parthasarathy and Kozyrakis, Christos},
  title =        {Heracles: Improving Resource Efficiency at Scale},
  booktitle =    {Proceedings of the 42Nd Annual International
                  Symposium on Computer Architecture},
  year =         2015,
  pages =        {450--462},
  doi =          {10.1145/2749469.2749475},
  url =          {https://doi.org/10.1145/2749469.2749475},
  acmid =        2749475,
  address =      {New York, NY, USA},
  isbn =         {978-1-4503-3402-0},
  location =     {Portland, Oregon},
  numpages =     13,
  publisher =    {ACM},
  series =       {ISCA '15},
}

@inproceedings{reinman99,
  author =       {G. {Reinman} and T. {Anstin} and B. {Calder}},
  title =        {A scalable front-end architecture for fast
                  instruction delivery},
  booktitle =    {Proceedings of the 26th International Symposium on
                  Computer Architecture (Cat. No.99CB36367)},
  year =         1999,
  pages =        {234-245},
  doi =          {10.1109/ISCA.1999.765954},
  url =          {https://doi.org/10.1109/ISCA.1999.765954},
  keywords =     {parallel architectures;instruction sets;circuit
                  analysis computing;VLSI;scalable front-end
                  architecture;fast instruction
                  delivery;instruction-level parallelism;instruction
                  delivery mechanism;I-cache misses;branch
                  mispredictions;VLSI interconnect scaling;fetch
                  block-oriented predictor;circuit level delay
                  analysis;Pipelines;Very large scale
                  integration;Integrated circuit
                  interconnections;Counting
                  circuits;Decoding;Clocks;Analytical models;Circuit
                  simulation;Delay;Circuit analysis},
  month =        {May},
}

@inproceedings{yeh92_compr_instr_fetch_mechan_for,
  author =       {{Tse-Yu Yeh} and Y. N. {Patt}},
  title =        {A Comprehensive Instruction Fetch Mechanism For A
                  Processor Supporting Speculative Execution},
  booktitle =    {[1992] Proceedings the 25th Annual International
                  Symposium on Microarchitecture MICRO 25},
  year =         1992,
  pages =        {129-139},
  doi =          {10.1109/MICRO.1992.697009},
  url =          {https://doi.org/10.1109/MICRO.1992.697009},
  keywords =     {Delay;Pipelines;Performance evaluation;Performance
                  loss;Accuracy;Algorithms;Analytical
                  models;Electrical capacitance tomography},
  month =        {Dec},
}

@inproceedings{manolopoulos10_multip_add_fused_unit,
  author =       {K. {Manolopoulos} and D. {Reisis} and
                  V. A. {Chouliaras}},
  title =        {An efficient dual-mode floating-point Multiply-Add
                  Fused Unit},
  booktitle =    {2010 17th IEEE International Conference on
                  Electronics, Circuits and Systems},
  year =         2010,
  pages =        {5-8},
  doi =          {10.1109/ICECS.2010.5724440},
  url =          {https://doi.org/10.1109/ICECS.2010.5724440},
  issn =         {null},
  keywords =     {floating point arithmetic;microprocessor
                  chips;multiplying circuits;efficient dual-mode
                  floating-point;multiply-add fused unit;MAF
                  units;processor performance;MAF
                  functionality;dual-mode MAF
                  architecture;double-precision
                  operations;single-precision operations;dual-path
                  approach;floating-point addition;TSMC;size 0.13
                  mum;Lead},
  month =        {Dec},
}

@inproceedings{soares10_flexs,
  author =       {Soares, Livio and Stumm, Michael},
  title =        {FlexSC: Flexible System Call Scheduling with
                  Exception-Less System Calls},
  booktitle =    {Proceedings of the 9th USENIX Conference on
                  Operating Systems Design and Implementation},
  year =         2010,
  pages =        {33-46},
  address =      {USA},
  location =     {Vancouver, BC, Canada},
  numpages =     14,
  publisher =    {USENIX Association},
  series =       {OSDI'10},
}

@inproceedings{padmanabha17_mirag_cores,
  author =       {Padmanabha, Shruti and Lukefahr, Andrew and Das,
                  Reetuparna and Mahlke, Scott},
  title =        {Mirage Cores: The Illusion of Many out-of-Order
                  Cores Using in-Order Hardware},
  booktitle =    {Proceedings of the 50th Annual IEEE/ACM
                  International Symposium on Microarchitecture},
  year =         2017,
  pages =        {745-758},
  doi =          {10.1145/3123939.3123969},
  url =          {https://doi.org/10.1145/3123939.3123969},
  address =      {New York, NY, USA},
  isbn =         9781450349529,
  keywords =     {energy-efficient architectures, CMP scheduling,
                  heterogeneous multicores},
  location =     {Cambridge, Massachusetts},
  numpages =     14,
  publisher =    {Association for Computing Machinery},
  series =       {MICRO-50 '17},
}

@misc{ban19_coales_tlb_exploit_diver_contig_memor_mappin,
  author =       {Yikun Ban and Yuchen Zhou and Xu Cheng and Jiangfang
                  Yi},
  month =        12,
  notes =        {arXiv preprint:
                  https://arxiv.org/pdf/1908.08774.pdf},
  title =        {Coalesced TLB to Exploit Diverse Contiguity of
                  Memory Mapping},
  year =         2019,
}

@article{barroso03_web_searc_planet,
  author =       {Luiz Andre Barroso and Jeffrey Dean and Urs
                  H{\"o}lzle},
  title =        {Web Search for a Planet: the Google Cluster
                  Architecture},
  volume =       23,
  pages =        {22-28},
  year =         2003,
  journaltitle = {IEEE Micro},
}

@article{dean13_tail_at_scale,
  author =       {Jeffrey Dean and Luiz Andr{\'e} Barroso},
  title =        {The Tail At Scale},
  volume =       56,
  pages =        {74-80},
  year =         2013,
  url =
                  {http://cacm.acm.org/magazines/2013/2/160173-the-tail-at-scale/fulltext},
  journaltitle = {Communications of the ACM},
}

@inproceedings{shahrad19_archit_implic_funct_servic_comput,
  author =       {Shahrad, Mohammad and Balkind, Jonathan and
                  Wentzlaff, David},
  title =        {Architectural Implications of Function-as-a-Service
                  Computing},
  booktitle =    {Proceedings of the 52nd Annual IEEE/ACM
                  International Symposium on Microarchitecture},
  year =         2019,
  pages =        {1063-1075},
  doi =          {10.1145/3352460.3358296},
  url =          {https://doi.org/10.1145/3352460.3358296},
  abstract =     {Serverless computing is a rapidly growing cloud
                  application model, popularized by Amazon's Lambda
                  platform. Serverless cloud services provide
                  fine-grained provisioning of resources, which scale
                  automatically with user
                  demand. Function-as-a-Service (FaaS) applications
                  follow this serverless model, with the developer
                  providing their application as a set of functions
                  which are executed in response to a user- or
                  system-generated event. Functions are designed to be
                  short-lived and execute inside containers or virtual
                  machines, introducing a range of system-level
                  overheads. This paper studies the architectural
                  implications of this emerging paradigm. Using the
                  commercial-grade Apache OpenWhisk FaaS platform on
                  real servers, this work investigates and identifies
                  the architectural implications of FaaS serverless
                  computing. The workloads, along with the way that
                  FaaS inherently interleaves short functions from
                  many tenants frustrates many of the
                  locality-preserving architectural structures common
                  in modern processors. In particular, we find that:
                  FaaS containerization brings up to 20x slowdown
                  compared to native execution, cold-start can be over
                  10x a short function's execution time, branch
                  mispredictions per kilo-instruction are 20x higher
                  for short functions, memory bandwidth increases by
                  6x due to the invocation pattern, and IPC decreases
                  by as much as 35 \% due to inter-function
                  interference. We open-source FaaSProfiler, the FaaS
                  testing and profiling platform that we developed for
                  this work.},
  address =      {New York, NY, USA},
  isbn =         9781450369381,
  keywords =     {serverless, function-as-a-service, faas, cloud,
                  architecture, OpenWhisk},
  location =     {Columbus, OH, USA},
  numpages =     13,
  publisher =    {Association for Computing Machinery},
  series =       {MICRO '52},
}

@inproceedings{mcgrath17_server_comput,
  author =       {G. {McGrath} and P. R. {Brenner}},
  title =        {Serverless Computing: Design, Implementation, and
                  Performance},
  booktitle =    {2017 IEEE 37th International Conference on
                  Distributed Computing Systems Workshops (ICDCSW)},
  year =         2017,
  pages =        {405-410},
  doi =          {10.1109/ICDCSW.2017.36},
  url =          {https://doi.org/10.1109/ICDCSW.2017.36},
  ISSN =         {2332-5666},
  abstract =     {We present the design of a novel
                  performance-oriented serverless computing platform
                  implemented in. NET, deployed in Microsoft Azure,
                  and utilizing Windows containers as function
                  execution environments. Implementation challenges
                  such as function scaling and container discovery,
                  lifecycle, and reuse are discussed in detail. We
                  propose metrics to evaluate the execution
                  performance of serverless platforms and conduct
                  tests on our prototype as well as AWS Lambda, Azure
                  Functions, Google Cloud Functions, and IBM's
                  deployment of Apache OpenWhisk. Our measurements
                  show the prototype achieving greater throughput than
                  other platforms at most concurrency levels, and we
                  examine the scaling and instance expiration trends
                  in the implementations. Additionally, we discuss the
                  gaps and limitations in our current design, propose
                  possible solutions, and highlight future research.},
  keywords =     {cloud computing;concurrency (computers);Microsoft
                  Windows (operating systems);performance
                  evaluation;performance-oriented serverless
                  computing;.NET;Microsoft Azure;Windows
                  containers;function execution environments;execution
                  performance evaluation;AWS Lambda;Azure
                  Functions;Google Cloud Functions;IBM's
                  deployment;Apache OpenWhisk;concurrency
                  levels;Containers;Web services;Metadata;Resource
                  management;Prototypes;Runtime;Google;serverless
                  computing;serverless
                  performance;FaaS;Function-as-a-Service;AWS
                  Lambda;Azure Functions;Google Cloud Functions;Apache
                  OpenWhisk;IBM OpenWhisk},
  month =        {June},
}

@inproceedings{lloyd18_server_comput,
  author =       {W. {Lloyd} and S. {Ramesh} and S. {Chinthalapati}
                  and L. {Ly} and S. {Pallickara}},
  title =        {Serverless Computing: An Investigation of Factors
                  Influencing Microservice Performance},
  booktitle =    {2018 IEEE International Conference on Cloud
                  Engineering (IC2E)},
  year =         2018,
  pages =        {159-169},
  doi =          {10.1109/IC2E.2018.00039},
  url =          {https://doi.org/10.1109/IC2E.2018.00039},
  abstract =     {Serverless computing platforms provide
                  function(s)-as-a-Service (FaaS) to end users while
                  promising reduced hosting costs, high availability,
                  fault tolerance, and dynamic elasticity for hosting
                  individual functions known as
                  microservices. Serverless Computing environments,
                  unlike Infrastructure-as-a-Service (IaaS) cloud
                  platforms, abstract infrastructure management
                  including creation of virtual machines (VMs),
                  operating system containers, and request load
                  balancing from users. To conserve cloud server
                  capacity and energy, cloud providers allow hosting
                  infrastructure to go COLD, deprovisioning containers
                  when service demand is low freeing infrastructure to
                  be harnessed by others. In this paper, we present
                  results from our comprehensive investigation into
                  the factors which influence microservice performance
                  afforded by serverless computing. We examine hosting
                  implications related to infrastructure elasticity,
                  load balancing, provisioning variation,
                  infrastructure retention, and memory reservation
                  size. We identify four states of serverless
                  infrastructure including: provider cold, VM cold,
                  container cold, and warm and demonstrate how
                  microservice performance varies up to 15x based on
                  these states.},
  keywords =     {cloud computing;resource allocation;virtual
                  machines;serverless computing platforms;hosting
                  costs;fault tolerance;dynamic elasticity;individual
                  functions;microservices;virtual machines;operating
                  system containers;cloud server capacity;cloud
                  providers;service demand;influence microservice
                  performance;infrastructure elasticity;load
                  balancing;infrastructure retention;serverless
                  infrastructure
                  including;infrastructure-as-a-service;infrastructure
                  management;serverless computing
                  environments;FaaS;IaaS;VM;Containers;Cloud
                  computing;Load
                  management;Elasticity;Servers;Operating
                  systems;Fault tolerance;Resource Management and
                  Performance;Serverless
                  Computing;Function-as-a-Service;Provisioning
                  Variation},
  month =        {April},
}

@inproceedings{wang18_peekin_behin_curtain_server_platf,
  author =       {Liang Wang and Mengyuan Li and Yinqian Zhang and
                  Thomas Ristenpart and Michael Swift},
  title =        {Peeking Behind the Curtains of Serverless Platforms},
  booktitle =    {2018 {USENIX} Annual Technical Conference ({USENIX}
                  {ATC} 18)},
  year =         2018,
  pages =        {133--146},
  url =
                  {https://www.usenix.org/conference/atc18/presentation/wang-liang},
  address =      {Boston, MA},
  isbn =         {ISBN 978-1-939133-01-4},
  month =        jul,
  publisher =    {{USENIX} Association},
}

@inproceedings{lee18_evaluat_produc_server_comput_envir,
  author =       {H. {Lee} and K. {Satyam} and G. {Fox}},
  title =        {Evaluation of Production Serverless Computing
                  Environments},
  booktitle =    {2018 IEEE 11th International Conference on Cloud
                  Computing (CLOUD)},
  year =         2018,
  pages =        {442-450},
  doi =          {10.1109/CLOUD.2018.00062},
  url =          {https://doi.org/10.1109/CLOUD.2018.00062},
  ISSN =         {2159-6190},
  abstract =     {Serverless computing provides a small runtime
                  container to execute lines of codes without
                  infrastructure management which is similar to
                  Platform as a Service (PaaS) but a functional
                  level. Amazon started the event-driven compute named
                  Lambda functions in 2014 with a 25 concurrent
                  limitation, but it now supports at least a thousand
                  of concurrent invocation to process event messages
                  generated by resources like databases, storage and
                  system logs. Other providers, i.e., Google,
                  Microsoft, and IBM offer a dynamic scaling manager
                  to handle parallel requests of stateless functions
                  in which additional containers are provisioning on
                  new compute nodes for distribution. However, while
                  functions are often developed for microservices and
                  lightweight workload, they are associated with
                  distributed data processing using the concurrent
                  invocations. We claim that the current serverless
                  computing environments can support dynamic
                  applications in parallel when a partitioned task is
                  executable on a small function instance. We present
                  results of throughput, network bandwidth, a file I/O
                  and compute performance regarding the concurrent
                  invocations. We deployed a series of functions for
                  distributed data processing to address the
                  elasticity and then demonstrated the differences
                  between serverless computing and virtual machines
                  for cost efficiency and resource utilization.},
  keywords =     {cloud computing;virtual machines;distributed data
                  processing;stateless functions;parallel
                  requests;dynamic scaling manager;concurrent
                  invocation;Lambda functions;event-driven
                  compute;infrastructure management;production
                  serverless computing
                  environments;Google;Throughput;Cloud
                  computing;Databases;Containers;Runtime;Data
                  processing;FaaS, Serverless, Event-driven Computing,
                  Amazon Lambda, Google Functions, Microsoft Azure
                  Functions, IBM OpenWhisk},
  month =        {July},
}

@article{figiela18_perfor_evaluat_heter_cloud_funct,
  author =       {Figiela, Kamil and Gajek, Adam and Zima, Adam and
                  Obrok, Beata and Malawski, Maciej},
  title =        {Performance Evaluation of Heterogeneous Cloud
                  Functions},
  journal =      {Concurrency and Computation: Practice and
                  Experience},
  volume =       {30},
  number =       {23},
  pages =        {e4792},
  year =         {2018},
  doi =          {10.1002/cpe.4792},
  url =          {https://doi.org/10.1002/cpe.4792},
  abstract =     {Summary Cloud Functions, often called
                  Function-as-a-Service (FaaS), pioneered by AWS
                  Lambda, are an increasingly popular method of
                  running distributed applications. As in other cloud
                  offerings, cloud functions are heterogeneous due to
                  variations in underlying hardware, runtime systems,
                  as well as resource management and billing
                  models. In this paper, we focus on performance
                  evaluation of cloud functions, taking into account
                  heterogeneity aspects. We developed a cloud function
                  benchmarking framework, consisting of one suite
                  based on Serverless Framework and one based on
                  HyperFlow. We deployed the CPU-intensive benchmarks:
                  Mersenne Twister and Linpack. We measured the data
                  transfer times between cloud functions and storage,
                  and we measured the lifetime of the runtime
                  environment. We evaluated all the major cloud
                  function providers: AWS Lambda, Azure Functions,
                  Google Cloud Functions, and IBM Cloud Functions. We
                  made our results available online and continuously
                  updated. We report on the results of the performance
                  evaluation, and we discuss the discovered insights
                  into resource allocation policies.},
  eprint =
                  {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4792},
  keywords =     {cloud computing, cloud functions, FaaS, performance
                  evaluation, serverless},
  note =         {e4792 cpe.4792},
}

@article{eyk18_server_is_more,
  author =       {E. {van Eyk} and L. {Toader} and S. {Talluri} and
                  L. {Versluis} and A. {U} and A. {Iosup}},
  title =        {Serverless Is More: From Paas To Present Cloud
                  Computing},
  journal =      {IEEE Internet Computing},
  volume =       {22},
  number =       {5},
  pages =        {8-17},
  year =         {2018},
  doi =          {10.1109/MIC.2018.053681358},
  url =          {https://doi.org/10.1109/MIC.2018.053681358},
  ISSN =         {1941-0131},
  abstract =     {In the late-1950s, leasing time on an IBM 704 cost
                  hundreds of dollars per minute. Today, cloud
                  computing, that is, using IT as a service, on-demand
                  and pay-per-use, is a widely used computing paradigm
                  that offers large economies of scale. Born from a
                  need to make platform as a service (PaaS) more
                  accessible, fine-grained, and affordable, serverless
                  computing has garnered interest from both industry
                  and academia. This article aims to give an
                  understanding of these early days of serverless
                  computing: what it is, where it comes from, what is
                  the current status of serverless technology, and
                  what are its main obstacles and opportunities.},
  keywords =     {cloud computing;serverless computing;serverless
                  technology;PaaS;affordable computing;cloud
                  computing;computing paradigm;IT service;Cloud
                  computing;Internet;Economics;Servers;serverless;function-as-a-service;cloud
                  computing;workflows;internet;internet computing},
  month =        {Sep.},
}

@inproceedings{bhalla14_trikon,
  author =       {R. {Bhalla} and P. {Kallurkar} and N. {Gupta} and
                  S. R. {Sarangi}},
  title =        {TriKon: A hypervisor aware manycore processor},
  booktitle =    {2014 21st International Conference on High
                  Performance Computing (HiPC)},
  year =         2014,
  pages =        {1-10},
  doi =          {10.1109/HiPC.2014.7116710},
  url =          {https://doi.org/10.1109/HiPC.2014.7116710},
  ISSN =         {1094-7256},
  abstract =     {Virtualization is increasingly being deployed to run
                  applications in a cloud computing
                  environment. Sadly, there are overheads associated
                  with hypervisors that can prohibitively reduce
                  application performance. A major source of the
                  overheads is the destructive interference between
                  the application, OS, and hypervisor in the memory
                  system. We characterize such overheads in this
                  paper, and propose the design of a novel Triangle
                  cache that can effectively mitigate destructive
                  interference across these three classes of
                  workloads. We subsequently, proceed to design the
                  TriKon manycore processor that consists of a set of
                  heterogeneous cores with caches of different sizes,
                  and Triangle caches. To maximize the throughput of
                  the system as a whole, we propose a dynamic
                  scheduling algorithm for scheduling a class of
                  system and CPU intensive applications on the set of
                  heterogeneous cores. The area of the TriKon
                  processor is within 2 \% of a baseline processor,
                  and with such a system, we could achieve a
                  performance gain of 12 \% for a suite of
                  benchmarks. Within this suite, the system intensive
                  benchmarks show a performance gain of 20 \% while
                  the performance of the compute intensive ones
                  remains unaffected. Also, by allocating extra area
                  for cores with sophisticated cache designs, we
                  further improved the performance of the system
                  intensive benchmarks to 30 \%.},
  keywords =     {cache storage;cloud computing;multiprocessing
                  systems;processor
                  scheduling;virtualisation;hypervisor aware manycore
                  processor;virtualization;cloud computing
                  environment;destructive interference
                  mitigation;OS;memory system;Triangle cache;TriKon
                  manycore processor;heterogeneous cores;CPU intensive
                  applications;baseline processor;dynamic scheduling
                  algorithm;performance gain;cache designs;Virtual
                  machine monitors;Benchmark testing;Multicore
                  processing;Interference;Operating
                  systems;Context;Processor
                  scheduling;cloud;hypervisor;architecture support for
                  virtu-alization},
  month =        {Dec},
}

@inproceedings{kallurkar16,
  author =       {P. {Kallurkar} and S. R. {Sarangi}},
  title =        {pTask: A smart prefetching scheme for OS intensive
                  applications},
  booktitle =    {2016 49th Annual IEEE/ACM International Symposium on
                  Microarchitecture (MICRO)},
  year =         2016,
  pages =        {1-12},
  doi =          {10.1109/MICRO.2016.7783706},
  url =          {https://doi.org/10.1109/MICRO.2016.7783706},
  abstract =     {Instruction prefetching is a standard approach to
                  improve the performance of operating system (OS)
                  intensive workloads such as web servers, file
                  servers and database servers. Sophisticated
                  instruction prefetching techniques such as PIF [12]
                  and RDIP [17] record the execution history of a
                  program in dedicated hardware structures and use
                  this information for prefetching if a known
                  execution pattern is repeated. The storage overheads
                  of the additional hardware structures are
                  prohibitively high (64-200 KB per core). This makes
                  it difficult for the deployment of such schemes in
                  real systems. We propose a solution that uses
                  minimal hardware modifications to tackle this
                  problem. We notice that the execution of server
                  applications keeps switching between tasks such as
                  the application, system call handlers, and interrupt
                  handlers. Each task has a distinct instruction
                  footprint, and is separated by a special OS
                  event. We propose a sophisticated technique to
                  capture the instruction stream in the vicinity of
                  such OS events; the captured information is then
                  compressed significantly and is stored in a
                  process's virtual address space. Special OS routines
                  then use this information to prefetch instructions
                  for the OS and the application codes. Using modest
                  hardware support (4 registers per core), we report
                  an increase in instruction throughput of 2-14 \%
                  (mean: 7 \%) over state of the art instruction
                  prefetching techniques for a suite of 8 popular OS
                  intensive applications.},
  keywords =     {operating systems (computers);storage
                  management;pTask;smart prefetching;OS intensive
                  applications;instruction prefetching;operating
                  system intensive workloads;Web servers;file
                  servers;database servers;PIF;RDIP;hardware
                  structures;system call handlers;OS event;application
                  codes;instruction prefetching
                  techniques;Prefetching;Benchmark
                  testing;Hardware;Web servers;Databases},
  month =        {Oct},
}

@inproceedings{kallurkar17_sched,
  author =       {Kallurkar, Prathmesh and Sarangi, Smruti R.},
  title =        {Schedtask: A Hardware-Assisted Task Scheduler},
  booktitle =    {Proceedings of the 50th Annual IEEE/ACM
                  International Symposium on Microarchitecture},
  year =         2017,
  pages =        {612-624},
  doi =          {10.1145/3123939.3123984},
  url =          {https://doi.org/10.1145/3123939.3123984},
  abstract =     {The execution of workloads such as web servers and
                  database servers typically switches back and forth
                  between different tasks such as user applications,
                  system call handlers, and interrupt handlers. The
                  combined size of the instruction footprints of such
                  tasks typically exceeds that of the i-cache (16--32
                  KB). This causes a lot of i-cache misses and thereby
                  reduces the application's performance. Hence, we
                  propose SchedTask, a hardware-assisted task
                  scheduler that improves the performance of such
                  workloads by executing tasks with similar
                  instruction footprints on the same core. We start by
                  decomposing the combined execution of the OS and the
                  applications into sequences of instructions called
                  SuperFunctions. We propose a scheme to determine the
                  amount of overlap between the instruction footprints
                  of different SuperFunctions by using Bloom
                  filters. We then use a hierarchical scheduler to
                  execute SuperFunctions with similar instruction
                  footprints on the same core. For a suite of 8
                  popular OS-intensive workloads, we report an
                  increase in the application's performance of up to
                  29 percentage points (mean: 11.4 percentage points)
                  over state of the art scheduling techniques.},
  address =      {New York, NY, USA},
  isbn =         9781450349529,
  keywords =     {scheduling, cache pollution, architectural support
                  for operating system},
  location =     {Cambridge, Massachusetts},
  numpages =     13,
  publisher =    {Association for Computing Machinery},
  series =       {MICRO-50 '17},
}

@article{li07_os_aware_branc_predic,
  author =       {T. {Li} and L. K. {John} and A. {Sivasubramaniam}
                  and N. {Vijaykrishnan} and J. {Rubio}},
  title =        {Os-Aware Branch Prediction: Improving Microprocessor
                  Control Flow Prediction for Operating Systems},
  volume =       56,
  number =       1,
  pages =        {2-17},
  year =         2007,
  journaltitle = {IEEE Transactions on Computers},
}

@inproceedings{co01_effec_contex_switc_branc_predic_perfor,
  author =       {Michele Co and Kevin Skadron},
  title =        {The effects of context switching on branch predictor
                  performance},
  booktitle =    {2001 IEEE International Symposium on Performance
                  Analysis of Systems and Software. ISPASS.},
  year =         2001,
  pages =        {77-84},
  doi =          {10.1109/ISPASS.2001.990679},
  url =          {https://doi.org/10.1109/ISPASS.2001.990679},
  keywords =     {Accuracy;Parallel processing;Predictive
                  models;Context modeling;Computer science;Operating
                  systems;Switches;Testing},
  month =        {Nov},
}

@inproceedings{mogul91_effec_contex_switc_cache_perfor,
  author =       {Mogul, Jeffrey C. and Borg, Anita},
  title =        {The Effect of Context Switches on Cache Performance},
  booktitle =    {Proceedings of the Fourth International Conference
                  on Architectural Support for Programming Languages
                  and Operating Systems},
  year =         1991,
  pages =        {75-84},
  doi =          {10.1145/106972.106982},
  url =          {https://doi.org/10.1145/106972.106982},
  address =      {New York, NY, USA},
  isbn =         0897913809,
  location =     {Santa Clara, California, USA},
  numpages =     10,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS IV},
}

@thesis{maas18_hardw_softw_suppor_manag_languag,
  URL =
                  {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-152.html},
  abstract =     {An increasing number of workloads are moving to
                  cloud data centers, including large-scale machine
                  learning, big data analytics and back-ends for the
                  Internet of Things. Many of these workloads are
                  written in managed languages such as Java, Python or
                  Scala. The performance and efficiency of
                  managed-language workloads are therefore crucial in
                  terms of hardware cost, energy efficiency and
                  quality of service for these data centers.  While
                  managed-language issues such as garbage collection
                  (GC) and JIT compilation have seen a significant
                  amount of research on single-node deployments, data
                  center workloads run across a large number of
                  independent language virtual machines and face new
                  systems challenges that were not previously
                  addressed. At the same time, there has been a large
                  amount of work on specialized systems software and
                  custom hardware for data centers, but most of this
                  work does not fundamentally address managed
                  languages and does not modify the language runtime
                  system, effectively treating it as a black box.  In
                  this thesis, we argue that we can substantially
                  improve the performance, efficiency and
                  responsiveness of managed applications in cloud data
                  centers by treating the language runtime system as a
                  fundamental part of the data center stack and
                  co-designing it with both the software systems layer
                  and the hardware layer. In particular, we argue that
                  the cloud operators' full control over the software
                  and hardware stack enables them to co-design these
                  different layers to a degree that would be difficult
                  to achieve in other settings. To support this
                  thesis, we investigate two examples of co-designing
                  the language runtime system with the remainder of
                  the stack, spanning both the hardware and software
                  layers.  On the software side, we show how to better
                  support distributed managed-language applications
                  through a "Holistic" Language Runtime System, which
                  treats the runtimes underpinning a distributed
                  application as a distributed system itself. We first
                  introduce the concept of a Holistic Runtime
                  System. We then present Taurus, a prototype
                  implementation of such a system, based on the
                  OpenJDK Hotspot JVM. By applying Taurus to two
                  representative real-world workloads, we show that it
                  is effective both in reducing the overall runtime
                  and resource consumption, as well as improving long
                  tail-latencies.  On the hardware side, we describe
                  how custom data center SoCs provide an opportunity
                  to revisit the old idea of hardware support for
                  garbage collection. We first show that garbage
                  collection is a suitable workload to be offloaded
                  from the CPU to data-parallel accelerators, by
                  demonstrating how integrated GPUs can be used to
                  perform garbage collection for applications running
                  on the CPU. We then generalize these ideas into a
                  custom hardware accelerator for garbage collection
                  that performs GC more efficiently than running the
                  operation on a traditional CPU. We show this design
                  in the context of a stop-the-world garbage
                  collector, and describe how it could be extended to
                  a fully concurrent, pause-free GC.  Finally, we
                  discuss how hardware-software research on managed
                  languages requires new research infrastructure to
                  achieve a higher degree of realism and industry
                  adoption. We then present the foundation of a new
                  research platform for this type of work, using
                  open-source hardware based on the free and open
                  RISC-V ISA combined with the Jikes Research Virtual
                  Machine. Using this research infrastructure, we
                  evaluate the performance and efficiency of our
                  proposed hardware-assisted garbage collector
                  design.},
  author =       {Maas, Martin},
  institution =  {EECS Department, University of California, Berkeley},
  month =        {Dec},
  number =       {UCB/EECS-2018-152},
  title =        {Hardware and Software Support for Managed-Language
                  Workloads in Data Centers},
  type =         { Ph. D.},
  year =         2018,
}

@inproceedings{shahrad20_server_wild,
  author =       {Mohammad Shahrad and Rodrigo Fonseca and Inigo Goiri
                  and Gohar Chaudhry and Paul Batum and Jason Cooke
                  and Eduardo Laureano and Colby Tresness and Mark
                  Russinovich and Ricardo Bianchini},
  title =        {Serverless in the Wild: Characterizing and
                  Optimizing the Serverless Workload at a Large Cloud
                  Provider},
  booktitle =    {2020 {USENIX} Annual Technical Conference ({USENIX}
                  {ATC} 20)},
  year =         2020,
  pages =        {205--218},
  url =
                  {https://www.usenix.org/conference/atc20/presentation/shahrad},
  isbn =         {978-1-939133-14-4},
  month =        jul,
  publisher =    {{USENIX} Association},
}

@inproceedings{rohou15_branc_dont,
  author =       {E. {Rohou} and B. N. {Swamy} and A. {Seznec}},
  title =        {Branch prediction and the performance of
                  interpreters - Don't trust folklore},
  booktitle =    {2015 IEEE/ACM International Symposium on Code
                  Generation and Optimization (CGO)},
  year =         2015,
  pages =        {103-114},
}

@inproceedings{ayers19_asmdb,
  author =       {Ayers, Grant and Nagendra, Nayana Prasad and August,
                  David I. and Cho, Hyoun Kyu and Kanev, Svilen and
                  Kozyrakis, Christos and Krishnamurthy, Trivikram and
                  Litz, Heiner and Moseley, Tipp and Ranganathan,
                  Parthasarathy},
  title =        {AsmDB: Understanding and Mitigating Front-End Stalls
                  in Warehouse-Scale Computers},
  booktitle =    {Proceedings of the 46th International Symposium on
                  Computer Architecture},
  year =         2019,
  pages =        {462-473},
  doi =          {10.1145/3307650.3322234},
  url =          {https://doi.org/10.1145/3307650.3322234},
  abstract =     {The large instruction working sets of private and
                  public cloud workloads lead to frequent instruction
                  cache misses and costs in the millions of
                  dollars. While prior work has identified the growing
                  importance of this problem, to date, there has been
                  little analysis of where the misses come from, and
                  what the opportunities are to improve them. To
                  address this challenge, this paper makes three
                  contributions. First, we present the design and
                  deployment of a new, always-on, fleet-wide
                  monitoring system, AsmDB, that tracks front-end
                  bottlenecks. AsmDB uses hardware support to collect
                  bursty execution traces, fleet-wide temporal and
                  spatial sampling, and sophisticated offline
                  post-processing to construct full-program dynamic
                  control-flow graphs. Second, based on a longitudinal
                  analysis of AsmDB data from real-world online
                  services, we present two detailed insights on the
                  sources of front-end stalls: (1) cold code that is
                  brought in along with hot code leads to significant
                  cache fragmentation and a corresponding large number
                  of instruction cache misses; (2) distant branches
                  and calls that are not amenable to traditional cache
                  locality or next-line prefetching strategies account
                  for a large fraction of cache misses. Third, we
                  prototype two optimizations that target these
                  insights. For misses caused by fragmentation, we
                  focus on memcmp, one of the hottest functions
                  contributing to cache misses, and show how
                  fine-grained layout optimizations lead to
                  significant benefits. For misses at the targets of
                  distant jumps, we propose new hardware support for
                  software code prefetching and prototype a new
                  feedback-directed compiler optimization that
                  combines static program flow analysis with dynamic
                  miss profiles to demonstrate significant benefits
                  for several large warehouse-scale
                  workloads. Improving upon prior work, our proposal
                  avoids invasive hardware modifications by
                  prefetching via software in an efficient and
                  scalable way. Simulation results show that such an
                  approach can eliminate up to 96 \% of instruction
                  cache misses with negligible overheads.},
  address =      {New York, NY, USA},
  isbn =         9781450366694,
  location =     {Phoenix, Arizona},
  numpages =     12,
  publisher =    {Association for Computing Machinery},
  series =       {ISCA '19},
}

@inproceedings{khan20_i_spy,
  author =       {Tanvir Khan and Akshitha Sriraman and Joseph
                  Devietti and Gilles Pokam and Heiner Litz and Baris
                  Kasikci},
  title =        {I-SPY: Context-Driven Conditional Instruction
                  Prefetching with Coalescing},
  booktitle =    {2020 53rd Annual IEEE/ACM International Symposium on
                  Microarchitecture (MICRO)},
  year =         2020,
  pages =        {nil},
  doi =          {nil},
  url =          {https://doi.org/nil},
  abstract =     { Modern data center applications have rapidly
                  expanding instruction footprints that lead to
                  frequent instruction cache misses, increasing cost
                  and degrading data center performance and energy
                  efficiency. Mitigating instruction cache misses is
                  challenging since existing techniques (1) require
                  significant hardware modifications, (2) expect
                  impractical on-chip storage, or (3) prefetch
                  instructions based on inaccurate understanding of
                  program miss behavior.  To overcome these
                  limitations, we first investigate the challenges of
                  effective instruction prefetching. We then use
                  insights derived from our investigation to develop
                  I-SPY, a novel profile-driven prefetching
                  technique. I-SPY uses dynamic miss profiles to drive
                  an offline analysis of I-cache miss behavior, which
                  it uses to inform prefetching decisions. Two key
                  techniques underlie I-SPY's design: (1) conditional
                  prefetching, which only prefetches instructions if
                  the program context is known to lead to misses, and
                  (2) prefetch coalescing, which merges multiple
                  prefetches of non-contiguous cache lines into a
                  single prefetch instruction. I-SPY exposes these
                  techniques via a family of light-weight hardware
                  code prefetch instructions.  We study I-SPY in the
                  context of nine data center applications and show
                  that it provides an average of 15.5\% (up to 45.9\%)
                  speedup and 95.9\% (and up to 98.4\%) reduction in
                  instruction cache misses, outperforming the
                  state-of-the-art prefetching technique by 22.5\%. We
                  show that I-SPY achieves performance improvements
                  that are on average 90.5\% of the performance of an
                  ideal cache with no misses.},
  keywords =     {Prefetching, frontent stalls, memory systems},
  month =        10,
}

@inproceedings{dimitrios20_draco,
  author =       {Dimitros Skarlatos and Qingrong Chen and Jianyan Chen and
                  Tianyin Xu and Josep Torrellas},
  title =        {Draco: Architectural and Operating System Support
                  for System Call Security},
  booktitle =    {2020 53rd Annual IEEE/ACM International Symposium on
                  Microarchitecture (MICRO)},
  year =         2020,
  pages =        {nil},
  doi =          {nil},
  url =          {https://doi.org/nil},
  abstract =     {System call checking is extensively used to protect
                  the operating system kernel from user
                  attacks. However, existing solutions such as Seccomp
                  execute lengthy rule-based checking programs against
                  system calls and their arguments, leading to
                  substantial execution overhead.  To minimize
                  checking overhead, this paper proposes Draco, a new
                  architecture that caches system call IDs and
                  argument values after they have been checked and
                  validated. System calls are first looked-up in a
                  special cache and, on a hit, skip all checks. We
                  present both a software and a hardware
                  implementation of Draco.  The latter introduces a
                  System Call Lookaside Buffer (SLB) to keep
                  recently-validated system calls, and a System Call
                  Target Buffer to preload the SLB in advance. In our
                  evaluation, we find that the average execution time
                  of macro and micro benchmarks with conventional
                  Seccomp checking is 1.14$\times$ and 1.25$\times$
                  higher, respectively, than on an insecure baseline
                  that performs no security checks. With our software
                  Draco, the average execution time reduces to
                  1.10$\times$ and 1.18$\times$ higher, respectively,
                  than on the insecure baseline. With our hardware
                  Draco, the execution time is within 1\% of the
                  insecure baseline },
  keywords =     {System call checking, Security, Operating system,
                  Containers, Virtualization, Microarchitecture},
  month =        {Oct},
}

@inproceedings{pourhabibi20_optim_prime,
  author =       {Pourhabibi, Arash and Gupta, Siddharth and Kassir,
                  Hussein and Sutherland, Mark and Tian, Zilu and
                  Drumond, Mario Paulo and Falsafi, Babak and Koch,
                  Christoph},
  title =        {Optimus Prime: Accelerating Data Transformation in
                  Servers},
  booktitle =    {Proceedings of the Twenty-Fifth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems},
  year =         2020,
  pages =        {1203-1216},
  doi =          {10.1145/3373376.3378501},
  url =          {https://doi.org/10.1145/3373376.3378501},
  abstract =     {Modern online services are shifting away from
                  monolithic applications to loosely-coupled
                  microservices because of their improved scalability,
                  reliability, programmability and development
                  velocity. Microservices communicating over the
                  datacenter network require data transformation (DT)
                  to convert messages back and forth between their
                  internal formats. This work identifies DT as a
                  bottleneck due to reductions in latency of the
                  surrounding system components, namely application
                  runtimes, protocol stacks, and network hardware. We
                  therefore propose Optimus Prime (OP), a programmable
                  DT accelerator that uses a novel abstraction, an
                  in-memory schema, to represent DT operations. The
                  schema is compatible with today's DT frameworks and
                  enables any compliant accelerator to perform the
                  transformations comprising a request in
                  parallel. Our evaluation shows that OP's DT
                  throughput matches the line rate of today's NICs and
                  has ~60x higher throughput compared to software, at
                  a tiny fraction of the CPU's silicon area and
                  power. We also evaluate a set of microservices
                  running on Thrift, and show up to 30 \% reduction in
                  service latency.},
  address =      {New York, NY, USA},
  isbn =         9781450371025,
  keywords =     {data transformation, hardware accelerators,
                  datacenters, microservices, networked systems},
  location =     {Lausanne, Switzerland},
  numpages =     14,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS '20},
}

@article{xu17_energ_effic_cloud_virtual_machin,
  author =       {C. {Xu} and Z. {Zhao} and H. {Wang} and R. {Shea}
                  and J. {Liu}},
  title =        {Energy Efficiency of Cloud Virtual Machines: From
                  Traffic Pattern and Cpu Affinity Perspectives},
  volume =       11,
  number =       2,
  pages =        {835-845},
  year =         2017,
  doi =          {10.1109/JSYST.2015.2429731},
  url =          {https://doi.org/10.1109/JSYST.2015.2429731},
  ISSN =         {1937-9234},
  abstract =     {Networking and machine virtualization play critical
                  roles in the success of modern cloud computing. The
                  energy consumption of physical machines has been
                  carefully examined in the past, including the impact
                  from network traffic. When it comes to virtual
                  machines (VMs) in cloud data centers, the interplay
                  between energy consumption and network traffic,
                  however, becomes much more complicated. Through
                  real-world measurement on both Xen- and KVM-based
                  platforms, we show that these state-of-the-art
                  virtualization designs noticeably increase the
                  demand of CPU resources when handling network
                  transactions, generating excessive interrupt
                  requests with ceaseless context switching, which in
                  turn, increases energy consumption. Even when a
                  physical machine is in an idle state, its VM's
                  network transactions will incur nontrivial energy
                  consumption. More interestingly, the energy
                  consumption significantly varies with traffic
                  allocation strategies and virtual CPU affinity
                  conditions, which was not seen in conventional
                  physical machines. Looking closely into the
                  virtualization architectures, we then pinpoint the
                  root causes and examine that our measurement results
                  can be extended for various network
                  configurations. Moreover, we also provide initial
                  solutions toward optimizing energy consumption in
                  virtualized environments.},
  journaltitle = {IEEE Systems Journal},
  keywords =     {cloud computing;energy consumption;microprocessor
                  chips;power aware computing;virtual
                  machines;virtualization architectures;virtual CPU
                  affinity;VM network transactions;context
                  switching;interrupt requests;network transaction
                  handling;energy efficiency;cloud virtual
                  machines;traffic pattern;CPU affinity
                  perspectives;networking virtualization;machine
                  virtualization;modern cloud computing;physical
                  machine energy consumption;network traffic;cloud
                  data centers;Xen-based platforms;KVM-based
                  platforms;virtualization designs;Energy
                  consumption;Virtualization;Virtual machine
                  monitors;Context;Computer architecture;Power
                  demand;Containers;Cloud computing;energy
                  consumption;measurement;networking;virtualization},
  month =        {June},
}

@inproceedings{tomas16_servic_level_perfor_aware_dynam,
  author =       {L. {Tom{\'a}s} and E. B. {Lakew} and E. {Elmroth}},
  title =        {Service Level and Performance Aware Dynamic Resource
                  Allocation in Overbooked Data Centers},
  booktitle =    {2016 16th IEEE/ACM International Symposium on
                  Cluster, Cloud and Grid Computing (CCGrid)},
  year =         2016,
  pages =        {42-51},
  doi =          {10.1109/CCGrid.2016.29},
  url =          {https://doi.org/10.1109/CCGrid.2016.29},
  abstract =     {Many cloud computing providers use overbooking to
                  increase their low utilization ratios. This however
                  increases the risk of performance degradation due to
                  interference among co-located VMs. To address this
                  problem we present a service level and performance
                  aware controller that: (1) provides performance
                  isolation for high QoS VMs, and (2) reduces the VM
                  interference between low QoS VMs by dynamically
                  mapping virtual cores to physical cores, thus
                  limiting the amount of resources that each VM can
                  access depending on their performance. Our
                  evaluation based on real cloud applications and both
                  stress, synthetic and realistic workloads
                  demonstrates that a more efficient use of the
                  resources is achieved, dynamically allocating the
                  available capacity to the applications that need it
                  more, which in turn lead to a more stable and
                  predictable performance over time.},
  keywords =     {cloud computing;computer centres;quality of
                  service;resource allocation;virtual
                  machines;capacity allocation;realistic
                  workloads;synthetic workloads;cloud
                  applications;virtual core-physical core mapping;VM
                  interference reduction;QoS VM;performance
                  isolation;performance aware controller;colocated
                  VM;performance degradation;cloud computing
                  providers;overbooked data centers;performance aware
                  dynamic resource allocation;service level;Quality of
                  service;Interference;Resource
                  management;Throughput;Servers;Degradation;Limiting},
  month =        {May},
}

@inproceedings{zhang20_high_densit_multi_tenan_bare_metal_cloud,
  author =       {Zhang, Xiantao and Zheng, Xiao and Wang, Zhi and
                  Yang, Hang and Shen, Yibin and Long, Xin},
  title =        {High-Density Multi-Tenant Bare-Metal Cloud},
  booktitle =    {Proceedings of the Twenty-Fifth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems},
  year =         2020,
  pages =        {483-495},
  doi =          {10.1145/3373376.3378507},
  url =          {https://doi.org/10.1145/3373376.3378507},
  abstract =     {Virtualization is the cornerstone of the
                  infrastructure-as-a-service (IaaS) cloud, where VMs
                  from multiple tenants share a single physical
                  server. This increases the utilization of
                  data-center servers, allowing cloud providers to
                  provide cost-efficient services. However, the
                  multi-tenant nature of this service leads to serious
                  security concerns, especially in regard to
                  side-channel attacks. In addition, virtualization
                  incurs non-negligible overhead in the performance of
                  CPU, memory, and I/O. To this end, the bare-metal
                  cloud has become an emerging type of service in the
                  public clouds, where a cloud user can rent dedicated
                  physical servers. The bare-metal cloud provides
                  users with strong isolation, full and direct access
                  to the hardware, and more predicable
                  performance. However, the existing single-tenant
                  bare-metal service has poor scalability, low cost
                  efficiency, and weak adaptability because it can
                  only lease entire physical servers to users and have
                  no control over user programs after the server is
                  leased. In this paper, we propose the design of a
                  new high-density multi-tenant bare-metal cloud
                  called BM-Hive. In BM-Hive, each bare-metal guest
                  runs on its own compute board, a PCIe extension
                  board with the dedicated CPU and memory
                  modules. Moreover, BM-Hive features a
                  hardware-software hybrid virtio I/O system that
                  enables the guest to directly access the cloud
                  network and storage services. BM-Hive can
                  significantly improve the cost efficiency of the
                  bare-metal service by hosting up to 16 bare-metal
                  guests in a single physical server. In addition,
                  BM-Hive strictly isolates the bare-metal guests at
                  the hardware level for better security and
                  isolation. We have deployed BM-Hive in one of the
                  largest public cloud infrastructures. It currently
                  serves tens of thousands of users at the same
                  time. Our evaluation of BM-Hive demonstrates its
                  strong performance over VMs.},
  address =      {New York, NY, USA},
  isbn =         9781450371025,
  keywords =     {virtualization, bare-metal cloud, high-density,
                  cloud infrastructure},
  location =     {Lausanne, Switzerland},
  numpages =     13,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS '20},
}

@article{larus01_using_cohor_sched_to_enhan,
  author =       {Larus, James R. and Parkes, Michael},
  title =        {Using Cohort Scheduling To Enhance Server
                  Performance (Extended Abstract)},
  journal =      {SIGPLAN Not.},
  volume =       {36},
  number =       {8},
  pages =        {182-187},
  year =         {2001},
  doi =          {10.1145/384196.384222},
  url =          {https://doi.org/10.1145/384196.384222},
  address =      {New York, NY, USA},
  issn =         {0362-1340},
  issue_date =   {Aug. 2001},
  month =        aug,
  numpages =     {6},
  publisher =    {Association for Computing Machinery},
}

@inproceedings{lozi16_linux_sched,
  author =       {Lozi, Jean-Pierre and Lepers, Baptiste and Funston,
                  Justin and Gaud, Fabien and Qu\'{e}ma, Vivien and
                  Fedorova, Alexandra},
  title =        {The Linux Scheduler: A Decade of Wasted Cores},
  booktitle =    {Proceedings of the Eleventh European Conference on
                  Computer Systems},
  year =         2016,
  doi =          {10.1145/2901318.2901326},
  url =          {https://doi.org/10.1145/2901318.2901326},
  abstract =     {As a central part of resource management, the OS
                  thread scheduler must maintain the following,
                  simple, invariant: make sure that ready threads are
                  scheduled on available cores. As simple as it may
                  seem, we found that this invariant is often broken
                  in Linux. Cores may stay idle for seconds while
                  ready threads are waiting in runqueues. In our
                  experiments, these performance bugs caused many-fold
                  performance degradation for synchronization-heavy
                  scientific applications, 13\% higher latency for
                  kernel make, and a 14-23\% decrease in TPC-H
                  throughput for a widely used commercial
                  database. The main contribution of this work is the
                  discovery and analysis of these bugs and providing
                  the fixes. Conventional testing techniques and
                  debugging tools are ineffective at confirming or
                  understanding this kind of bugs, because their
                  symptoms are often evasive. To drive our
                  investigation, we built new tools that check for
                  violation of the invariant online and visualize
                  scheduling activity. They are simple, easily
                  portable across kernel versions, and run with a
                  negligible overhead. We believe that making these
                  tools part of the kernel developers' tool belt can
                  help keep this type of bug at bay.},
  address =      {New York, NY, USA},
  articleno =    1,
  isbn =         9781450342407,
  location =     {London, United Kingdom},
  numpages =     16,
  publisher =    {Association for Computing Machinery},
  series =       {EuroSys '16},
}

@article{chakraborty12_suppor_overc_virtual_machin_throug,
  author =       {K. {Chakraborty} and P. M. {Wells} and G. S. {Sohi}},
  title =        {Supporting Overcommitted Virtual Machines Through
                  Hardware Spin Detection},
  journal =      {IEEE Transactions on Parallel and Distributed
                  Systems},
  volume =       {23},
  number =       {2},
  pages =        {353-366},
  year =         {2012},
  doi =          {10.1109/TPDS.2011.143},
  url =          {https://doi.org/10.1109/TPDS.2011.143},
  ISSN =         {1558-2183},
  abstract =     {Multiprocessor operating systems (OSs) pose several
                  unique and conflicting challenges to System Virtual
                  Machines (System VMs). For example, most existing
                  system VMs resort to gang scheduling a guest OS's
                  virtual processors (VCPUs) to avoid OS
                  synchronization overhead. However, gang scheduling
                  is infeasible for some application domains, and is
                  inflexible in other domains. In an overcommitted
                  environment, an individual guest OS has more VCPUs
                  than available physical processors (PCPUs),
                  precluding the use of gang scheduling. In such an
                  environment, we demonstrate a more than two-fold
                  increase in application runtime when transparently
                  virtualizing a chip-multiprocessor's cores. To
                  combat this problem, we propose a hardware technique
                  to detect when a VCPU is wasting CPU cycles, and
                  preempt that VCPU to run a different, more
                  productive VCPU. Our technique can dramatically
                  reduce cycles wasted on OS synchronization, without
                  requiring any semantic information from the
                  software. We then present a server consolidation
                  case study to demonstrate the potential of more
                  flexible scheduling policies enabled by our
                  technique. We propose one such policy that logically
                  partitions the CMP cores between guest VMs. This
                  policy increases throughput by 10-25 percent for
                  consolidated server workloads due to improved cache
                  locality and core utilization.},
  keywords =     {multiprocessing systems;operating systems
                  (computers);scheduling;virtual
                  machines;virtualisation;hardware spin
                  detection;multiprocessor operating systems;system
                  virtual machines;gang scheduling;OS virtual
                  processors;physical processors;chip-multiprocessor
                  core virtualization;VCPU;OS synchronization;server
                  consolidation case study;CMP cores;cache
                  locality;core
                  utilization;Hardware;Synchronization;Kernel;Arrays;Virtual
                  machining;Servers;Multicore;virtualization;synchronization;operating
                  systems.},
  month =        {Feb},
}



@article{lee02_opts,
  author =       "Moon-Sang Lee and Young-Jae Kang and Joon-Won Lee
                  and Seung-Ryoul Maeng",
  title =        {{OPTS}: Increasing Branch Prediction Accuracy Under
                  Context Switch},
  journal =      "Microprocessors and Microsystems",
  volume =       "26",
  number =       "6",
  pages =        "291 - 300",
  year =         "2002",
  doi =          "https://doi.org/10.1016/S0141-9331(02)00041-8",
  url =
                  {https://doi.org/https://doi.org/10.1016/S0141-9331(02)00041-8},
  abstract =     "Accurate branch prediction is essential for
                  obtaining high performance in pipelined superscalar
                  processors. Though many dynamic branch predictors
                  have been proposed to obtain high prediction
                  accuracy, they cannot perform as expected under
                  context switches. It is observed that context
                  switches, even at fairly large intervals, can
                  seriously degrade the performance of dynamic branch
                  predictors. In this paper we measure the effect of
                  context switch on branch prediction, and present a
                  new scheme which saves and restores branch predictor
                  table when a context switch occurs. This scheme
                  takes advantage of multiple small predictors which
                  preserve branch predictor tables of independent
                  processes. The effectiveness of reducing
                  interprocess interference is evaluated by
                  simulations.",
  issn =         "0141-9331",
  keywords =     "Microprocessor, Branch prediction, Context switch",
}

@inproceedings{reza12_reduc_migrat_cache_misses,
  author =       {S. {Reza} and G. T. {Byrd}},
  title =        {Reducing Migration-induced Cache Misses},
  booktitle =    {2012 IEEE 26th International Parallel and
                  Distributed Processing Symposium Workshops PhD
                  Forum},
  year =         2012,
  pages =        {1732-1741},
  doi =          {10.1109/IPDPSW.2012.215},
  url =          {https://doi.org/10.1109/IPDPSW.2012.215},
  abstract =     {In a large multiprocessor server platform, using
                  multicore chips, the scheduler often migrates a
                  scheduling entity, i.e. a thread or process or
                  virtual machine, in order to achieve better load
                  balancing or ensure fairness. The migration impact
                  is likely to be more severe in virtualized
                  environments, where high over-subscription of
                  logical CPUs is very common for server consolidation
                  workloads or virtual desktop infrastructure
                  deployment. We demonstrate the performance benefit
                  of saving and restoring cached data during
                  migration. In particular, we measure the efficiency
                  (benefit per cache block) of saving various subsets
                  of the cached data, in order to balance
                  implementation cost and complexity with improvements
                  in cycle time. We also describe an implementation
                  that moves cached data when a thread migrates, and
                  we show the benefits in terms of reduced misses and
                  reduced processor cycles.},
  keywords =     {cache storage;multiprocessing systems;operating
                  systems (computers);resource
                  allocation;scheduling;virtual
                  machines;migration-induced cache miss
                  reduction;large multiprocessor server
                  platform;multicore chips;scheduler;scheduling entity
                  migration;virtual machine thread;virtual machine
                  process;load balancing;fairness;migration
                  impact;virtualized environment;logical CPU
                  over-subscription;server consolidation
                  workload;virtual desktop infrastructure
                  deployment;cached data saving;cached data
                  restoration;cache block benefit;cycle time
                  improvement;processor
                  cycle;Context;Switches;Benchmark testing;Instruction
                  sets;Servers;Multicore
                  processing;Bandwidth;caches;multiprocessors;process
                  migration},
  month =        {May},
}

@inproceedings{mahmoudi19_optim_server_comput,
  author =       {Mahmoudi, Nima and Lin, Changyuan and Khazaei,
                  Hamzeh and Litoiu, Marin},
  title =        {Optimizing Serverless Computing: Introducing an
                  Adaptive Function Placement Algorithm},
  booktitle =    {Proceedings of the 29th Annual International
                  Conference on Computer Science and Software
                  Engineering},
  year =         2019,
  pages =        {203-213},
  abstract =     {The main concept behind serverless computing is to
                  build and run applications without the need for
                  server management. It refers to a fine-grained
                  deployment model where applications, comprising of
                  one or more functions, are uploaded to a platform
                  and then executed, scaled, and billed in response to
                  the exact demand needed at the moment. While elite
                  cloud vendors such as Amazon, Google, Microsoft, and
                  IBM are now providing serverless computing, their
                  approach for the placement of functions,
                  i.e. associated container or sandbox, on servers is
                  oblivious to the workload which may lead to poor
                  performance and/or higher operational cost for
                  software owners. In this paper, using statistical
                  machine learning, we design and evaluate an adaptive
                  function placement algorithm which can be used by
                  serverless computing platforms to optimize the
                  performance of running functions while minimizing
                  the operational cost. Given a fixed amount of
                  resources, our smart spread function placement
                  algorithm results in higher performance compared to
                  existing approaches; this will be achieved by
                  maintaining the users' desired quality of service
                  for a longer time which prevents premature scaling
                  of the cloud resources. Extensive experimental
                  studies revealed that the proposed adaptive function
                  placement algorithm can be easily adopted by
                  serverless computing providers and integrated to
                  container orchestration platforms without
                  introducing any limiting side effects.},
  address =      {USA},
  keywords =     {machine learning, predictive performance modeling,
                  serverless computing, container placement
                  algorithms, optimization},
  location =     {Toronto, Ontario, Canada},
  numpages =     11,
  publisher =    {IBM Corp.},
  series =       {CASCON '19},
}

@article{jung00_sched_polic_preser_cache_local_multip_system,
  author =       "Inbum Jung and Jongwoong Hyun and Joonwon Lee",
  title =        {A Scheduling Policy for Preserving Cache Locality in
                  a Multiprogrammed System},
  journal =      "Journal of Systems Architecture",
  volume =       "46",
  number =       "13",
  pages =        "1191 - 1204",
  year =         "2000",
  doi =          "https://doi.org/10.1016/S1383-7621(00)00020-5",
  url =
                  {https://doi.org/https://doi.org/10.1016/S1383-7621(00)00020-5},
  abstract =     "In a multiprogrammed system, when the operating
                  system switches contexts, in addition to the cost
                  for handling the processes being swapped out and in,
                  the cache performance of processors also can be
                  affected. If frequent context switching replaces the
                  data loaded into cache memory before they are
                  completely reused, the programs suffer from cache
                  misses due to the damage in cache locality. In
                  particular, for the programs with good cache
                  locality, such as blocked programs, a scheduling
                  mechanism of keeping cache locality against context
                  switching is essential to achieve good processor
                  utilization. To solve this requirement, we propose a
                  preemption-safe policy to exploit the cache locality
                  of blocked programs in a multiprogrammed system. The
                  proposed policy delays context switching until a
                  block is fully reused, but also compensates for the
                  monopolized processor time on processor scheduling
                  mechanisms. Our simulation results show that in a
                  situation where blocked programs are run on
                  multiprogrammed shared-memory multiprocessors, the
                  proposed policy improves the performance of these
                  programs due to a decrease in cache misses. In such
                  situations, it also has a beneficial impact on the
                  overall system performance due to the enhanced
                  processor utilization.",
  issn =         "1383-7621",
  keywords =     "Blocked algorithm, Multiprogrammed system, Cache
                  locality, Context switching",
}

@inproceedings{snavely00_symbiot_jobsc_simul_multit_proces,
  author =       {Snavely, Allan and Tullsen, Dean M.},
  title =        {Symbiotic Jobscheduling for a Simultaneous
                  Multithreaded Processor},
  booktitle =    {Proceedings of the Ninth International Conference on
                  Architectural Support for Programming Languages and
                  Operating Systems},
  year =         2000,
  pages =        {234-244},
  doi =          {10.1145/378993.379244},
  url =          {https://doi.org/10.1145/378993.379244},
  abstract =     {Simultaneous Multithreading machines fetch and
                  execute instructions from multiple instruction
                  streams to increase system utilization and speedup
                  the execution of jobs. When there are more jobs in
                  the system than there is hardware to support
                  simultaneous execution, the operating system
                  scheduler must choose the set of jobs to
                  coscheduleThis paper demonstrates that performance
                  on a hardware multithreaded processor is sensitive
                  to the set of jobs that are coscheduled by the
                  operating system jobscheduler. Thus, the full
                  benefits of SMT hardware can only be achieved if the
                  scheduler is aware of thread interactions. Here, a
                  mechanism is presented that allows the scheduler to
                  significantly raise the performance of SMT
                  architectures. This is done without any advance
                  knowledge of a workload's characteristics, using
                  sampling to identify jobs which run well together.We
                  demonstrate an SMT jobscheduler called SOS. SOS
                  combines an overhead-free sample phase which
                  collects information about various possible
                  schedules, and a symbiosis phase which uses that
                  information to predict which schedule will provide
                  the best performance. We show that a small sample of
                  the possible schedules is sufficient to identify a
                  good schedule quickly. On a system with random job
                  arrivals and departures, response time is improved
                  as much as 17 \% over a schedule which does not
                  incorporate symbiosis.},
  address =      {New York, NY, USA},
  isbn =         1581133170,
  location =     {Cambridge, Massachusetts, USA},
  numpages =     11,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS IX},
}

@inproceedings{kumar04_singl_isa,
  author =       {R. {Kumar} and D. M. {Tullsen} and P. {Ranganathan}
                  and N. P. {Jouppi} and K. I. {Farkas}},
  title =        {Single-ISA heterogeneous multi-core architectures
                  for multithreaded workload performance},
  booktitle =    {Proceedings. 31st Annual International Symposium on
                  Computer Architecture, 2004.},
  year =         2004,
  pages =        {64-75},
  doi =          {10.1109/ISCA.2004.1310764},
  url =          {https://doi.org/10.1109/ISCA.2004.1310764},
  ISSN =         {1063-6897},
  abstract =     {A single-ISA heterogeneous multi-core architecture
                  is a chip multiprocessor composed of cores of
                  varying size, performance, and complexity. This
                  paper demonstrates that this architecture can
                  provide significantly higher performance in the same
                  area than a conventional chip multiprocessor. It
                  does so by matching the various jobs of a diverse
                  workload to the various cores. This type of
                  architecture covers a spectrum of workloads
                  particularly well, providing high single-thread
                  performance when thread parallelism is low, and high
                  throughput when thread parallelism is high. This
                  paper examines two such architectures in detail,
                  demonstrating dynamic core assignment policies that
                  provide significant performance gains over naive
                  assignment, and even outperform the best static
                  assignment. It examines policies for heterogeneous
                  architectures both with and without multithreading
                  cores. One heterogeneous architecture we examine
                  outperforms the comparable-area homogeneous
                  architecture by up to 63 \%, and our best core
                  assignment strategy achieves up to 31 \% speedup
                  over a naive policy.},
  keywords =     {multiprocessing systems;parallel
                  architectures;multi-threading;microprocessor
                  chips;instruction sets;single-ISA heterogeneous
                  multicore architectures;multithreaded workload
                  performance;chip multiprocessor;job
                  matching;single-thread performance;thread
                  parallelism;dynamic core assignment policies;static
                  assignment;heterogeneous
                  architectures;multithreading cores;comparable-area
                  homogeneous architecture;naive policy;Computer
                  architecture;Throughput;Yarn;Parallel
                  processing;Performance
                  gain;Multithreading;Delay;Computer science;Milling
                  machines;Microprocessors},
  month =        {June},
}

@inproceedings{chakraborty06_comput_spread,
  author =       {Chakraborty, Koushik and Wells, Philip M. and Sohi,
                  Gurindar S.},
  title =        {Computation Spreading: Employing Hardware Migration
                  to Specialize CMP Cores on-the-Fly},
  booktitle =    {Proceedings of the 12th International Conference on
                  Architectural Support for Programming Languages and
                  Operating Systems},
  year =         2006,
  pages =        {283-292},
  doi =          {10.1145/1168857.1168893},
  url =          {https://doi.org/10.1145/1168857.1168893},
  abstract =     {In canonical parallel processing, the operating
                  system (OS) assigns a processing core to a single
                  thread from a multithreaded server
                  application. Since different threads from the same
                  application often carry out similar computation,
                  albeit at different times, we observe extensive code
                  reuse among different processors, causing redundancy
                  (e.g., in our server workloads, 45-65\% of all
                  instruction blocks are accessed by all
                  processors). Moreover, largely independent fragments
                  of computation compete for the same private
                  resources causing destructive
                  interference. Together, this redundancy and
                  interference lead to poor utilization of private
                  microarchitecture resources such as caches and
                  branch predictors.We present Computation Spreading
                  (CSP), which employs hardware migration to
                  distribute a thread's dissimilar fragments of
                  computation across the multiple processing cores of
                  a chip multiprocessor (CMP), while grouping similar
                  computation fragments from different threads
                  together. This paper focuses on a specific example
                  of CSP for OS intensive server applications:
                  separating application level (user) computation from
                  the OS calls it makes.When performing CSP, each core
                  becomes temporally specialized to execute certain
                  computation fragments, and the same core is
                  repeatedly used for such fragments. We examine two
                  specific thread assignment policies for CSP, and
                  show that these policies, across four server
                  workloads, are able to reduce instruction misses in
                  private L2 caches by 27-58\%, private L2 load misses
                  by 0-19\%, and branch mispredictions by 9-25\%.},
  address =      {New York, NY, USA},
  isbn =         1595934510,
  keywords =     {cache locality, dynamic specialization},
  location =     {San Jose, California, USA},
  numpages =     10,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS XII},
}

@inproceedings{koka05_oppor,
  author =       {Koka, Pranay, and Mikko H. Lipasti},
  title =        {Opportunities for cache friendly process scheduling},
  booktitle =    {Workshop on Interaction Between Operating Systems
                  and Computer Architecture},
  year =         2005,
}

@inproceedings{abad18_packag_aware_sched_faas_funct,
  author =       {Abad, Cristina L. and Boza, Edwin F. and van Eyk,
                  Erwin},
  title =        {Package-Aware Scheduling of FaaS Functions},
  booktitle =    {Companion of the 2018 ACM/SPEC International
                  Conference on Performance Engineering},
  year =         2018,
  pages =        {101-106},
  doi =          {10.1145/3185768.3186294},
  url =          {https://doi.org/10.1145/3185768.3186294},
  abstract =     {We consider the problem of scheduling small cloud
                  functions on serverless computing platforms. Fast
                  deployment and execution of these functions is
                  critical, for example, for microservices
                  architectures. However, functions that require large
                  packages or libraries are bloated and start
                  slowly. A solution is to cache packages at the
                  worker nodes instead of bundling them with the
                  functions. However, existing FaaS schedulers are
                  vanilla load balancers, agnostic of any packages
                  that may have been cached in response to prior
                  function executions, and cannot reap the benefits of
                  package caching (other than by chance). To address
                  this problem, we propose a package-aware scheduling
                  algorithm that tries to assign functions that
                  require the same package to the same worker
                  node. Our algorithm increases the hit rate of the
                  package cache and, as a result, reduces the latency
                  of the cloud functions. At the same time, we
                  consider the load sustained by the workers and
                  actively seek to avoid imbalance beyond a
                  configurable threshold. Our preliminary evaluation
                  shows that, even with our limited exploration of the
                  configuration space so-far, we can achieve 66 \%
                  performance improvement at the cost of a
                  (manageable) higher node imbalance.},
  address =      {New York, NY, USA},
  isbn =         9781450356299,
  keywords =     {scheduling, serverless computing,
                  functions-as-a-service, cloud computing, load
                  balancing},
  location =     {Berlin, Germany},
  numpages =     6,
  publisher =    {Association for Computing Machinery},
  series =       {ICPE '18}
}

@inproceedings{kaffes19_centr_core_granul_sched_server_funct,
  author =       {Kaffes, Kostis and Yadwadkar, Neeraja J. and
                  Kozyrakis, Christos},
  title =        {Centralized Core-Granular Scheduling for Serverless
                  Functions},
  booktitle =    {Proceedings of the ACM Symposium on Cloud Computing},
  year =         2019,
  pages =        {158-164},
  doi =          {10.1145/3357223.3362709},
  url =          {https://doi.org/10.1145/3357223.3362709},
  abstract =     {In recent years, many applications have started
                  using serverless computing platforms primarily due
                  to the ease of deployment and cost efficiency they
                  offer. However, the existing scheduling mechanisms
                  of serverless platforms fall short in catering to
                  the unique characteristics of such applications:
                  burstiness, short and variable execution times,
                  statelessness and use of a single
                  core. Specifically, the existing mechanisms fall
                  short in meeting the requirements generated due to
                  the combined effect of these characteristics:
                  scheduling at a scale of millions of function
                  invocations per second while achieving predictable
                  performance.In this paper, we argue for a
                  cluster-level centralized and core-granular
                  scheduler for serverless functions. By maintaining a
                  global view of the cluster resources, the
                  centralized approach eliminates queue imbalances
                  while the core granularity reduces interference;
                  together these properties enable reduced performance
                  variability. We expect such a scheduler to increase
                  the adoption of serverless computing platforms by
                  various latency and throughput sensitive
                  applications.},
  address =      {New York, NY, USA},
  isbn =         9781450369732,
  keywords =     {cloud computing, scheduling, resource allocation,
                  serverless computing},
  location =     {Santa Cruz, CA, USA},
  numpages =     7,
  publisher =    {Association for Computing Machinery},
  series =       {SoCC '19}
}

@inproceedings{eyk18_spec_rg_cloud_group_vision,
  author =       {van Eyk, Erwin and Iosup, Alexandru and Abad,
                  Cristina L. and Grohmann, Johannes and Eismann,
                  Simon},
  title =        {A SPEC RG Cloud Group's Vision on the Performance
                  Challenges of FaaS Cloud Architectures},
  booktitle =    {Companion of the 2018 ACM/SPEC International
                  Conference on Performance Engineering},
  year =         2018,
  pages =        {21-24},
  doi =          {10.1145/3185768.3186308},
  url =          {https://doi.org/10.1145/3185768.3186308},
  abstract =     {As a key part of the serverless computing paradigm,
                  Function-as-a-Service (FaaS) platforms enable users
                  to run arbitrary functions without being concerned
                  about operational issues. However, there are several
                  performance-related issues surrounding the
                  state-of-the-art FaaS platforms that can deter
                  widespread adoption of FaaS, including sizeable
                  overheads, unreliable performance, and new forms of
                  the cost-performance trade-off. In this work we, the
                  SPEC RG Cloud Group, identify six
                  performance-related challenges that arise
                  specifically in this FaaS model, and present our
                  roadmap to tackle these problems in the near
                  future. This paper aims at motivating the community
                  to solve these challenges together.},
  address =      {New York, NY, USA},
  isbn =         9781450356299,
  keywords =     {benchmarking, performance evaluation, serverless
                  computing, FaaS, function-as-a-service, reference
                  architecture},
  location =     {Berlin, Germany},
  numpages =     4,
  publisher =    {Association for Computing Machinery},
  series =       {ICPE '18}
}



@inproceedings{yang13_bubbl_flux,
  author =       {Yang, Hailong and Breslow, Alex and Mars, Jason and
                  Tang, Lingjia},
  title =        {Bubble-Flux: Precise Online QoS Management for
                  Increased Utilization in Warehouse Scale Computers},
  booktitle =    {Proceedings of the 40th Annual International
                  Symposium on Computer Architecture},
  year =         2013,
  pages =        {607-618},
  doi =          {10.1145/2485922.2485974},
  url =          {https://doi.org/10.1145/2485922.2485974},
  abstract =     {Ensuring the quality of service (QoS) for
                  latency-sensitive applications while allowing
                  co-locations of multiple applications on servers is
                  critical for improving server utilization and
                  reducing cost in modern warehouse-scale computers
                  (WSCs). Recent work relies on static profiling to
                  precisely predict the QoS degradation that results
                  from performance interference among co-running
                  applications to increase the number of "safe"
                  co-locations. However, these static profiling
                  techniques have several critical limitations: 1) a
                  priori knowledge of all workloads is required for
                  profiling, 2) it is difficult for the prediction to
                  capture or adapt to phase or load changes of
                  applications, and 3) the prediction technique is
                  limited to only two co-running applications.To
                  address all of these limitations, we present
                  Bubble-Flux, an integrated dynamic interference
                  measurement and online QoS management mechanism to
                  provide accurate QoS control and maximize server
                  utilization. Bubble-Flux uses a Dynamic Bubble to
                  probe servers in real time to measure the
                  instantaneous pressure on the shared hardware
                  resources and precisely predict how the QoS of a
                  latency-sensitive job will be affected by potential
                  co-runners. Once "safe" batch jobs are selected and
                  mapped to a server, Bubble-Flux uses an Online Flux
                  Engine to continuously monitor the QoS of the
                  latency-sensitive application and control the
                  execution of batch jobs to adapt to dynamic input,
                  phase, and load changes to deliver satisfactory
                  QoS. Batch applications remain in a state of flux
                  throughout execution. Our results show that the
                  utilization improvement achieved by Bubble-Flux is
                  up to 2.2x better than the prior static approach.},
  address =      {New York, NY, USA},
  isbn =         9781450320795,
  location =     {Tel-Aviv, Israel},
  numpages =     12,
  publisher =    {Association for Computing Machinery},
  series =       {ISCA '13}
}


@inproceedings{chen19_parties,
  author =       {Chen, Shuang and Delimitrou, Christina and
                  Mart\'{\i}nez, Jos\'{e} F.},
  title =        {PARTIES: QoS-Aware Resource Partitioning for
                  Multiple Interactive Services},
  booktitle =    {Proceedings of the Twenty-Fourth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems},
  year =         2019,
  pages =        {107-120},
  doi =          {10.1145/3297858.3304005},
  url =          {https://doi.org/10.1145/3297858.3304005},
  abstract =     {Multi-tenancy in modern datacenters is currently
                  limited to a single latency-critical, interactive
                  service, running alongside one or more low-priority,
                  best-effort jobs. This limits the efficiency gains
                  from multi-tenancy, especially as an increasing
                  number of cloud applications are shifting from batch
                  jobs to services with strict latency
                  requirements. We present PARTIES, a QoS-aware
                  resource manager that enables an arbitrary number of
                  interactive, latency-critical services to share a
                  physical node without QoS violations. PARTIES
                  leverages a set of hardware and software resource
                  partitioning mechanisms to adjust allocations
                  dynamically at runtime, in a way that meets the QoS
                  requirements of each co-scheduled workload, and
                  maximizes throughput for the machine. We evaluate
                  PARTIES on state-of-the-art server platforms across
                  a set of diverse interactive services. Our results
                  show that PARTIES improves throughput under QoS by
                  61 \% on average, compared to existing resource
                  managers, and that the rate of improvement increases
                  with the number of co-scheduled applications per
                  physical host.},
  address =      {New York, NY, USA},
  isbn =         9781450362405,
  keywords =     {cloud computing, resource management, isolation,
                  interference, quality of service, resource
                  partitioning, datacenters},
  location =     {Providence, RI, USA},
  numpages =     14,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS '19}
}

@inproceedings{shen19_x_contain,
  author =       {Shen, Zhiming and Sun, Zhen and Sela, Gur-Eyal and
                  Bagdasaryan, Eugene and Delimitrou, Christina and
                  Van Renesse, Robbert and Weatherspoon, Hakim},
  title =        {X-Containers: Breaking Down Barriers to Improve
                  Performance and Isolation of Cloud-Native
                  Containers},
  booktitle =    {Proceedings of the Twenty-Fourth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems},
  year =         2019,
  pages =        {121-135},
  doi =          {10.1145/3297858.3304016},
  url =          {https://doi.org/10.1145/3297858.3304016},
  abstract =     {"Cloud-native" container platforms, such as
                  Kubernetes, have become an integral part of
                  production cloud environments. One of the principles
                  in designing cloud-native applications is called
                  Single Concern Principle, which suggests that each
                  container should handle a single responsibility
                  well. In this paper, we propose X-Containers as a
                  new security paradigm for isolating single-concerned
                  cloud-native containers. Each container is run with
                  a Library OS (LibOS) that supports multi-processing
                  for concurrency and compatibility. A minimal
                  exokernel ensures strong isolation with small kernel
                  attack surface. We show an implementation of the
                  X-Containers architecture that leverages Xen
                  paravirtualization (PV) to turn Linux kernel into a
                  LibOS. Doing so results in a highly efficient LibOS
                  platform that does not require hardware-assisted
                  virtualization, improves inter-container isolation,
                  and supports binary compatibility and
                  multi-processing. By eliminating some security
                  barriers such as seccomp and Meltdown patch,
                  X-Containers have up to 27X higher raw system call
                  throughput compared to Docker containers, while also
                  achieving competitive or superior performance on
                  various benchmarks compared to recent container
                  platforms such as Google's gVisor and Intel's Clear
                  Containers.},
  address =      {New York, NY, USA},
  isbn =         9781450362405,
  keywords =     {exokernel, cloud-native, library os, containers,
                  x-containers},
  location =     {Providence, RI, USA},
  numpages =     15,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS '19}
}


@book{smith05_virtual,
  author =       {Smith, Jim and Nair, Ravi},
  title =        {Virtual machines: versatile platforms for systems
                  and processes},
  year =         2005,
  publisher =    {Elsevier},
  url =          {https://doi.org/B978-1-55860-910-5.X5000-9},
  doi =          {B978-1-55860-910-5.X5000-9},
  isbn =         {978-1-55860-910-5},
  numpages =     656,
}


@inproceedings{zhuravlev10_addres_shared_resour_conten_multic_proces_sched,
  author =       {Zhuravlev, Sergey and Blagodurov, Sergey and
                  Fedorova, Alexandra},
  title =        {Addressing Shared Resource Contention in Multicore
                  Processors via Scheduling},
  booktitle =    {Proceedings of the Fifteenth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems},
  year =         2010,
  pages =        {129-142},
  doi =          {10.1145/1736020.1736036},
  url =          {https://doi.org/10.1145/1736020.1736036},
  abstract =     {Contention for shared resources on multicore
                  processors remains an unsolved problem in existing
                  systems despite significant research efforts
                  dedicated to this problem in the past. Previous
                  solutions focused primarily on hardware techniques
                  and software page coloring to mitigate this
                  problem. Our goal is to investigate how and to what
                  extent contention for shared resource can be
                  mitigated via thread scheduling. Scheduling is an
                  attractive tool, because it does not require extra
                  hardware and is relatively easy to integrate into
                  the system. Our study is the first to provide a
                  comprehensive analysis of contention-mitigating
                  techniques that use only scheduling. The most
                  difficult part of the problem is to find a
                  classification scheme for threads, which would
                  determine how they affect each other when competing
                  for shared resources. We provide a comprehensive
                  analysis of such classification schemes using a
                  newly proposed methodology that enables to evaluate
                  these schemes separately from the scheduling
                  algorithm itself and to compare them to the
                  optimal. As a result of this analysis we discovered
                  a classification scheme that addresses not only
                  contention for cache space, but contention for other
                  shared resources, such as the memory controller,
                  memory bus and prefetching hardware. To show the
                  applicability of our analysis we design a new
                  scheduling algorithm, which we prototype at user
                  level, and demonstrate that it performs within 2 \%
                  of the optimal. We also conclude that the highest
                  impact of contention-aware scheduling techniques is
                  not in improving performance of a workload as a
                  whole but in improving quality of service or
                  performance isolation for individual applications.},
  address =      {New York, NY, USA},
  isbn =         9781605588391,
  keywords =     {scheduling, shared resource contention, multicore
                  processors},
  location =     {Pittsburgh, Pennsylvania, USA},
  numpages =     14,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS XV},
}

@inproceedings{petrucci15_octop_man,
  author =       {V. {Petrucci} and M. A. {Laurenzano} and
                  J. {Doherty} and Y. {Zhang} and D. {Moss{\'e}} and
                  J. {Mars} and L. {Tang}},
  title =        {Octopus-Man: QoS-driven task management for
                  heterogeneous multicores in warehouse-scale
                  computers},
  booktitle =    {2015 IEEE 21st International Symposium on High
                  Performance Computer Architecture (HPCA)},
  year =         2015,
  pages =        {246-258},
  doi =          {10.1109/HPCA.2015.7056037},
  url =          {https://doi.org/10.1109/HPCA.2015.7056037},
  ISSN =         {2378-203X},
  abstract =     {Heterogeneous multicore architectures have the
                  potential to improve energy efficiency by
                  integrating power-efficient wimpy cores with
                  high-performing brawny cores. However, it is an open
                  question as how to deliver energy reduction while
                  ensuring the quality of service (QoS) of
                  latency-sensitive web-services running on such
                  heterogeneous multicores in warehouse-scale
                  computers (WSCs). In this work, we first investigate
                  the implications of heterogeneous multicores in WSCs
                  and show that directly adopting heterogeneous
                  multicores without re-designing the software stack
                  to provide QoS management leads to significant QoS
                  violations. We then present Octopus-Man, a novel
                  QoS-aware task management solution that dynamically
                  maps latency-sensitive tasks to the least
                  power-hungry processing resources that are
                  sufficient to meet the QoS requirements. Using
                  carefully-designed feedback-control mechanisms,
                  Octopus-Man addresses critical challenges that
                  emerge due to uncertainties in workload fluctuations
                  and adaptation dynamics in a real system. Our
                  evaluation using web-search and memcached running on
                  a real-system Intel heterogeneous prototype
                  demonstrates that Octopus-Man improves energy
                  efficiency by up to 41 \% (CPU power) and up to 15
                  \% (system power) over an all-brawny WSC design
                  while adhering to specified QoS targets.},
  keywords =     {energy conservation;multiprocessing systems;power
                  aware computing;quality of service;Web
                  services;QoS-driven task management;warehouse-scale
                  computers;heterogeneous multicore
                  architectures;energy efficiency;power-efficient
                  wimpy cores;high-performing brawny cores;energy
                  reduction;quality of service;latency-sensitive
                  Web-services;software stack;QoS management;QoS
                  violations;Octopus-Man;QoS-aware task management
                  solution;latency-sensitive tasks;power-hungry
                  processing resources;QoS
                  requirements;feedback-control mechanisms;workload
                  fluctuations;adaptation dynamics;Web-search;Intel
                  heterogeneous prototype;CPU power;system power;WSC
                  design;Quality of service;Multicore
                  processing;Servers;Runtime;Prototypes;Monitoring},
  month =        {Feb},
}

@inproceedings{bohn11_nist_cloud_comput_refer_archit,
  author =       {R. B. {Bohn} and J. {Messina} and F. {Liu} and
                  J. {Tong} and J. {Mao}},
  title =        {NIST Cloud Computing Reference Architecture},
  booktitle =    {2011 IEEE World Congress on Services},
  year =         2011,
  pages =        {594-596},
  doi =          {10.1109/SERVICES.2011.105},
  url =          {https://doi.org/10.1109/SERVICES.2011.105},
  ISSN =         {2378-3818},
  abstract =     {This paper presents the first version of the NIST
                  Cloud Computing Reference Architecture (RA). This is
                  a vendor neutral conceptual model that concentrates
                  on the role and interactions of the identified
                  actors in the cloud computing sphere. Five primary
                  actors were identified - Cloud Service Consumer,
                  Cloud Service Provider, Cloud Broker, Cloud Auditor
                  and Cloud Carrier. Their roles and activities are
                  discussed in this report. A primary goal for
                  generating this model was to give the United States
                  Government (USG) a method for understanding and
                  communicating the components of a cloud computing
                  system for Federal IT executives, Program Managers
                  and IT procurement officials.},
  keywords =     {cloud computing;service-oriented
                  architecture;software development management;NIST
                  cloud computing reference architecture;vendor
                  neutral conceptual model;cloud service
                  consumer;cloud service provider;cloud broker;cloud
                  auditor;cloud carrier;United States
                  Government;federal IT executives;program managers;IT
                  procurement officials;Cloud computing;Computer
                  architecture;Security;NIST;Computational
                  modeling;Service oriented architecture},
  month =        {July}
}

@article{hoseinyfarahabady18_model_predic_contr_manag_qos,
  author =       {M. R. {HoseinyFarahabady} and A. Y. {Zomaya} and
                  Z. {Tari}},
  title =        {A Model Predictive Controller for Managing Qos
                  Enforcements and Microarchitecture-Level
                  Interferences in a Lambda Platform},
  volume =       29,
  number =       7,
  pages =        {1442-1455},
  year =         2018,
  doi =          {10.1109/TPDS.2017.2779502},
  url =          {https://doi.org/10.1109/TPDS.2017.2779502},
  ISSN =         {1558-2183},
  abstract =     {Lambda paradigm, also known as Function as a Service
                  (FaaS), is a novel event-driven concept that allows
                  companies to build scalable and reliable enterprise
                  applications in an off-premise computing data-center
                  as a serverless solution. In practice, however, an
                  important goal for the service provider of a Lambda
                  platform is to devise an efficient way to
                  consolidate multiple Lambda functions in a single
                  host. While the majority of existing resource
                  management solutions use only operating-system level
                  metrics (e.g., average utilization of computing and
                  I/O resources) to allocate the available resources
                  among the submitted workloads in a balanced way, a
                  resource allocation schema that is oblivious to the
                  issue of shared-resource contention can result in a
                  significant performance variability and degradation
                  within the entire platform. This paper proposes a
                  predictive controller scheme that dynamically
                  allocates resources in a Lambda platform. This
                  scheme uses a prediction tool to estimate the future
                  rate of every event stream and takes into account
                  the quality of service enforcements requested by the
                  owner of each Lambda function. This is formulated as
                  an optimization problem where a set of cost
                  functions are introduced (i) to reduce the total QoS
                  violation incidents; (ii) to keep the CPU
                  utilization level within an accepted range; and
                  (iii) to avoid the fierce contention among
                  collocated applications for obtaining shared
                  resources. Performance evaluation is carried out by
                  comparing the proposed solution with an enhanced
                  interference-aware version of three well-known
                  heuristics, namely spread, binpack (the two native
                  clustering solutions employed by Docker Swarm) and
                  best-effort resource allocation schema. Experimental
                  results show that the proposed controller improves
                  the overall performance (in terms of reducing the
                  end-to-end response time) by 14.9 percent on average
                  compared to the best result of the other
                  heuristics. The proposed solution also increases the
                  overall CPU utilization by 18 percent on average
                  (for lightweight workloads), while achieves an
                  average 87 percent (maximum 146 percent) improvement
                  in preventing QoS violation incidents.},
  journaltitle = {IEEE Transactions on Parallel and Distributed
                  Systems},
  keywords =     {application program interfaces;cloud
                  computing;computer centres;optimal
                  control;optimisation;predictive control;quality of
                  service;resource allocation;software
                  engineering;virtualisation;model predictive
                  controller;QoS enforcements;microarchitecture-level
                  interferences;Lambda platform;Lambda
                  paradigm;reliable enterprise applications;serverless
                  solution;multiple Lambda functions;resource
                  management solutions;operating-system level
                  metrics;resource allocation schema;shared-resource
                  contention;function as a service;quality of
                  service;total QoS violation incident reduction;CPU
                  utilization;enhanced interference-awareness;Docker
                  Swarm;Resource management;Quality of
                  service;Interference;Bandwidth;Measurement;Servers;Cloud
                  computing;Serverless lambda platform;function as a
                  service (FaaS);model predictive control;dynamic
                  resource allocation/scheduling;performance
                  degradation},
  month =        {July}
}

@patent{muff12_system,
  abstract =     {A hypervisor and one or more programs, e.g., guest
                  operating systems and/or user processes or
                  applications hosted by the hypervisor to configured
                  to selectively save and restore the state of branch
                  prediction logic through separate hypervisor-mode
                  and guest-mode and/or user-mode instructions. By
                  doing so, different branch prediction strategies may
                  be employed for different operating systems and user
                  applications hosted thereby to provide finer grained
                  optimization of the branch prediction logic.},
  author =       {Adam J. Muff and Paul E. Schardt and Robert
                  A. Shearer and Matthew R. Tubbs},
  month =        1,
  number =       {US8935694B2},
  title =        {System and method for selectively saving and
                  restoring state of branch prediction logic through
                  separate hypervisor-mode and guest-mode and/or
                  user-mode instructions},
  type =         {patentus},
  year =         2012,
}

@inproceedings{du20_catal,
  author =       {Du, Dong and Yu, Tianyi and Xia, Yubin and Zang,
                  Binyu and Yan, Guanglu and Qin, Chenggang and Wu,
                  Qixuan and Chen, Haibo},
  title =        {Catalyzer: Sub-Millisecond Startup for Serverless
                  Computing with Initialization-Less Booting},
  booktitle =    {Proceedings of the Twenty-Fifth International
                  Conference on Architectural Support for Programming
                  Languages and Operating Systems},
  year =         2020,
  pages =        {467-481},
  doi =          {10.1145/3373376.3378512},
  url =          {https://doi.org/10.1145/3373376.3378512},
  abstract =     {Serverless computing promises cost-efficiency and
                  elasticity for high-productive software
                  development. To achieve this, the serverless sandbox
                  system must address two challenges: strong isolation
                  between function instances, and low startup latency
                  to ensure user experience. While strong isolation
                  can be provided by virtualization-based sandboxes,
                  the initialization of sandbox and application causes
                  non-negligible startup overhead. Conventional
                  sandbox systems fall short in low-latency startup
                  due to their application-agnostic nature: they can
                  only reduce the latency of sandbox initialization
                  through hypervisor and guest kernel customization,
                  which is inadequate and does not mitigate the
                  majority of startup overhead.This paper proposes
                  Catalyzer, a serverless sandbox system design
                  providing both strong isolation and extremely fast
                  function startup. Instead of booting from scratch,
                  Catalyzer restores a virtualization-based function
                  instance from a well-formed checkpoint image and
                  thereby skips the initialization on the critical
                  path (init-less). Catalyzer boosts the restore
                  performance by on-demand recovering both user-level
                  memory state and system state. We also propose a new
                  OS primitive, sfork (sandbox fork), to further
                  reduce the startup latency by directly reusing the
                  state of a running sandbox instance. Fundamentally,
                  Catalyzer removes the initialization cost by reusing
                  state, which enables general optimizations for
                  diverse serverless functions. The evaluation shows
                  that Catalyzer reduces startup latency by orders of
                  magnitude, achieves < 1ms latency in the best case,
                  and significantly reduces the end-to-end latency for
                  real-world workloads. Catalyzer has been adopted by
                  Ant Financial, and we also present lessons learned
                  from industrial development.},
  address =      {New York, NY, USA},
  isbn =         9781450371025,
  keywords =     {startup latency, serverless computing, checkpoint
                  and restore, operating system},
  location =     {Lausanne, Switzerland},
  numpages =     15,
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS '20},
}

@inproceedings{prodan09,
  author =       {R. {Prodan} and S. {Ostermann}},
  title =        {A survey and taxonomy of infrastructure as a service
                  and web hosting cloud providers},
  booktitle =    {2009 10th IEEE/ACM International Conference on Grid
                  Computing},
  year =         2009,
  pages =        {17-25},
  doi =          {10.1109/GRID.2009.5353074},
  url =          {https://doi.org/10.1109/GRID.2009.5353074},
  ISSN =         {2152-1093},
  abstract =     {With an increasing number of providers claiming to
                  offer Cloud infrastructures, there is a lack in the
                  community for a common terminology, accompanied by a
                  clear definition and classification of Cloud
                  features. We conduct in this paper a survey on a
                  selection of Cloud providers, and propose a taxonomy
                  of eight important Cloud computing elements covering
                  service type, resource deployment, hardware, runtime
                  tuning, business model, middleware, and
                  performance. We conclude that the provisioning of
                  Service Level Agreements as utilities, of open and
                  interoperable middleware solutions, as well as of
                  sustained performance metrics for high-performance
                  computing applications are three elements with the
                  highest need of further community research.},
  keywords =     {Internet;middleware;open systems;infrastructure as a
                  service;Web hosting cloud providers;cloud
                  infrastructure;cloud computing elements;service
                  type;resource deployment;runtime tuning;business
                  model;service level agreements;interoperable
                  middleware solution;Taxonomy;Cloud
                  computing;Hardware;Companies;Software
                  maintenance;Terminology;Middleware;Web and internet
                  services;Computer science;Runtime},
  month =        {Oct},
}

@inproceedings{qureshi06_utilit_based_cache_partit,
  author =       {M. K. {Qureshi} and Y. N. {Patt}},
  title =        {Utility-Based Cache Partitioning: A Low-Overhead,
                  High-Performance, Runtime Mechanism to Partition
                  Shared Caches},
  booktitle =    {2006 39th Annual IEEE/ACM International Symposium on
                  Microarchitecture (MICRO'06)},
  year =         2006,
  pages =        {423-432},
  doi =          {10.1109/MICRO.2006.49},
  url =          {https://doi.org/10.1109/MICRO.2006.49},
  ISSN =         {2379-3155},
  abstract =     {This paper investigates the problem of partitioning
                  a shared cache between multiple concurrently
                  executing applications. The commonly used LRU policy
                  implicitly partitions a shared cache on a demand
                  basis, giving more cache resources to the
                  application that has a high demand and fewer cache
                  resources to the application that has a low
                  demand. However, a higher demand for cache resources
                  does not always correlate with a higher performance
                  from additional cache resources. It is beneficial
                  for performance to invest cache resources in the
                  application that benefits more from the cache
                  resources rather than in the application that has
                  more demand for the cache resources. This paper
                  proposes utility-based cache partitioning (UCP), a
                  low-overhead, runtime mechanism that partitions a
                  shared cache between multiple applications depending
                  on the reduction in cache misses that each
                  application is likely to obtain for a given amount
                  of cache resources. The proposed mechanism monitors
                  each application at runtime using a novel,
                  cost-effective, hardware circuit that requires less
                  than 2kB of storage. The information collected by
                  the monitoring circuits is used by a partitioning
                  algorithm to decide the amount of cache resources
                  allocated to each application. Our evaluation, with
                  20 multiprogrammed workloads, shows that UCP
                  improves performance of a dual-core system by up to
                  23 \% and on average 11 \% over LRU-based cache
                  partitioning},
  keywords =     {cache storage;logic partitioning;microprocessor
                  chips;multiprogramming;resource
                  allocation;utility-based cache partitioning;runtime
                  mechanism;LRU policy;multiprogrammed
                  workloads;LRU-based cache partitioning;on-chip cache
                  resource;Runtime;Application
                  software;Circuits;Yarn;Hardware;Monitoring;Partitioning
                  algorithms;Resource management;Multicore processing},
  month =        {Dec}
}

@inproceedings{oakes17_pipsq,
  author =       {E. {Oakes} and L. {Yang} and K. {Houck} and
                  T. {Harter} and A. C. {Arpaci-Dusseau} and
                  R. H. {Arpaci-Dusseau}},
  title =        {Pipsqueak: Lean Lambdas with Large Libraries},
  booktitle =    {2017 IEEE 37th International Conference on
                  Distributed Computing Systems Workshops (ICDCSW)},
  year =         2017,
  pages =        {395-400},
  doi =          {10.1109/ICDCSW.2017.32},
  url =          {https://doi.org/10.1109/ICDCSW.2017.32},
  ISSN =         {2332-5666},
  abstract =     {Microservices are usually fast to deploy because
                  each microservice is small, and thus each can be
                  installed and started quickly. Unfortunately, lean
                  microservices that depend on large libraries will
                  start slowly and harm elasticity. In this paper, we
                  explore the challenges of lean microservices that
                  rely on large libraries in the context of Python
                  packages and the OpenLambda serverless computing
                  platform. We analyze the package types and
                  compressibility of libraries distributed via the
                  Python Package Index and propose PipBench, a new
                  tool for evaluating package support. We also propose
                  Pipsqueak, a package-aware compute platform based on
                  OpenLambda.},
  keywords =     {programming languages;software libraries;large
                  libraries;Python packages;OpenLambda serverless
                  computing platform;Python package index;cloud
                  computing;Libraries;Containers;Virtual
                  machining;Tools;Cloud computing;Memory
                  management;Linux;serverless computing;cloud
                  computing;distributed computing;distributed
                  systems;python;distributed cache;software
                  repository},
  month =        {June},
}

@article{zhuravlev12_survey_sched_techn_addres_shared,
  author =       {Zhuravlev, Sergey and Saez, Juan Carlos and
                  Blagodurov, Sergey and Fedorova, Alexandra and
                  Prieto, Manuel},
  title =        {Survey of Scheduling Techniques for Addressing
                  Shared Resources in Multicore Processors},
  volume =       45,
  number =       1,
  year =         2012,
  doi =          {10.1145/2379776.2379780},
  url =          {https://doi.org/10.1145/2379776.2379780},
  abstract =     {Chip multicore processors (CMPs) have emerged as the
                  dominant architecture choice for modern computing
                  platforms and will most likely continue to be
                  dominant well into the foreseeable future. As with
                  any system, CMPs offer a unique set of
                  challenges. Chief among them is the shared resource
                  contention that results because CMP cores are not
                  independent processors but rather share common
                  resources among cores such as the last level cache
                  (LLC). Shared resource contention can lead to severe
                  and unpredictable performance impact on the threads
                  running on the CMP. Conversely, CMPs offer
                  tremendous opportunities for mulithreaded
                  applications, which can take advantage of
                  simultaneous thread execution as well as fast inter
                  thread data sharing. Many solutions have been
                  proposed to deal with the negative aspects of CMPs
                  and take advantage of the positive. This survey
                  focuses on the subset of these solutions that
                  exclusively make use of OS thread-level scheduling
                  to achieve their goals. These solutions are
                  particularly attractive as they require no changes
                  to hardware and minimal or no changes to the OS. The
                  OS scheduler has expanded well beyond its original
                  role of time-multiplexing threads on a single core
                  into a complex and effective resource manager. This
                  article surveys a multitude of new and exciting work
                  that explores the diverse new roles the OS scheduler
                  can successfully take on.},
  address =      {New York, NY, USA},
  articleno =    4,
  issn =         {0360-0300},
  issue_date =   {November 2012},
  journaltitle = {ACM Comput. Surv.},
  keywords =     {cooperative resource sharing, shared resource
                  contention, Survey, power-aware scheduling, thread
                  level scheduling, thermal effects},
  month =        dec,
  numpages =     28,
  publisher =    {Association for Computing Machinery},
}

@unpublished{kumar20_shoot_down_server_front_end_bottl,
  abstract =     {The front-end bottleneck is a well-established
                  problem in servers workloads owing to their deep
                  software stacks and large instruction
                  footprints. Despite years of research into effective
                  L1-I and BTB prefetching, state-of-the-art
                  techniques force a trade-off between metadata
                  storage cost and performance. Temporal Stream
                  prefetchers deliver high performance but require a
                  prohibitive amount of metadata to accommodate the
                  temporal history. Meanwhile, BTB-directed
                  prefetchers incur low cost by using the existing
                  in-core branch prediction structures, but fall short
                  on performance due to BTB's inability to capture the
                  massive control flow working set of server
                  applications. This work overcomes the fundamental
                  limitation of BTB-directed prefetchers, which is
                  capturing a large control flow working set within an
                  affordable BTB storage budget. We re-envision the
                  BTB organization to maximize its control flow
                  coverage by observing that an application's
                  instruction footprint can be mapped as a combination
                  of its unconditional branch working set and, for
                  each unconditional branch, a spatial encoding of the
                  cache blocks around the branch target. Effectively
                  capturing a map of the application's instruction
                  footprint in the BTB enables highly effective
                  BTB-directed prefetching that outperforms the
                  state-of-the-art prefetchers by up to 14 \% for
                  equivalent storage budget.},
  author =       {Rakesh Kumar and Boris Grot},
  title =        {Shooting Down The Server Front-End Bottleneck},
  year =         2020,
}

@misc{kumar20_shoot_down_server_front_end,
  author =       {Rakesh Kumar and Boris Grot},
  title =        {Shooting Down The Server Front-End Bottleneck -
                  Summary of Differences},
  year =         2020
}

@inproceedings{yu20_charac_server_platf_server,
  author =       {Yu, Tianyi and Liu, Qingyuan and Du, Dong and Xia,
                  Yubin and Zang, Binyu and Lu, Ziqian and Yang,
                  Pingchao and Qin, Chenggang and Chen, Haibo},
  title =        {Characterizing Serverless Platforms with
                  Serverlessbench},
  booktitle =    {Proceedings of the 11th ACM Symposium on Cloud
                  Computing},
  year =         2020,
  pages =        {30-44},
  doi =          {10.1145/3419111.3421280},
  url =          {https://doi.org/10.1145/3419111.3421280},
  abstract =     {Serverless computing promises auto-scalability and
                  cost-efficiency (in "pay-as-you-go" manner) for
                  high-productive software development. Because of its
                  virtue, serverless computing has motivated
                  increasingly new applications and services in the
                  cloud. This, however, also presents new challenges
                  including how to efficiently design high-performance
                  serverless platforms and how to efficiently program
                  on the platforms.This paper proposes
                  ServerlessBench, an open-source benchmark suite for
                  characterizing serverless platforms. It includes
                  test cases exploring characteristic metrics of
                  serverless computing, e.g., communication
                  efficiency, startup latency, stateless overhead, and
                  performance isolation. We have applied the benchmark
                  suite to evaluate the most popular serverless
                  computing platforms, including AWS Lambda,
                  Open-Whisk, and Fn, and present new serverless
                  implications from the study. For example, we show
                  scenarios where decoupling an application into a
                  composition of serverless functions can be
                  beneficial in cost-saving and performance, and that
                  the "stateless" property in serverless computing can
                  hurt the execution performance of serverless
                  functions. These implications form several design
                  guidelines, which may help platform designers to
                  optimize serverless platforms and application
                  developers to design their functions best fit to the
                  platforms.},
  address =      {New York, NY, USA},
  isbn =         9781450381376,
  location =     {Virtual Event, USA},
  numpages =     15,
  publisher =    {Association for Computing Machinery},
  series =       {SoCC '20}
}

@article{ishii55_rebas_instr_prefet,
  author =       {Y. Ishii and J. Lee and K. Nathella and D. Sunwoo},
  title =        {Rebasing Instruction Prefetching: an Industry
                  Perspective},
  journal =      {IEEE Computer Architecture Letters},
  volume =       {preprint},
  number =       01,
  pages =        {1-1},
  year =         2020,
  doi =          {10.1109/LCA.2020.3035068},
  url =          {https://doi.org/10.1109/LCA.2020.3035068},
  abstract =     {Instruction prefetching can play a pivotal role in
                  improving the performance of workloads with large
                  instruction footprints and frequent, costly frontend
                  stalls. In particular, Fetch Directed Prefetching
                  (FDP) is an effective technique to mitigate frontend
                  stalls since it leverages existing branch prediction
                  resources in a processor and incurs very little
                  hardware overhead. Modern processors have been
                  trending towards provisioning more frontend
                  resources, and this bodes well for FDP as it
                  requires these resources to be effective. However,
                  recent academic research has been using outdated and
                  less than optimal frontend baselines that employ
                  smaller structures, which may result in misleading
                  outcomes. In this paper, we present a detailed FDP
                  microarchitecture and evaluate two improvements,
                  better branch history management and post fetch
                  correction. We believe that our FDP-based frontend
                  design can serve as a new reference baseline for
                  instruction prefetching research to bridge the gap
                  between academia and industry.},
  address =      {Los Alamitos, CA, USA},
  issn =         {1556-6064},
  keywords =     {},
  month =        10,
  publisher =    {IEEE Computer Society},
}


@inproceedings{jevdjic13_stack_dram_caches_server,
  author =       {Jevdjic, Djordje and Volos, Stavros and Falsafi,
                  Babak},
  title =        {Die-Stacked DRAM Caches for Servers: Hit Ratio,
                  Latency, or Bandwidth? Have It All with Footprint
                  Cache},
  booktitle =    {Proceedings of the 40th Annual International
                  Symposium on Computer Architecture},
  year =         {2013},
  pages =        {404-415},
  doi =          {10.1145/2485922.2485957},
  url =          {https://doi.org/10.1145/2485922.2485957},
  abstract =     {Recent research advocates using large die-stacked
                  DRAM caches to break the memory bandwidth
                  wall. Existing DRAM cache designs fall into one of
                  two categories --- block-based and page-based. The
                  former organize data in conventional blocks (e.g.,
                  64B), ensuring low off-chip bandwidth utilization,
                  but co-locate tags and data in the stacked DRAM,
                  incurring high lookup latency. Furthermore, such
                  designs suffer from low hit ratios due to poor
                  temporal locality. In contrast, page-based caches,
                  which manage data at larger granularity (e.g., 4KB
                  pages), allow for reduced tag array overhead and
                  fast lookup, and leverage high spatial locality at
                  the cost of moving large amounts of data on and off
                  the chip.This paper introduces Footprint Cache, an
                  efficient die-stacked DRAM cache design for server
                  processors. Footprint Cache allocates data at the
                  granularity of pages, but identifies and fetches
                  only those blocks within a page that will be touched
                  during the page's residency in the cache --- i.e.,
                  the page's footprint. In doing so, Footprint Cache
                  eliminates the excessive off-chip traffic associated
                  with page-based designs, while preserving their high
                  hit ratio, small tag array overhead, and low lookup
                  latency. Cycle-accurate simulation results of a
                  16-core server with up to 512MB Footprint Cache
                  indicate a 57 \% performance improvement over a
                  baseline chip without a die-stacked cache. Compared
                  to a state-of-the-art block-based design, our design
                  improves performance by 13 \% while reducing dynamic
                  energy of stacked DRAM by 24 \%.},
  address =      {New York, NY, USA},
  isbn =         {9781450320795},
  location =     {Tel-Aviv, Israel},
  numpages =     {12},
  publisher =    {Association for Computing Machinery},
  series =       {ISCA '13}
}



@inproceedings{hameed10_under_sourc_ineff_gener_purpos_chips,
  author =       {Hameed, Rehan and Qadeer, Wajahat and Wachs, Megan
                  and Azizi, Omid and Solomatnikov, Alex and Lee,
                  Benjamin C. and Richardson, Stephen and Kozyrakis,
                  Christos and Horowitz, Mark},
  title =        {Understanding Sources of Inefficiency in
                  General-Purpose Chips},
  booktitle =    {Proceedings of the 37th Annual International
                  Symposium on Computer Architecture},
  year =         2010,
  pages =        {37-47},
  doi =          {10.1145/1815961.1815968},
  url =          {https://doi.org/10.1145/1815961.1815968},
  abstract =     {Due to their high volume, general-purpose
                  processors, and now chip multiprocessors (CMPs), are
                  much more cost effective than ASICs, but lag
                  significantly in terms of performance and energy
                  efficiency. This paper explores the sources of these
                  performance and energy overheads in general-purpose
                  processing systems by quantifying the overheads of a
                  720p HD H.264 encoder running on a general-purpose
                  CMP system. It then explores methods to eliminate
                  these overheads by transforming the CPU into a
                  specialized system for H.264 encoding. We evaluate
                  the gains from customizations useful to broad
                  classes of algorithms, such as SIMD units, as well
                  as those specific to particular computation, such as
                  customized storage and functional units.The ASIC is
                  500x more energy efficient than our original
                  four-processor CMP. Broadly applicable optimizations
                  improve performance by 10x and energy by
                  7x. However, the very low energy costs of actual
                  core ops (100s fJ in 90nm) mean that over 90 \% of
                  the energy used in these solutions is still
                  "overhead". Achieving ASIC-like performance and
                  efficiency requires algorithm-specific
                  optimizations. For each sub-algorithm of H.264, we
                  create a large, specialized functional unit that is
                  capable of executing 100s of operations per
                  instruction. This improves performance and energy by
                  an additional 25x and the final customized CMP
                  matches an ASIC solution's performance within 3x of
                  its energy and within comparable area.},
  address =      {New York, NY, USA},
  isbn =         9781450300537,
  keywords =     {ASIC, chip multiprocessor, customization, tensilica,
                  energy efficiency, high performance, h.264},
  location =     {Saint-Malo, France},
  numpages =     11,
  publisher =    {Association for Computing Machinery},
  series =       {ISCA '10},
}

@inproceedings{rhu13_local_aware_memor_hierar_energ,
  author =       {Rhu, Minsoo and Sullivan, Michael and Leng, Jingwen
                  and Erez, Mattan},
  title =        {A Locality-Aware Memory Hierarchy for
                  Energy-Efficient GPU Architectures},
  booktitle =    {Proceedings of the 46th Annual IEEE/ACM
                  International Symposium on Microarchitecture},
  year =         2013,
  pages =        {86-98},
  doi =          {10.1145/2540708.2540717},
  url =          {https://doi.org/10.1145/2540708.2540717},
  abstract =     {As GPU's compute capabilities grow, their memory
                  hierarchy increasingly becomes a bottleneck. Current
                  GPU memory hierarchies use coarse-grained memory
                  accesses to exploit spatial locality, maximize peak
                  bandwidth, simplify control, and reduce cache
                  meta-data storage. These coarse-grained memory
                  accesses, however, are a poor match for emerging GPU
                  applications with irregular control flow and memory
                  access patterns. Meanwhile, the massive
                  multi-threading of GPUs and the simplicity of their
                  cache hierarchies make CPU-specific memory system
                  enhancements ineffective for improving the
                  performance of irregular GPU applications. We design
                  and evaluate a locality-aware memory hierarchy for
                  throughput processors, such as GPUs. Our proposed
                  design retains the advantages of coarse-grained
                  accesses for spatially and temporally local programs
                  while permitting selective fine-grained access to
                  memory. By adaptively adjusting the access
                  granularity, memory bandwidth and energy are reduced
                  for data with low spatial/temporal locality without
                  wasting control overheads or prefetching potential
                  for data with high spatial locality. As such, our
                  locality-aware memory hierarchy improves GPU
                  performance, energy-efficiency, and memory
                  throughput for a large range of applications.},
  address =      {New York, NY, USA},
  isbn =         9781450326384,
  keywords =     {SIMT, adaptive granularity memory, fine-grained
                  memory access, irregular memory access patterns,
                  GPU, SIMD},
  location =     {Davis, California},
  numpages =     13,
  publisher =    {Association for Computing Machinery},
  series =       {MICRO-46},
}

@inproceedings{zebchuk13_multi_grain_coher_direc,
  author =       {Zebchuk, Jason and Falsafi, Babak and Moshovos,
                  Andreas},
  title =        {Multi-Grain Coherence Directories},
  booktitle =    {Proceedings of the 46th Annual IEEE/ACM
                  International Symposium on Microarchitecture},
  year =         2013,
  pages =        {359-370},
  doi =          {10.1145/2540708.2540739},
  url =          {https://doi.org/10.1145/2540708.2540739},
  abstract =     {Conventional directory coherence operates at the
                  finest granularity possible, that of a cache
                  block. While simple, this organization fails to
                  exploit frequent application behavior: at any given
                  point in time, large, continuous chunks of memory
                  are often accessed only by a single core.We take
                  advantage of this behavior and investigate reducing
                  the coherence directory size by tracking coherence
                  at multiple different granularities. We show that
                  such a Multi-grain Directory (MGD) can significantly
                  reduce the required number of directory entries
                  across a variety of different workloads. Our
                  analysis shows a simple dual-grain directory (DGD)
                  obtains the majority of the benefit while tracking
                  individual cache blocks and coarse-grain regions of
                  1KB to 8KB. We propose a practical DGD design that
                  is transparent to software, requires no changes to
                  the coherence protocol, and has no unnecessary
                  bandwidth overhead. This design can reduce the
                  coherence directory size by 41 \% to 66 \% with no
                  statistically significant performance loss.},
  address =      {New York, NY, USA},
  isbn =         9781450326384,
  keywords =     {coherence directory, cache coherence},
  location =     {Davis, California},
  numpages =     12,
  publisher =    {Association for Computing Machinery},
  series =       {MICRO-46}
}

@article{putnam16_recon_fabric_accel_large_scale_datac_servic,
  author =       {Putnam, Andrew and Caulfield, Adrian M. and Chung,
                  Eric S. and Chiou, Derek and Constantinides, Kypros
                  and Demme, John and Esmaeilzadeh, Hadi and Fowers,
                  Jeremy and Gopal, Gopi Prashanth and Gray, Jan and
                  Haselman, Michael and Hauck, Scott and Heil, Stephen
                  and Hormati, Amir and Kim, Joo-Young and Lanka,
                  Sitaram and Larus, James and Peterson, Eric and
                  Pope, Simon and Smith, Aaron and Thong, Jason and
                  Xiao, Phillip Yi and Burger, Doug},
  title =        {A Reconfigurable Fabric for Accelerating Large-Scale
                  Datacenter Services},
  journal =      {Commun. ACM},
  volume =       {59},
  number =       {11},
  pages =        {114-122},
  year =         {2016},
  doi =          {10.1145/2996868},
  url =          {https://doi.org/10.1145/2996868},
  abstract =     {Datacenter workloads demand high computational
                  capabilities, flexibility, power efficiency, and low
                  cost. It is challenging to improve all of these
                  factors simultaneously. To advance datacenter
                  capabilities beyond what commodity server designs
                  can provide, we designed and built a composable,
                  reconfigurable hardware fabric based on field
                  programmable gate arrays (FPGA). Each server in the
                  fabric contains one FPGA, and all FPGAs within a
                  48-server rack are interconnected over a
                  low-latency, high-bandwidth network.We describe a
                  medium-scale deployment of this fabric on a bed of
                  1632 servers, and measure its effectiveness in
                  accelerating the ranking component of the Bing web
                  search engine. We describe the requirements and
                  architecture of the system, detail the critical
                  engineering challenges and solutions needed to make
                  the system robust in the presence of failures, and
                  measure the performance, power, and resilience of
                  the system. Under high load, the large-scale
                  reconfigurable fabric improves the ranking
                  throughput of each server by 95 \% at a desirable
                  latency distribution or reduces tail latency by 29
                  \% at a fixed throughput. In other words, the
                  reconfigurable fabric enables the same throughput
                  using only half the number of servers.},
  address =      {New York, NY, USA},
  issn =         {0001-0782},
  issue_date =   {November 2016},
  month =        oct,
  numpages =     {9},
  publisher =    {Association for Computing Machinery}
}

@inproceedings{arelakis14_sc2,
  author =       {Arelakis, Angelos and Stenstrom, Per},
  title =        {SC2: A Statistical Compression Cache Scheme},
  booktitle =    {Proceeding of the 41st Annual International
                  Symposium on Computer Architecuture},
  year =         2014,
  pages =        {145-156},
  abstract =     {Low utilization of on-chip cache capacity limits
                  performance and wastes energy because of the long
                  latency, limited bandwidth, and energy consumption
                  associated with off-chip memory accesses. Value
                  replication is an important source of low capacity
                  utilization. While prior cache compression
                  techniques manage to code frequent values densely,
                  they trade off a high compression ratio for low
                  decompression latency, thus missing opportunities to
                  utilize capacity more effectively.This paper
                  presents, for the first time, a detailed designspace
                  exploration of caches that utilize statistical
                  compression. We show that more aggressive approaches
                  like Huffman coding, which have been neglected in
                  the past due to the high processing overhead for
                  (de)compression, are suitable techniques for caches
                  and memory. Based on our key observation that value
                  locality varies little over time and across
                  applications, we first demonstrate that the overhead
                  of statistics acquisition for code generation is low
                  because new encodings are needed rarely, making it
                  possible to off-load it to software routines. We
                  then show that the high compression ratio obtained
                  by Huffman-coding makes it possible to utilize the
                  performance benefits of 4X larger last-level caches
                  with about 50 \% lower power consumption than such
                  larger caches},
  isbn =         9781479943944,
  location =     {Minneapolis, Minnesota, USA},
  numpages =     12,
  publisher =    {IEEE Press},
  series =       {ISCA '14},
}

@inproceedings{eklov10_stats,
  author =       {D. {Eklov} and E. {Hagersten}},
  title =        {StatStack: Efficient modeling of LRU caches},
  booktitle =    {2010 IEEE International Symposium on Performance
                  Analysis of Systems Software (ISPASS)},
  year =         2010,
  pages =        {55-65},
  doi =          {10.1109/ISPASS.2010.5452069},
  url =          {https://doi.org/10.1109/ISPASS.2010.5452069},
  abstract =     {Efficient execution on modern architectures requires
                  good data locality, which can be measured by the
                  powerful stack distance abstraction. Based on this
                  abstraction, the miss rate for LRU caches of any
                  size can be predicted. However, measuring stack
                  distance requires the number of unique memory
                  objects to be counted between successive accesses to
                  the same data object, which requires complex and
                  inefficient data collection. This paper presents a
                  new efficient way of estimating the stack distances
                  of an application. Instead of counting the number of
                  unique memory objects touched between successive
                  accesses to the same data, our scheme only requires
                  the number of memory accesses to be counted, a task
                  efficiently handled by existing builtin hardware
                  counters. Furthermore, this information only needs
                  to be captured for a small fraction of the memory
                  accesses. A new efficient off-line algorithm is
                  proposed to estimate the corresponding stack
                  distance based on this sparse information. We
                  evaluate the accuracy of the proposed estimation
                  compared with full stack distance measurements for
                  28 of the applications in the SPEC CPU2006 benchmark
                  suite. The estimation shows excellent accuracy based
                  on information about only every 10,000th memory
                  access.},
  keywords =     {cache storage;LRU caches;stack distance
                  abstraction;data collection;SPEC CPU2006 benchmark
                  suite;stack distance measurements;off-line
                  algorithm;Application software;Hardware;Counting
                  circuits;Delay;Bandwidth;Runtime;Information
                  technology;Distance measurement;Random access
                  memory;Instruments},
  month =        {March},
}

@inproceedings{berg04_statc,
  author =       {E. {Berg} and E. {Hagersten}},
  title =        {StatCache: a probabilistic approach to efficient and
                  accurate data locality analysis},
  booktitle =    {IEEE International Symposium on - ISPASS Performance
                  Analysis of Systems and Software, 2004},
  year =         2004,
  pages =        {20-27},
  doi =          {10.1109/ISPASS.2004.1291352},
  url =          {https://doi.org/10.1109/ISPASS.2004.1291352},
  abstract =     {The widening memory gap reduces performance of
                  applications with poor data locality. Therefore,
                  there is a need for methods to analyze data locality
                  and help application optimization. In this paper we
                  present StatCache, a novel sampling-based method for
                  performing data-locality analysis on realistic
                  workloads. StatCache is based on a probabilistic
                  model of the cache, rather than a functional cache
                  simulator. It uses statistics from a single run to
                  accurately estimate miss ratios of fully-associative
                  caches of arbitrary sizes and generate working-set
                  graphs. We evaluate StatCache using the SPEC CPU2000
                  benchmarks and show that StatCache gives accurate
                  results with a sampling rate as low as 10/sup
                  -4/. We also provide a proof-of-concept
                  implementation, and discuss potentially very fast
                  implementation alternatives.},
  keywords =     {cache storage;probability;sampling methods;data
                  structures;performance evaluation;benchmark
                  testing;StatCache;data locality
                  analysis;optimization;sampling-based
                  method;probabilistic model;statistics;working-set
                  graphs;SPEC CPU2000 benchmarks;cache memories;Data
                  analysis;Sampling
                  methods;Statistics;Runtime;Application
                  software;Information analysis;Information
                  technology;Modems;High performance computing;Cache
                  memory},
  month =        {March}
}


@inproceedings{mytkowicz09_produc_wrong_data_doing_anyth_obvious_wrong,
  author =       {Mytkowicz, Todd and Diwan, Amer and Hauswirth,
                  Matthias and Sweeney, Peter F.},
  title =        {Producing Wrong Data without Doing Anything
                  Obviously Wrong!},
  booktitle =    {Proceedings of the 14th International Conference on
                  Architectural Support for Programming Languages and
                  Operating Systems},
  year =         {2009},
  pages =        {265-276},
  doi =          {10.1145/1508244.1508275},
  url =          {https://doi.org/10.1145/1508244.1508275},
  abstract =     {This paper presents a surprising result: changing a
                  seemingly innocuous aspect of an experimental setup
                  can cause a systems researcher to draw wrong
                  conclusions from an experiment. What appears to be
                  an innocuous aspect in the experimental setup may in
                  fact introduce a significant bias in an
                  evaluation. This phenomenon is called measurement
                  bias in the natural and social sciences.Our results
                  demonstrate that measurement bias is significant and
                  commonplace in computer system evaluation. By
                  significant we mean that measurement bias can lead
                  to a performance analysis that either over-states an
                  effect or even yields an incorrect conclusion. By
                  commonplace we mean that measurement bias occurs in
                  all architectures that we tried (Pentium 4, Core 2,
                  and m5 O3CPU), both compilers that we tried (gcc and
                  Intel's C compiler), and most of the SPEC CPU2006 C
                  programs. Thus, we cannot ignore measurement
                  bias. Nevertheless, in a literature survey of 133
                  recent papers from ASPLOS, PACT, PLDI, and CGO, we
                  determined that none of the papers with experimental
                  results adequately consider measurement
                  bias.Inspired by similar problems and their
                  solutions in other sciences, we describe and
                  demonstrate two methods, one for detecting (causal
                  analysis) and one for avoiding (setup randomization)
                  measurement bias.},
  address =      {New York, NY, USA},
  isbn =         {9781605584065},
  keywords =     {bias, performance, measurement},
  location =     {Washington, DC, USA},
  numpages =     {12},
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS XIV},
}

@inproceedings{eklov11_cache_pirat,
  author =       {D. {Eklov} and N. {Nikoleris} and
                  D. {Black-Schaffer} and E. {Hagersten}},
  title =        {Cache Pirating: Measuring the Curse of the Shared
                  Cache},
  booktitle =    {2011 International Conference on Parallel
                  Processing},
  year =         2011,
  pages =        {165-175},
  doi =          {10.1109/ICPP.2011.15},
  ISSN =         {2332-5690},
  abstract =     {We present a low-overhead method for accurately
                  measuring application performance (CPI) and off-chip
                  bandwidth (GB/s) as a function of available shared
                  cache capacity. The method is implemented on real
                  hardware, with no modifications to the application
                  or operating system. We accomplish this by
                  co-running a Pirate application that "steals" cache
                  space with the Target application. By adjusting how
                  much space the Pirate steals during the Target's
                  execution, and using hardware performance counters
                  to record the Target's performance, we can
                  accurately and efficiently capture performance data
                  for the Target application as a function of its
                  available shared cache. At the same time we use
                  performance counters to monitor the Pirate to ensure
                  that it is successfully stealing the desired amount
                  of cache. To evaluate this approach, we show that 1)
                  the cache available to the Target behaves as
                  expected, 2) the Pirate steals the desired amount of
                  cache, and ) the Pirate does not bias the Target's
                  performance. As a result, we are able to accurately
                  measure the Target's performance while stealing up
                  to an average of 6.8MB of the 8MB of cache on our
                  Nehalem based test system with an average
                  measurement overhead of only 5.5 \%.},
  keywords =     {cache storage;cache pirating;shared cache
                  capacity;Pirate application;Target
                  application;performance counter;Nehalem based test
                  system;application performance measurement;off-chip
                  bandwidth
                  measurement;Bandwidth;Throughput;Hardware;Size
                  measurement;Radiation detectors;Instruction
                  sets;Benchmark testing;performance analysis;cache
                  performance;multicore performance;memory
                  system;memory bandwidth},
  month =        {Sep.},
}

@inproceedings{eklov13_bandw_bandit,
  author =       {D. {Eklov} and N. {Nikoleris} and
                  D. {Black-Schaffer} and E. {Hagersten}},
  title =        {Bandwidth Bandit: Quantitative characterization of
                  memory contention},
  booktitle =    {Proceedings of the 2013 IEEE/ACM International
                  Symposium on Code Generation and Optimization (CGO)},
  year =         2013,
  pages =        {1-10},
  doi =          {10.1109/CGO.2013.6494987},
  abstract =     {On multicore processors, co-executing applications
                  compete for shared resources, such as cache capacity
                  and memory bandwidth. This leads to suboptimal
                  resource allocation and can cause substantial
                  performance loss, which makes it important to
                  effectively manage these shared resources. This,
                  however, requires insights into how the applications
                  are impacted by such resource sharing. While there
                  are several methods to analyze the performance
                  impact of cache contention, less attention has been
                  paid to general, quantitative methods for analyzing
                  the impact of contention for memory bandwidth. To
                  this end we introduce the Bandwidth Bandit, a
                  general, quantitative, profiling method for
                  analyzing the performance impact of contention for
                  memory bandwidth on multicore machines. The
                  profiling data captured by the Bandwidth Bandit is
                  presented in a bandwidth graph. This graph
                  accurately captures the measured application's
                  performance as a function of its available memory
                  bandwidth, and enables us to determine how much the
                  application suffers when its available bandwidth is
                  reduced. To demonstrate the value of this data, we
                  present a case study in which we use the bandwidth
                  graph to analyze the performance impact of memory
                  contention when co-running multiple instances of
                  single threaded application.},
  keywords =     {cache storage;performance evaluation;shared memory
                  systems;bandwidth bandit;quantitative
                  characterization;memory contention;multicore
                  processors;cache capacity;memory
                  bandwidth;suboptimal resource allocation;resource
                  sharing;profiling method;multicore
                  machines;bandwidth graph;single threaded
                  application;Bandwidth;Parallel processing;Memory
                  management;Instruction sets;Benchmark
                  testing;Multicore processing;Resource
                  management;Bandwidth;Memory;Caches},
  month =        {Feb},
}

@inproceedings{ustiugov21_bench_analy_optim_server_funct_snaps,
  author =       {Dmitrii Ustiugov and Plamen Petrov and Marios Kogias
                  and Edouard Bugnion and Boris Grot},
  title =        {Benchmarking, Analysis, and Optimization of
                  Serverless Function Snapshots},
  booktitle =    {Proceedings of the 26th ACM International Conference
                  on Architectural Support for Programming Languages
                  and Operating Systems (ASPLOS'21)},
  year =         2021,
  doi =          {10.1145/3445814.3446714},
  url =          {https://doi.org/10.1145/3445814.3446714},
  publisher =    {{ACM}},
}

@inproceedings{yasin14_top_down,
  author =       {A. {Yasin}},
  title =        {A Top-Down method for performance analysis and
                  counters architecture},
  booktitle =    {2014 IEEE International Symposium on Performance
                  Analysis of Systems and Software (ISPASS)},
  year =         2014,
  pages =        {35-44},
  doi =          {10.1109/ISPASS.2014.6844459},
  url =          {https://doi.org/10.1109/ISPASS.2014.6844459},
  abstract =     {Optimizing an application's performance for a given
                  microarchitecture has become painfully
                  difficult. Increasing microarchitecture complexity,
                  workload diversity, and the unmanageable volume of
                  data produced by performance tools increase the
                  optimization challenges. At the same time resource
                  and time constraints get tougher with recently
                  emerged segments. This further calls for accurate
                  and prompt analysis methods. The insights from this
                  method guide a proposal for a novel performance
                  counters architecture that can determine the true
                  bottlenecks of a general out-of-order
                  processor. Unlike other approaches, our analysis
                  method is low-cost and already featured in
                  in-production systems - it requires just eight
                  simple new performance events to be added to a
                  traditional PMU. It is comprehensive - no
                  restriction to predefined set of performance
                  issues. It accounts for granular bottlenecks in
                  super-scalar cores, missed by earlier approaches.},
  keywords =     {Pipelines;Radiation detectors;Out of order;Electric
                  breakdown;Measurement;Bandwidth;Microarchitecture},
  month =        {March}
}

@inproceedings{kim19_funct,
  author =       {J. {Kim} and K. {Lee}},
  title =        {FunctionBench: A Suite of Workloads for Serverless
                  Cloud Function Service},
  booktitle =    {2019 IEEE 12th International Conference on Cloud
                  Computing (CLOUD)},
  year =         2019,
  pages =        {502-504},
  doi =          {10.1109/CLOUD.2019.00091},
  url =          {https://doi.org/10.1109/CLOUD.2019.00091},
  ISSN =         {2159-6190},
  abstract =     {Serverless computing is attracting considerable
                  attention recently, but many published papers use
                  micro-benchmarks for evaluation that might result in
                  impracticality. To address this, we present
                  FunctionBench, a suite of practical function
                  workloads for public services. It contains realistic
                  data-oriented applications that utilize various
                  resources during execution. The source codes
                  customized for various cloud service providers are
                  publicly available. We are positive that it suggests
                  opportunities for new function applications with
                  lessen experiment setup overheads.},
  keywords =     {Cloud computing;FAA;Computational modeling;Random
                  access memory;Google;Training;Predictive
                  models;workload;benchmark;FaaS;serverless;cloud},
  month =        {July}
}

@inproceedings{yasin14_deep_cloud,
  author =       {Yasin, Ahmad and Ben-Asher, Yosi and Mendelson, Avi},
  title =        {Deep-dive analysis of the data analytics workload in
                  CloudSuite},
  booktitle =    {2014 IEEE International Symposium on Workload
                  Characterization (IISWC)},
  year =         2014,
  pages =        {202-211},
  doi =          {10.1109/IISWC.2014.6983059},
  url =          {https://doi.org/10.1109/IISWC.2014.6983059}
}







@inproceedings{tariq20_sequoia,
  author =       {Tariq, Ali and Pahl, Austin and Nimmagadda, Sharat
                  and Rozner, Eric and Lanka, Siddharth},
  title =        {Sequoia: Enabling Quality-of-Service in Serverless
                  Computing},
  booktitle =    {Proceedings of the 11th ACM Symposium on Cloud
                  Computing},
  year =         {2020},
  pages =        {311-327},
  doi =          {10.1145/3419111.3421306},
  url =          {https://doi.org/10.1145/3419111.3421306},
  abstract =     {Serverless computing is a rapidly growing paradigm
                  that easily harnesses the power of the cloud. With
                  serverless computing, developers simply provide an
                  event-driven function to cloud providers, and the
                  provider seamlessly scales function invocations to
                  meet demands as event-triggers occur. As current and
                  future serverless offerings support a wide variety
                  of serverless applications, effective techniques to
                  manage serverless workloads becomes an important
                  issue. This work examines current management and
                  scheduling practices in cloud providers, uncovering
                  many issues including inflated application run
                  times, function drops, inefficient allocations, and
                  other undocumented and unexpected behavior. To fix
                  these issues, a new quality-of-service function
                  scheduling and allocation framework, called Sequoia,
                  is designed. Sequoia allows developers or
                  administrators to easily def ne how serverless
                  functions and applications should be deployed,
                  capped, prioritized, or altered based on easily
                  configured, flexible policies.  Results with
                  controlled and realistic workloads show Sequoia
                  seamlessly adapts to policies, eliminates mid-chain
                  drops, reduces queuing times by up to 6.4X, enforces
                  tight chain-level fairness, and improves run-time
                  performance up to 25X.},
  address =      {New York, NY, USA},
  isbn =         {9781450381376},
  keywords =     {serverless computing, quality-of-service,
                  measurement},
  location =     {Virtual Event, USA},
  numpages =     {17},
  publisher =    {Association for Computing Machinery},
  series =       {SoCC '20}
}

@inproceedings{hoseinyfarahabady17_qos_aware_resour_alloc_contr,
  author =       {HoseinyFarahabady, MohammadReza and Lee, Young Choon
                  and Zomaya, Albert Y.  and Tari, Zahir},
  title =        {A QoS-Aware Resource Allocation Controller for
                  Function as a Service (FaaS) Platform},
  booktitle =    {Service-Oriented Computing},
  year =         2017,
  pages =        {241--255},
  abstract =     {Function as a Service (FaaS) is a recent
                  event-driven serverless paradigm that allows
                  enterprises to build their applications in a fault
                  tolerant distributed manner. Having been considered
                  as an attractive replacement of traditional Service
                  Oriented Architecture (SOA), the FaaS platform
                  leverages the management of massive data sets or the
                  handling of event streams. However, the realization
                  of such leverage is largely dependent on the
                  effective exploitation of FaaS
                  elasticity/scalability.},
  address =      {Cham},
  editor =       {Maximilien, Michael and Vallecillo, Antonio and
                  Wang, Jianmin and Oriol, Marc},
  isbn =         {978-3-319-69035-3},
  publisher =    {Springer International Publishing},
}

@inproceedings{copik21_sebs,
  author =       {Marcin Copik and Grzegorz Kwasniewski and Maciej
                  Besta and Michal Podstawski and Torsten Hoefler},
  title =        {SeBS: A Serverless Benchmark Suite for
                  Function-as-a-Service Computing},
  booktitle =    {Proceedings of the 22nd International Middleware
                  Conference},
  year =         2021,
  doi =          {10.1145/3464298.3476133},
  url =          {https://doi.org/10.1145/3464298.3476133},
  publisher =    {Association for Computing Machinery},
  series =       {Middleware '21},
}


@inproceedings{suresh19_fnsch,
  author =       {Suresh, Amoghvarsha and Gandhi, Anshul},
  title =        {FnSched: An Efficient Scheduler for Serverless
                  Functions},
  booktitle =    {Proceedings of the 5th International Workshop on
                  Serverless Computing},
  year =         {2019},
  pages =        {19-24},
  doi =          {10.1145/3366623.3368136},
  url =          {https://doi.org/10.1145/3366623.3368136},
  abstract =     {An imminent challenge in the serverless computing
                  landscape is the escalating cost of infrastructure
                  needed to handle the growing traffic at scale. This
                  work presents FnSched, a function-level scheduler
                  designed to minimize provider resource costs while
                  meeting customer performance requirements. FnSched
                  works by carefully regulating the resource usage of
                  colocated functions on each invoker, and autoscaling
                  capacity by concentrating load on few invokers in
                  response to varying traffic. We implement a
                  prototype of FnSched and show that, compared to
                  existing baselines, FnSched significantly improves
                  resource efficiency, by as much as 36 \%-55 \%,
                  while providing acceptable application latency.},
  address =      {New York, NY, USA},
  isbn =         {9781450370387},
  location =     {Davis, CA, USA},
  numpages =     {6},
  publisher =    {Association for Computing Machinery},
  series =       {WOSC '19},
}

@inproceedings{lukewarm_serverless,
  author =       {Schall, David and Margaritov, Artemiy and Ustiugov
                  Dimitrii and Sandberg, Andreas and Grot, Boris},
  title =        {Lukewarm Serverless Functions: Characterization and
                  Optimization},
  booktitle =    {Proceeding of the 49st Annual International
                  Symposium on Computer Architecuture},
  year =         2022,
  pages =        {??},
  abstract =     {Serverless computing has emerged as a widely-used
                  paradigm for deploying services in the cloud. In
                  serverless, developers organize their applications
                  as a collection of functions, which are invoked
                  on-demand in response to a trigger, such as a user
                  request or an invocation by another function. To
                  avoid long start-up delays associated with booting a
                  new function instance, cloud providers tend to keep
                  recently-triggered functions idle (or warm) on the
                  server for some time after the most recent
                  invocation in anticipation of another
                  invocation. Thus, at any given moment on a server,
                  there may be thousands of distinct warm function
                  instances whose executions are interleaved in time
                  based on incoming invocations.  This paper observes
                  that (1) there is a high degree of interleav- ing
                  among warm functions on a given server; (2) the
                  individual warm functions are invoked relatively
                  infrequently, often at the granularity of seconds or
                  minutes; and (3) many functions complete within a
                  few milliseconds. The infrequent invocations of a
                  function instance on a busy server leads to
                  thrashing of the functions microarchitectural state
                  between invocations.  Meanwhile, the short execution
                  time of the function impedes amortization of the
                  warm-up latency of the cache hierarchy, causing a
                  31-114\% increase in CPI compared to execution with
                  warm microarchitectural state. We identify on-chip
                  misses for instructions as a major contributor to
                  the performance loss and propose Jukebox, a
                  record-and-replay instruction prefetcher
                  specifically designed for reducing the start-up
                  latency of warm function instances. Jukebox requires
                  just 32KB of metadata per function instance, and
                  boosts performance by an average of 18.7\% for the
                  evaluated functions, which translates into a
                  corresponding throughput improvement at the server
                  level.,},isbn =         9781479943944,
  location =     {New York, New York, USA},
  numpages =     13,
  publisher =    {IEEE Press},
  series =       {ISCA '22},
}


@inproceedings{kotni21_faast,
  author =       {Swaroop Kotni and Ajay Nayak and Vinod Ganapathy and
                  Arkaprava Basu},
  title =        {Faastlane: Accelerating {Function-as-a-Service}
                  Workflows},
  booktitle =    {2021 USENIX Annual Technical Conference (USENIX ATC
                  21)},
  year =         2021,
  pages =        {805--820},
  url =
                  {https://www.usenix.org/conference/atc21/presentation/kotni},
  isbn =         {978-1-939133-23-6},
  month =        jul,
  publisher =    {USENIX Association},
  }

@inproceedings{akkus18_sand,
  author =       {Istemi Ekin Akkus and Ruichuan Chen and Ivica Rimac
                  and Manuel Stein and Klaus Satzke and Andre Beck and
                  Paarijaat Aditya and Volker Hilt},
  title =        {{SAND}: Towards {High-Performance} Serverless
                  Computing},
  booktitle =    {2018 USENIX Annual Technical Conference (USENIX ATC
                  18)},
  year =         2018,
  pages =        {923--935},
  url =
                  {https://www.usenix.org/conference/atc18/presentation/akkus},
  address =      {Boston, MA},
  isbn =         {978-1-939133-01-4},
  month =        jul,
  publisher =    {USENIX Association},
}


@inproceedings{klimovic18_pocket,
  author =       {Ana Klimovic and Yawen Wang and Patrick Stuedi and
                  Animesh Trivedi and Jonas Pfefferle and Christos
                  Kozyrakis},
  title =        {Pocket: Elastic Ephemeral Storage for Serverless
                  Analytics},
  booktitle =    {13th USENIX Symposium on Operating Systems Design
                  and Implementation (OSDI 18)},
  year =         2018,
  pages =        {427--444},
  url =
                  {https://www.usenix.org/conference/osdi18/presentation/klimovic},
  address =      {Carlsbad, CA},
  isbn =         {978-1-939133-08-3},
  month =        oct,
  publisher =    {USENIX Association},
}

@inproceedings{jia21_night,
  author =       {Jia, Zhipeng and Witchel, Emmett},
  title =        {Nightcore: Efficient and Scalable Serverless
                  Computing for Latency-Sensitive, Interactive
                  Microservices},
  booktitle =    {Proceedings of the 26th ACM International Conference
                  on Architectural Support for Programming Languages
                  and Operating Systems},
  year =         {2021},
  pages =        {152-166},
  doi =          {10.1145/3445814.3446701},
  url =          {https://doi.org/10.1145/3445814.3446701},
  abstract =     {The microservice architecture is a popular software
                  engineering approach for building flexible,
                  large-scale online services. Serverless functions,
                  or function as a service (FaaS), provide a simple
                  programming model of stateless functions which are a
                  natural substrate for implementing the stateless RPC
                  handlers of microservices, as an alternative to
                  containerized RPC servers. However, current
                  serverless platforms have millisecond-scale runtime
                  overheads, making them unable to meet the strict
                  sub-millisecond latency targets required by existing
                  interactive microservices. We present Nightcore, a
                  serverless function runtime with microsecond-scale
                  overheads that provides container-based isolation
                  between functions. Nightcore's design carefully
                  considers various factors having microsecond-scale
                  overheads, including scheduling of function
                  requests, communication primitives, threading models
                  for I/O, and concurrent function
                  executions. Nightcore currently supports serverless
                  functions written in C/C++, Go, Node.js, and
                  Python. Our evaluation shows that when running
                  latency-sensitive interactive microservices,
                  Nightcore achieves 1.36\texttimes{}-2.93\texttimes{}
                  higher throughput and up to 69 \% reduction in tail
                  latency.},
  address =      {New York, NY, USA},
  isbn =         {9781450383172},
  keywords =     {microservices, Cloud computing, serverless
                  computing, function-as-a-service},
  location =     {Virtual, USA},
  numpages =     {15},
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS '21},
}

@inproceedings{shillaker20_faasm,
  author =       {Simon Shillaker and Peter Pietzuch},
  title =        {Faasm: Lightweight Isolation for Efficient Stateful
                  Serverless Computing},
  booktitle =    {2020 USENIX Annual Technical Conference (USENIX ATC
                  20)},
  year =         2020,
  pages =        {419--433},
  url =
                  {https://www.usenix.org/conference/atc20/presentation/shillaker},
  isbn =         {978-1-939133-14-4},
  month =        jul,
  publisher =    {USENIX Association},
}

@article{sreekanti20_cloud,
  author =       {Sreekanti, Vikram and Wu, Chenggang and Lin, Xiayue
                  Charles and Schleier-Smith, Johann and Gonzalez,
                  Joseph E. and Hellerstein, Joseph M. and Tumanov,
                  Alexey},
  title =        {Cloudburst: Stateful Functions-As-A-service},
  journal =      {Proc. VLDB Endow.},
  volume =       13,
  number =       12,
  pages =        {2438-2452},
  year =         2020,
  doi =          {10.14778/3407790.3407836},
  url =          {https://doi.org/10.14778/3407790.3407836},
  abstract =     {Function-as-a-Service (FaaS) platforms and
                  "serverless" cloud computing are becoming
                  increasingly popular due to ease-of-use and
                  operational simplicity. Current FaaS offerings are
                  targeted at stateless functions that do minimal I/O
                  and communication. We argue that the benefits of
                  serverless computing can be extended to a broader
                  range of applications and algorithms while
                  maintaining the key benefits of existing FaaS
                  offerings. We present the design and implementation
                  of Cloudburst, a stateful FaaS platform that
                  provides familiar Python programming with
                  low-latency mutable state and communication, while
                  maintaining the autoscaling benefits of serverless
                  computing. Cloudburst accomplishes this by
                  leveraging Anna, an autoscaling key-value store, for
                  state sharing and overlay routing combined with
                  mutable caches co-located with function executors
                  for data locality. Performant cache consistency
                  emerges as a key challenge in this architecture. To
                  this end, Cloudburst provides a combination of
                  lattice-encapsulated state and new definitions and
                  protocols for distributed session
                  consistency. Empirical results on benchmarks and
                  diverse applications show that Cloudburst makes
                  stateful functions practical, reducing the
                  state-management overheads of current FaaS platforms
                  by orders of magnitude while also improving the
                  state of the art in serverless consistency.},
  issn =         {2150-8097},
  issue_date =   {August 2020},
  month =        {jul},
  numpages =     15,
  publisher =    {VLDB Endowment},
}

@inproceedings{barcelona-pons19_faas_track,
  author =       {Barcelona-Pons, Daniel and S\'{a}nchez-Artigas, Marc
                  and Par\'{\i}s, Gerard and Sutra, Pierre and
                  Garc\'{\i}a-L\'{o}pez, Pedro},
  title =        {On the FaaS Track: Building Stateful Distributed
                  Applications with Serverless Architectures},
  booktitle =    {Proceedings of the 20th International Middleware
                  Conference},
  year =         {2019},
  pages =        {41-54},
  doi =          {10.1145/3361525.3361535},
  url =          {https://doi.org/10.1145/3361525.3361535},
  abstract =     {Serverless computing is an emerging paradigm that
                  greatly simplifies the usage of cloud resources and
                  suits well to many tasks. Most notably,
                  Function-as-a-Service (FaaS) enables programmers to
                  develop cloud applications as individual functions
                  that can run and scale independently. Yet, due to
                  the disaggregation of storage and compute resources
                  in FaaS, applications that require fine-grained
                  support for mutable state and synchronization, such
                  as machine learning and scientific computing, are
                  hard to build.In this work, we present Crucial, a
                  system to program highly-concurrent stateful
                  applications with serverless architectures. Its
                  programming model keeps the simplicity of FaaS and
                  allows to port effortlessly multi-threaded
                  algorithms to this new environment. Crucial is built
                  upon the key insight that FaaS resembles to
                  concurrent programming at the scale of a data
                  center. As a consequence, a distributed shared
                  memory layer is the right answer to the need for
                  fine-grained state management and coordination in
                  serverless. We validate our system with the help of
                  micro-benchmarks and various applications. In
                  particular, we implement two common machine learning
                  algorithms: k-means clustering and logistic
                  regression. For both cases, Crucial obtains superior
                  or comparable performance to an equivalent Spark
                  cluster.},
  address =      {New York, NY, USA},
  isbn =         {9781450370097},
  keywords =     {FaaS, stateful, in-memory, synchronization,
                  Serverless},
  location =     {Davis, CA, USA},
  numpages =     {14},
  publisher =    {Association for Computing Machinery},
  series =       {Middleware '19},
}


@inproceedings{mahgoub21_sonic,
  author =       {Ashraf Mahgoub and Karthick Shankar and Subrata
                  Mitra and Ana Klimovic and Somali Chaterji and
                  Saurabh Bagchi},
  title =        {{SONIC}: Application-aware Data Passing for Chained
                  Serverless Applications},
  booktitle =    {2021 USENIX Annual Technical Conference (USENIX ATC
                  21)},
  year =         2021,
  pages =        {285--301},
  url =
                  {https://www.usenix.org/conference/atc21/presentation/mahgoub},
  isbn =         {978-1-939133-23-6},
  month =        jul,
  publisher =    {USENIX Association},
}

@inproceedings{stojkovic23_specf,
  author =       {Stojkovic, Jovan and Xu, Tianyin and Franke,
                  Hubertus and Torrellas, Josep},
  title =        {SpecFaaS: Accelerating Serverless Applications with
                  Speculative Function Execution},
  booktitle =    {2023 IEEE International Symposium on
                  High-Performance Computer Architecture (HPCA)},
  year =         2023,
  pages =        {814-827},
  doi =          {10.1109/HPCA56546.2023.10071120},
  ISSN =         {2378-203X},
  abstract =     {Serverless computing has emerged as a popular cloud
                  computing paradigm. Serverless environments are
                  convenient to users and efficient for cloud
                  providers. However, they can induce substantial
                  application execution overheads, especially in
                  applications with many functions.In this paper, we
                  propose to accelerate serverless applications with a
                  novel approach based on software-supported
                  speculative execution of functions. Our proposal is
                  termed Speculative Function-as-a-Service
                  (SpecFaaS). It is inspired by out-of-order execution
                  in modern processors, and is grounded in a
                  characterization analysis of FaaS applications. In
                  SpecFaaS, functions in an application are executed
                  early, speculatively, before their control and data
                  dependences are resolved. Control dependences are
                  predicted like in pipeline branch prediction, and
                  data dependences are speculatively satisfied with
                  memoization. With this support, the execution of
                  downstream functions is overlapped with that of
                  upstream functions, substantially reducing the
                  end-to-end execution time of applications. We
                  prototype SpecFaaS on Apache OpenWhisk, an
                  open-source serverless computing platform. For a set
                  of applications in a warmed-up environment, SpecFaaS
                  attains an average speedup of 4.6$\times$. Further,
                  on average, the application throughput increases by
                  3.9$\times$ and the tail latency decreases by 58.7
                  \%.},
  month =        {Feb},
}

@misc{burckhardt21_server_workf_durab_funct_nether,
  abstract =     {Serverless is an increasingly popular choice for
                  service architects because it can provide elasticity
                  and load-based billing with minimal developer
                  effort. A common and important use case is to
                  compose serverless functions and cloud storage into
                  reliable workflows. However, existing solutions for
                  authoring workflows provide a rudimentary experience
                  compared to writing standard code in a modern
                  programming language. Furthermore, executing
                  workflows reliably in an elastic serverless
                  environment poses significant performance
                  challenges. To address these, we propose Durable
                  Functions, a programming model for serverless
                  workflows, and Netherite, a distributed execution
                  engine to execute them efficiently. Workflows in
                  Durable Functions are expressed as task-parallel
                  code in a host language of choice. Internally, the
                  workflows are translated to fine-grained stateful
                  communicating processes, which are load-balanced
                  over an elastic cluster. The main challenge is to
                  minimize the cost of reliably persisting progress to
                  storage while supporting elastic scale. Netherite
                  solves this by introducing partitioning, recovery
                  logs, asynchronous snapshots, and speculative
                  communication.Our results show that Durable
                  Functions simplifies the expression of complex
                  workflows, and that Netherite achieves lower latency
                  and higher throughput than the prevailing approaches
                  for serverless workflows in Azure and AWS, by orders
                  of magnitude in some cases.},
  author =       {Burckhardt, Sebastian and Gillum, Chris and Justo,
                  David and Kallas, Konstantinos and McMahon, Connor
                  and Meiklejohn, Christopher S.},
  howpublished = {arXiv},
  month =        {February},
  title =        {Serverless Workflows with Durable Functions and
                  Netherite},
  url =
                  {https://www.microsoft.com/en-us/research/publication/serverless-workflows-with-durable-functions-and-netherite/},
  year =         2021,
}

@inproceedings {215949,
author = {Sol Boucher and Anuj Kalia and David G. Andersen and Michael Kaminsky},
title = {Putting the "Micro" Back in Microservice},
booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
year = {2018},
isbn = {978-1-939133-01-4},
address = {Boston, MA},
pages = {645--650},
url = {https://www.usenix.org/conference/atc18/presentation/boucher},
publisher = {USENIX Association},
month = jul,
}

@article{mahgoub22_wisef,
  author =       {Mahgoub, Ashraf and Yi, Edgardo Barsallo and
                  Shankar, Karthick and Minocha, Eshaan and Elnikety,
                  Sameh and Bagchi, Saurabh and Chaterji, Somali},
  title =        {Wisefuse: Workload Characterization and Dag
                  Transformation for Serverless Workflows},
  journal =      {Proc. ACM Meas. Anal. Comput. Syst.},
  volume =       6,
  number =       2,
  year =         2022,
  doi =          {10.1145/3530892},
  url =          {https://doi.org/10.1145/3530892},
  abstract =     {We characterize production workloads of serverless
                  DAGs at a major cloud provider. Our analysis
                  highlights two major factors that limit performance:
                  (a) lack of efficient communication methods between
                  the serverless functions in the DAG, and (b)
                  stragglers when a DAG stage invokes a set of
                  parallel functions that must complete before
                  starting the next DAG stage. To address these
                  limitations, we propose WISEFUSE, an automated
                  approach to generate an optimized execution plan for
                  serverless DAGs for a user-specified latency
                  objective or budget. We introduce three
                  optimizations: (1) Fusion combines in-series
                  functions together in a single VM to reduce the
                  communication overhead between cascaded
                  functions. (2) Bundling executes a group of parallel
                  invocations of a function in one VM to improve
                  resource sharing among the parallel workers to
                  reduce skew. (3) Resource Allocation assigns the
                  right VM size to each function or function bundle in
                  the DAG to reduce the E2E latency and cost. We
                  implement WISEFUSE to evaluate it experimentally
                  using three popular serverless applications with
                  different DAG structures, memory footprints, and
                  intermediate data sizes. Compared to competing
                  approaches and other alternatives, WISEFUSE shows
                  significant improvements in E2E latency and
                  cost. Specifically, for a machine learning pipeline,
                  WISEFUSE achieves P95 latency that is 67\% lower
                  than Photons, 39\% lower than Faastlane, and 90\%
                  lower than SONIC without increasing the cost.},
  address =      {New York, NY, USA},
  articleno =    26,
  issue_date =   {June 2022},
  keywords =     {serverless, workload characterization, dag
                  transformation},
  month =        {jun},
  numpages =     28,
  publisher =    {Association for Computing Machinery},
}

@inproceedings{asheim23_storag_effec_btb_organ_server,
  author =       {Asheim, Truls and Grot, Boris and Kumar, Rakesh},
  title =        {A Storage-Effective BTB Organization for Servers},
  booktitle =    {2023 IEEE International Symposium on
                  High-Performance Computer Architecture (HPCA)},
  year =         2023,
  pages =        {1153-1167},
  doi =          {10.1109/HPCA56546.2023.10070938},
}


@article{asheim21_btb_x,
  author =       {Asheim, Truls and Grot, Boris and Kumar, Rakesh},
  title =        {Btb-X: a Storage-Effective Btb Organization},
  journal =      {IEEE Computer Architecture Letters},
  volume =       20,
  number =       2,
  pages =        {134-137},
  year =         2021,
  doi =          {10.1109/LCA.2021.3109945},
  url =          {https://doi.org/10.1109/LCA.2021.3109945},
  abstract =     {Many contemporary applications feature
                  multi-megabyte instruction footprints that overwhelm
                  the capacity of branch target buffers (BTB) and
                  instruction caches (L1-I), causing frequent
                  front-end stalls that inevitably hurt
                  performance. BTB is crucial for performance as it
                  enables the front-end to accurately resolve the
                  upcoming execution path and steer instruction fetch
                  appropriately. Moreover, it also enables highly
                  effective fetch-directed instruction prefetching
                  that can eliminate many L1-I misses. For these
                  reasons, commercial processors allocate vast amounts
                  of storage capacity to BTBs. This letter aims to
                  reduce BTB storage requirements by optimizing the
                  organization of BTB entries. Our key insight is that
                  today's BTBs store the full target address for each
                  branch, yet the vast majority of dynamic branches
                  have short offsets requiring just a handful of bits
                  to encode. Based on this insight, we organize the
                  BTB as an ensemble of smaller BTBs, each storing
                  offsets within a particular range. Doing so enables
                  a dramatic reduction in storage for target
                  addresses. We also compress tags to reduce the tag
                  storage cost. Our final design, called BTB-X, uses
                  an ensemble of five BTBs with compressed tags that
                  enables it to track 2.8x more branches than a
                  conventional BTB with the same storage budget.},
  address =      {USA},
  issn =         {1556-6056},
  issue_date =   {July-Dec. 2021},
  month =        {jul},
  numpages =     4,
  publisher =    {IEEE Computer Society},
}

@inproceedings{asheim22_impac_microar_state_reuse_server_funct,
  author =       {Asheim, Truls and Khan, Tanvir Ahmed and Kasicki,
                  Baris and Kumar, Rakesh},
  title =        {Impact of Microarchitectural State Reuse on
                  Serverless Functions},
  booktitle =    {Proceedings of the Eighth International Workshop on
                  Serverless Computing},
  year =         {2022},
  pages =        {7-12},
  doi =          {10.1145/3565382.3565879},
  url =          {https://doi.org/10.1145/3565382.3565879},
  abstract =     {Serverless computing has seen rapid growth in the
                  past few years due to its seamless scalability and
                  zero resource provisioning overhead for
                  developers. In serverless, applications are composed
                  of a set of very short-running functions which are
                  invoked in response to events such as HTTP
                  requests. For better resource utilization, cloud
                  providers interleave the execution of thousands of
                  serverless functions on a single server.Recent work
                  argues that this interleaved execution and short
                  run-times cause the serverless functions to perform
                  poorly on modern processors. This is because
                  interleaved execution thrashes the
                  microarchitectural state of a function, thus forcing
                  its subsequent execution to start from a cold
                  state. Further, due to their short-running nature,
                  serverless functions are unable to amortize the
                  warm-up latency of microarchitectural structures,
                  meaning that most the function execution happen from
                  cold state.In this work, we analyze a function's
                  performance sensitivity to microarchitectural state
                  thrashing induced by interleaved execution. Unlike
                  prior work, our analysis reveals that not all
                  functions experience performance degradation because
                  of microarchitectural state thrashing. The two
                  dominating factors that dictate the impact of
                  thrashing on function performance are function
                  execution time and code footprint. For example, we
                  observe that only the functions with short execution
                  times (&lt; 1 ms) show performance degradation due
                  to thrashing and that this degradation is
                  exacerbated for functions with large code
                  footprints.},
  address =      {New York, NY, USA},
  isbn =         {9781450399272},
  keywords =     {top-down, microarchitecture, serverless,
                  measurement, FaaS},
  location =     {Quebec, Quebec City, Canada},
  numpages =     {6},
  publisher =    {Association for Computing Machinery},
  series =       {WoSC '22},
}

@inproceedings{asheim23_special_btb_organ_server,
  author =       {Asheim, Truls and Grot, Boris and Kumar, Rakesh},
  title =        {A Specialized BTB Organization for Servers},
  booktitle =    {Proceedings of the International Conference on
                  Parallel Architectures and Compilation Techniques},
  year =         {2023},
  pages =        {548-549},
  doi =          {10.1145/3559009.3569692},
  url =          {https://doi.org/10.1145/3559009.3569692},
  abstract =     {Contemporary server applications feature massive
                  instruction footprints stemming from deeply layered
                  software stacks. These footprints far exceed the
                  capacity of the branch target buffer (BTB) and
                  instruction cache (L1-I), resulting in the so-called
                  front-end bottleneck. BTB misses may lead to
                  wrong-path execution, triggering a pipeline flush
                  when misspeculation is detected. Such pipeline
                  flushes not only throw away tens of cycles of work
                  but also expose the fill latency of the
                  pipeline. Similarly, L1-I misses cause the core
                  front-end to stall for tens of cycles while the miss
                  is being served from lower-level caches.},
  address =      {New York, NY, USA},
  isbn =         {9781450398688},
  location =     {Chicago, Illinois},
  numpages =     {2},
  publisher =    {Association for Computing Machinery},
  series =       {PACT '22},
}

@article{perleberg1993branch,
  title={Branch target buffer design and optimization},
  author={Perleberg, Chris H and Smith, Alan Jay},
  journal={IEEE transactions on computers},
  volume={42},
  number={4},
  pages={396--412},
  year={1993},
  publisher={IEEE}
}

@article{seznec2002design,
  title={Design tradeoffs for the Alpha EV8 conditional branch predictor},
  author={Seznec, Andr{\'e} and Felix, Stephen and Krishnan, Venkata and Sazeides, Yiannakis},
  journal={ACM SIGARCH Computer Architecture News},
  volume={30},
  number={2},
  pages={295--306},
  year={2002},
  publisher={ACM New York, NY, USA}
}

@article{burcea2009phantom,
  title={Phantom-btb: a virtualized branch target buffer design},
  author={Burcea, Ioana and Moshovos, Andreas},
  journal={Acm Sigplan Notices},
  volume={44},
  number={3},
  pages={313--324},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@inproceedings{10.1145/279358.279364,
author = {Keeton, Kimberly and Patterson, David A. and He, Yong Qiang and Raphael, Roger C. and Baker, Walter E.},
title = {Performance Characterization of a Quad Pentium Pro SMP Using OLTP Workloads},
year = {1998},
isbn = {0818684917},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1145/279358.279364},
doi = {10.1145/279358.279364},
abstract = {Commercial applications are an important, yet often overlooked, workload with significantly different characteristics from technical workloads. The potential impact of these differences is that computers optimized for technical workloads may not provide good performance for commercial applications, and these applications may not fully exploit advances in processor design. To evaluate these issues, we use hardware counters to measure architectural features of a four-processor Pentium Pro-based server running a TPC-C-like workload on an Informix database. We examine the effectiveness of out-of-order execution, branch prediction, speculative execution, superscalar issue and retire, caching and multiprocessor scaling. We find that out-of-order execution, superscalar issue and retire, and branch prediction are not as effective for database workloads as they are for technical workloads, such as SPEC. We find that caches are effective at reducing processor traffic to memory; even larger caches would be helpful to satisfy more data requests. Multiprocessor scaling of this workload is good, but even modest bus utilization degrades application memory latency, limiting database throughput.},
booktitle = {Proceedings of the 25th Annual International Symposium on Computer Architecture},
pages = {1526},
numpages = {12},
location = {Barcelona, Spain},
series = {ISCA '98}
}

@article{keeton98_perfor_charac_quad_pentium_pro,
  author =       {Keeton, Kimberly and Patterson, David A. and He,
                  Yong Qiang and Raphael, Roger C. and Baker, Walter
                  E.},
  title =        {Performance Characterization of a Quad Pentium Pro
                  Smp Using Oltp Workloads},
  journal =      {SIGARCH Comput. Archit. News},
  volume =       26,
  number =       3,
  pages =        {15-26},
  year =         1998,
  doi =          {10.1145/279361.279364},
  url =          {https://doi.org/10.1145/279361.279364},
  abstract =     {Commercial applications are an important, yet often
                  overlooked, workload with significantly different
                  characteristics from technical workloads. The
                  potential impact of these differences is that
                  computers optimized for technical workloads may not
                  provide good performance for commercial
                  applications, and these applications may not fully
                  exploit advances in processor design. To evaluate
                  these issues, we use hardware counters to measure
                  architectural features of a four-processor Pentium
                  Pro-based server running a TPC-C-like workload on an
                  Informix database. We examine the effectiveness of
                  out-of-order execution, branch prediction,
                  speculative execution, superscalar issue and retire,
                  caching and multiprocessor scaling. We find that
                  out-of-order execution, superscalar issue and
                  retire, and branch prediction are not as effective
                  for database workloads as they are for technical
                  workloads, such as SPEC. We find that caches are
                  effective at reducing processor traffic to memory;
                  even larger caches would be helpful to satisfy more
                  data requests. Multiprocessor scaling of this
                  workload is good, but even modest bus utilization
                  degrades application memory latency, limiting
                  database throughput.},
  address =      {New York, NY, USA},
  issn =         {0163-5964},
  issue_date =   {June 1998},
  month =        {apr},
  numpages =     12,
  publisher =    {Association for Computing Machinery},
}

@article{10.1145/384265.291067,
author = {Ranganathan, Parthasarathy and Gharachorloo, Kourosh and Adve, Sarita V. and Barroso, Luiz Andr\'{e}},
title = {Performance of Database Workloads on Shared-Memory Systems with out-of-Order Processors},
year = {1998},
issue_date = {Dec. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/384265.291067},
doi = {10.1145/384265.291067},
abstract = {Database applications such as online transaction processing (OLTP) and decision support systems (DSS) constitute the largest and fastest-growing segment of the market for multiprocessor servers. However, most current system designs have been optimized to perform well on scientific and engineering workloads. Given the radically different behavior of database workloads (especially OLTP), it is important to re-evaluate key system design decisions in the context of this important class of applications.This paper examines the behavior of database workloads on shared-memory multiprocessors with aggressive out-of-order processors, and considers simple optimizations that can provide further performance improvements. Our study is based on detailed simulations of the Oracle commercial database engine. The results show that the combination of out-of-order execution and multiple instruction issue is indeed effective in improving performance of database workloads, providing gains of 1.5 and 2.6 times over an in-order single-issue processor for OLTP and DSS, respectively. In addition, speculative techniques enable optimized implementations of memory consistency models that significantly improve the performance of stricter consistency models, bringing the performance to within 10--15\% of the performance of more relaxed models.The second part of our study focuses on the more challenging OLTP workload. We show that an instruction stream buffer is effective in reducing the remaining instruction stalls in OLTP, providing a 17\% reduction in execution time (approaching a perfect instruction cache to within 15\%). Furthermore, our characterization shows that a large fraction of the data communication misses in OLTP exhibit migratory behavior; our preliminary results show that software prefetch and writeback/flush hints can be used for this data to further reduce execution time by 12\%.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {oct},
pages = {307318},
numpages = {12}
}

@inproceedings{ranganathan98_perfor_datab_workl_shared_memor,
  author =       {Ranganathan, Parthasarathy and Gharachorloo, Kourosh
                  and Adve, Sarita V. and Barroso, Luiz Andr\'{e}},
  title =        {Performance of Database Workloads on Shared-Memory
                  Systems with out-of-Order Processors},
  booktitle =    {Proceedings of the Eighth International Conference
                  on Architectural Support for Programming Languages
                  and Operating Systems},
  year =         {1998},
  pages =        {307-318},
  doi =          {10.1145/291069.291067},
  url =          {https://doi.org/10.1145/291069.291067},
  abstract =     {Database applications such as online transaction
                  processing (OLTP) and decision support systems (DSS)
                  constitute the largest and fastest-growing segment
                  of the market for multiprocessor servers. However,
                  most current system designs have been optimized to
                  perform well on scientific and engineering
                  workloads. Given the radically different behavior of
                  database workloads (especially OLTP), it is
                  important to re-evaluate key system design decisions
                  in the context of this important class of
                  applications.This paper examines the behavior of
                  database workloads on shared-memory multiprocessors
                  with aggressive out-of-order processors, and
                  considers simple optimizations that can provide
                  further performance improvements. Our study is based
                  on detailed simulations of the Oracle commercial
                  database engine. The results show that the
                  combination of out-of-order execution and multiple
                  instruction issue is indeed effective in improving
                  performance of database workloads, providing gains
                  of 1.5 and 2.6 times over an in-order single-issue
                  processor for OLTP and DSS, respectively. In
                  addition, speculative techniques enable optimized
                  implementations of memory consistency models that
                  significantly improve the performance of stricter
                  consistency models, bringing the performance to
                  within 10--15\% of the performance of more relaxed
                  models.The second part of our study focuses on the
                  more challenging OLTP workload. We show that an
                  instruction stream buffer is effective in reducing
                  the remaining instruction stalls in OLTP, providing
                  a 17\% reduction in execution time (approaching a
                  perfect instruction cache to within
                  15\%). Furthermore, our characterization shows that
                  a large fraction of the data communication misses in
                  OLTP exhibit migratory behavior; our preliminary
                  results show that software prefetch and
                  writeback/flush hints can be used for this data to
                  further reduce execution time by 12\%.},
  address =      {New York, NY, USA},
  isbn =         {1581131070},
  location =     {San Jose, California, USA},
  numpages =     {12},
  publisher =    {Association for Computing Machinery},
  series =       {ASPLOS VIII},
}


@inproceedings{soundararajan21_pdede,
  author =       {Soundararajan, Niranjan K and Braun, Peter and Khan,
                  Tanvir Ahmed and Kasikci, Baris and Litz, Heiner and
                  Subramoney, Sreenivas},
  title =        {Pdede: Partitioned, deduplicated, delta branch
                  target buffer},
  booktitle =    {MICRO-54: 54th Annual IEEE/ACM International
                  Symposium on Microarchitecture},
  year =         2021,
  pages =        {779--791},
}

@inproceedings{ansari20_divid,
  author =       {Ansari, Ali and Lotfi-Kamran, Pejman and
                  Sarbazi-Azad, Hamid},
  title =        {Divide and conquer frontend bottleneck},
  booktitle =    {2020 ACM/IEEE 47th Annual International Symposium on
                  Computer Architecture (ISCA)},
  year =         2020,
  pages =        {65--78},
  organization = {IEEE},
}


@inproceedings{khan21_rippl,
  author =       {Khan, Tanvir Ahmed and Zhang, Dexin and Sriraman,
                  Akshitha and Devietti, Joseph and Pokam, Gilles and
                  Litz, Heiner and Kasikci, Baris},
  title =        {Ripple: Profile-Guided Instruction Cache Replacement
                  for Data Center Applications},
  booktitle =    {Proceedings of the 48th International Symposium on
                  Computer Architecture (ISCA)},
  year =         2021,
  month =        jun,
  series =       {ISCA 2021},
}

@misc{serverless_state,
  author = {Datalog},
  title = {The state of serverless},
  howpublished = "\url{https://www.datadoghq.com/state-of-serverless-2022/}",
  year         = 2022
}

@misc{serverless_state_21,
  author = {Datalog},
  title = {The state of serverless},
  howpublished = "\url{https://www.datadoghq.com/state-of-serverless-2021/}",
  year         = 2021
}


@inproceedings{ishii21_re_fetch_direc_instr_prefet,
  author =       {Ishii, Yasuo and Lee, Jaekyu and Nathella,
                  Krishnendra and Sunwoo, Dam},
  title =        {Re-establishing Fetch-Directed Instruction
                  Prefetching: An Industry Perspective},
  booktitle =    {2021 IEEE International Symposium on Performance
                  Analysis of Systems and Software (ISPASS)},
  year =         2021,
  pages =        {172-182},
  doi =          {10.1109/ISPASS51385.2021.00034},
}

@INPROCEEDINGS{IBMz,
  author={Bonanno, James and Collura, Adam and Lipetz, Daniel and Mayer, Ulrich and Prasky, Brian and Saporito, Anthony},
  booktitle={2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={Two level bulk preload branch prediction}, 
  year={2013},
  volume={},
  number={},
  pages={71-82},
  doi={10.1109/HPCA.2013.6522308}
}

@ARTICLE{zen2,
  author={Suggs, David and Subramony, Mahesh and Bouvier, Dan},
  journal={IEEE Micro}, 
  title={The {AMD} Zen 2 Processor}, 
  year={2020},
  volume={40},
  number={2},
  pages={45-52},
  doi={10.1109/MM.2020.2974217}
}

  
@ARTICLE{neoverse,
  author={Pellegrini, Andrea and Stephens, Nigel and Bruce, Magnus and Ishii, Yasuo and Pusdesris, Joseph and Raja, Abhishek and Abernathy, Chris and Koppanalil, Jinson and Ringe, Tushar and Tummala, Ashok and Jalal, Jamshed and Werkheiser, Mark and Kona, Anitha},
  journal={IEEE Micro}, 
  title={The Arm Neoverse N1 Platform: Building Blocks for the Next-Gen Cloud-to-Edge Infrastructure SoC}, 
  year={2020},
  volume={40},
  number={2},
  pages={53-62},
  doi={10.1109/MM.2020.2972222}
}

@INPROCEEDINGS{exynos,
  author={Grayson, Brian and Rupley, Jeff and Zuraski, Gerald Zuraski and Quinnell, Eric and Jimnez, Daniel A. and Nakra, Tarun and Kitchin, Paul and Hensley, Ryan and Brekelbaum, Edward and Sinha, Vikas and Ghiya, Ankit},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Evolution of the Samsung Exynos CPU Microarchitecture}, 
  year={2020},
  volume={},
  number={},
  pages={40-51},
  doi={10.1109/ISCA45697.2020.00015}
}

@INPROCEEDINGS{splash2,
  author={Woo, S.C. and Ohara, M. and Torrie, E. and Singh, J.P. and Gupta, A.},
  booktitle={Proceedings 22nd Annual International Symposium on Computer Architecture},
  title={The SPLASH-2 programs: characterization and methodological considerations},
  year={1995},
  volume={},
  number={},
  pages={24-36}
}

@article{williams16_growin_need_micros_bioin,
  author =       {Christopher L. Williams and Jeffrey C. Sica and
                  Robert T. Killen and Ulysses G.J. Balis},
  title =        {The Growing Need for Microservices in
                  Bioinformatics},
  journal =      {Journal of Pathology Informatics},
  volume =       7,
  number =       1,
  pages =        45,
  year =         2016,
  doi =          {10.4103/2153-3539.194835},
  url =
                  {https://www.sciencedirect.com/science/article/pii/S2153353922005636},
  issn =         {2153-3539},
  keywords =     {Bioinformatics, crowd sourcing, defect analysis,
                  failure mode analysis, information technology,
                  microservices, pathology, reliability engineering,
                  software engineering},
}

@inproceedings{seznec96_dont_use_page_number_point_it,
  author =       {Seznec, Andr\'{e}},
  title =        {Don't Use the Page Number, but a Pointer to It},
  booktitle =    {Proceedings of the 23rd Annual International
                  Symposium on Computer Architecture},
  year =         {1996},
  pages =        {104-113},
  doi =          {10.1145/232973.232985},
  url =          {https://doi.org/10.1145/232973.232985},
  addtaress =      {New York, NY, USA},
  isbn =         {0897917863},
  keywords =     {address width, indirect-tagged caches, tag
                  implementation cost, reduced branch target buffers},
  location =     {Philadelphia, Pennsylvania, USA},
  numpages =     {10},
  publisher =    {Association for Computing Machinery},
  series =       {ISCA '96},
}

@misc{ipc1,
 title = {1st {I}nstruction {P}refetching {C}hampionship},
 note = {\url{https://research.ece.ncsu.edu/ipc/}},
 year = {2020}
}

@misc{cvp,
 title = {First {C}hampionship {V}alue {P}rediction},
 note = {\url{https://www.microarch.org/cvp1/cvp1online/contestants.html}},
 year = {2018}
}

@misc{champsim,
 title = {Champ{S}im {S}imulator},
 note = {\url{https://github.com/ChampSim/ChampSim}},
 year = {20203}
}

@manual{intelmanual,
author = {Intel Corporation},
title = {Intel 64 and IA-32 Architectures Software Developers Manual, Volume 2, Instruction set reference},
date = {2023-09},
version = {325383-081US},
}

@misc{perftool,
author = {Jake Edge},
title = {Perfcounters added to the mainline},
date = {2009-07-01},
howpublished = {\url{https://lwn.net/Articles/339361/}},
}

@INPROCEEDINGS{cacti,
  author={Li, Sheng and Chen, Ke and Ahn, Jung Ho and Brockman, Jay B. and Jouppi, Norman P.},
  booktitle={2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)}, 
  title={CACTI-P: Architecture-level modeling for SRAM-based structures with advanced leakage reduction techniques}, 
  year={2011},
  volume={},
  number={},
  pages={694-701},
  doi={10.1109/ICCAD.2011.6105405}}



@article{eismann20_review_server_use_cases_their_charac,
  author =       {Simon Eismann and Joel Scheuner and Erwin Van Eyk
                  and Maximilian Schwinger and Johannes Grohmann and
                  Nikolas Herbst and Cristina L. Abad and Alexandru
                  Iosup},
  title =        {A Review of Serverless Use Cases and Their
                  Characteristics},
  journal =      {CoRR},
  volume =       {abs/2008.11110},
  year =         2020,
  url =          {https://arxiv.org/abs/2008.11110},
  bibsource =    {dblp computer science bibliography,
                  https://dblp.org},
  biburl =
                  {https://dblp.org/rec/journals/corr/abs-2008-11110.bib},
  eprint =       {2008.11110},
  eprinttype =   {arXiv},
  timestamp =    {Fri, 28 Aug 2020 14:37:31 +0200},
}

@article{van2018serverless,
  title={Serverless is more: {From} {PaaS} to present cloud computing},
  author={Van Eyk, Erwin and Toader, Lucian and Talluri, Sacheendra and Versluis, Laurens and U{\u{a}}, Alexandru and Iosup, Alexandru},
  journal={IEEE Internet Computing},
  volume={22},
  number={5},
  pages={8--17},
  year={2018},
  publisher={IEEE},
  doi={10.1109/MIC.2018.053681358}
}
